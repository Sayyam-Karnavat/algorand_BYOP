
==================================================
Paper 1
Title: Audit Cards: Contextualizing AI Evaluations
Abstract: AI governance frameworks increasingly rely on audits, yet the results of
their underlying evaluations require interpretation and context to be
meaningfully informative. Even technically rigorous evaluations can offer
little useful insight if reported selectively or obscurely. Current literature
focuses primarily on technical best practices, but evaluations are an
inherently sociotechnical process, and there is little guidance on reporting
procedures and context. Through literature review, stakeholder interviews, and
analysis of governance frameworks, we propose "audit cards" to make this
context explicit. We identify six key types of contextual features to report
and justify in audit cards: auditor identity, evaluation scope, methodology,
resource access, process integrity, and review mechanisms. Through analysis of
existing evaluation reports, we find significant variation in reporting
practices, with most reports omitting crucial contextual information such as
auditors' backgrounds, conflicts of interest, and the level and type of access
to models. We also find that most existing regulations and frameworks lack
guidance on rigorous reporting. In response to these shortcomings, we argue
that audit cards can provide a structured format for reporting key claims
alongside their justifications, enhancing transparency, facilitating proper
interpretation, and establishing trust in reporting.
PDF URL: http://arxiv.org/pdf/2504.13839v1

Full Content:
arXiv:2504.13839v1  [cs.CY]  18 Apr 2025
Preprint. Under review.
Audit Cards: Contextualizing AI Evaluations
Leon Staufer∗
ML Alignment & Theory Scholars
Technical University of Munich
leon@staufer.me
Mick Yang∗
ML Alignment & Theory Scholars
University of Pennsylvania
mickyang@seas.upenn.edu
Anka Reuel
Stanford University
Stephen Casper
Massachusetts Institute of Technology
ML Alignment & Theory Scholars
Abstract
AI governance frameworks increasingly rely on audits, yet the results
of their underlying evaluations require interpretation and context to be
meaningfully informative. Even technically rigorous evaluations can of-
fer little useful insight if reported selectively or obscurely. Current litera-
ture focuses primarily on technical best practices, but evaluations are an
inherently sociotechnical process, and there is little guidance on report-
ing procedures and context. Through literature review, stakeholder inter-
views, and analysis of governance frameworks, we propose “audit cards”
to make this context explicit. We identify six key types of contextual fea-
tures to report and justify in audit cards: auditor identity, evaluation scope,
methodology, resource access, process integrity, and review mechanisms.
Through analysis of existing evaluation reports, we ﬁnd signiﬁcant varia-
tion in reporting practices, with most reports omitting crucial contextual
information such as auditors’ backgrounds, conﬂicts of interest, and the
level and type of access to models. We also ﬁnd that most existing regu-
lations and frameworks lack guidance on rigorous reporting. In response
to these shortcomings, we argue that audit cards can provide a structured
format for reporting key claims alongside their justiﬁcations, enhancing
transparency, facilitating proper interpretation, and establishing trust in
reporting.
1
Introduction
AI governance frameworks1 are being designed to increasingly rely on audits.2 Within
these frameworks, audits are meant to (1) identify potential risks, (2) incentivize more
responsive development practices, and (3) involve more stakeholders in the system de-
ployment process. However, audits can only fulﬁll this role effectively if the underlying
evaluations are both technically rigorous and effectively integrated into decision-making
(Hardy et al., 2024). However, not all evaluations are equally rigorous (e.g., Raji et al., 2022;
Birhane et al., 2024; M¨okander, 2023; Anderljung et al., 2023; Kolt et al., 2024; Casper et al.,
2024; Reuel et al., 2024b; Li & Goel, 2024).
To date, much prior literature has focused on the technical side of audits (see Section 2).
However, evaluations are never conducted in a vacuum (Wallach et al., 2025). Their results
∗Equal contribution. Correspondence to leon@staufer.me and mickyang@seas.upenn.edu.
1This includes frontier AI safety policies from AI companies (e.g. OpenAI Preparedness Frame-
work), regulations (e.g. EU AI Act), and other norms (e.g. NIST AI Risk Management Framework).
See Appendix D for a complete list.
2We understand audits as formalized evaluation processes of AI models. Usually, a speciﬁc eval-
uation procedure (e.g. benchmarking, red-teaming, uplift studies) underlies the audit. We refer to
these underlying procedures as evaluations. See Appendix A for details.
1
Preprint. Under review.
General principles of
reporting on audits
Justiﬁcation
Limitations
Assumptions
Speciﬁc contextual features to report on in audits
Who are the auditors
• Expertise
• Background
Review & Feedback
• Review mechanism
• Feedback channel
• Executive summary
• Defs. of key terms
• Access to results
Target
• Scope and goal
• Type of evaluation
• Continuous evaluations
• Obsolescence criteria
Procedure
• Description of the
underlying evaluation
procedure
• Implementation details
Integrity
• Selection process
• Conﬂicts of interest
• Compensation and
incentive structures
Access and Resources
• Level of access
• Compute, time,
ﬁnancial resources
available
Figure 1: Our audit card template requires reporting on three principles and six features.
(Left) The three principles (justiﬁcation, limitations, and assumptions) further transparency
about the process behind an audit. (Right) The six features offer key methodological and
contextual information. See Section 3 and Appendix C.1 for details.
are intimately shaped, not only by the technical methods employed, but by numerous non-
technical details of their context and design (see Section 3). For example, while the research
ﬁeld has long understood the importance of conﬂict of interest disclosures for academic in-
tegrity (e.g., Knerr & D’Amelia, 2020), auditor independence remains a consistent concern
in the AI ecosystem (Costanza-Chock et al., 2022; Evans et al., 2023).
Even technically rigorous evaluations can be uninformative or even actively misleading
if reported on selectively or obscurely (Ananny & Crawford, 2018). This risk is magniﬁed
given the vested interest of AI developers in obtaining favorable audit results. In AI and
other ﬁelds alike, companies undergoing evaluations have strong incentives to game au-
dits and inﬂuence reporting in potentially misleading ways (e.g., Krawiec, 2003; Lu, 2006;
Marquis et al., 2016).
Thus, for AI audits to play a meaningful role in governance, evaluation reporting must
be done in a way that minimizes the chance of omitting key contextual information. To
address this challenge, we propose “audit cards”. Akin to model and system cards (e.g.,
Mitchell et al., 2019), audit cards provide a structured format for reporting key background,
design choices, and justiﬁcations. They are designed to offer a standardized approach
to reporting that enhances transparency and facilitates informed interpretations of results.
This need not necessarily be reported in a separate audit card document, rather the relevant
context can be added to existing reports, such as sections in model and system cards.
Overall, we make four key contributions:
1. Audit cards and survey of evaluation literature: Based on a survey of 25 prior works on
AI evaluations (Section 3), we propose a template and checklist (Appendix C) for AI au-
dit cards including three key principles (justiﬁcation, limitations, and assumptions) and
six key types of information (auditor identity, evaluation scope, methodology, resource
access, process integrity, and review mechanisms).
2. Survey of frontier AI evaluation reports: We analyze the thoroughness of 20 existing
evaluation reports for frontier systems (Section 4).
3. Survey of governance frameworks: We analyze gaps between best reporting practices
and 21 existing AI governance frameworks (Section 5).
4. Stakeholder insights: We interview and present perspectives from 10 expert stakehold-
ers on AI evaluation reporting practices (Section 6).
2
Preprint. Under review.
2
Related work
Transparency and reporting:
In the past decade, frontier AI research and devel-
opment have largely shifted from being predominantly academia-driven to predomi-
nantly industry-driven (Maslej et al., 2024).
This shift has been accompanied by a de-
crease in transparency around proprietary state-of-the-art systems.
As such, increas-
ing transparency and awareness has emerged as a key goal of AI governance (e.g.,
Felzmann et al., 2020; Haresamudram et al., 2023; Winecoff & Bogen, 2024; Chan et al.,
2024; Bommasani et al., 2024; Kolt et al., 2024). One barrier to meaningful transparency
is a lack of standards for reporting (Maslej et al., 2024). In response to this challenge, previ-
ous works have proposed data, model, system, agent, and usage “cards” (Pushkarna et al.,
2022; Mitchell et al., 2019; Gursoy & Kakadiaris, 2022; Casper et al., 2025; Wahle et al., 2023)
for documenting key information within the AI ecosystem. In this paper, we build on past
work by introducing the notion of an “audit card,” surveying what prior literature suggests
they should contain, and analyzing the current state of reporting around audits.
AI audits: Formal evaluations, also known as “audits,” of AI systems have been proposed
as a key objective to facilitate transparency and scrutiny (Raji et al., 2022; Anderljung et al.,
2023; M¨okander, 2023; Li & Goel, 2024; Reuel et al., 2024a). Meanwhile, audits are increas-
ingly incorporated into frameworks for AI governance (e.g., EU AI Act, 2024). However, as
we will show in Section 5, current governance frameworks often lack substantial guidance
for how to report on audits.
Technically rigorous evaluations: AI systems are audited using a variety of approaches
including case studies, benchmarks, red-teaming, and mechanistic analysis (Bengio et al.,
2024). However, the science of evaluating AI systems is still nascent (Apollo Research,
2024). Not all evaluations are equally technically rigorous, and their apparent outcomes
can be highly sensitive to framing and design (e.g., Schaeffer et al., 2023; Burnell et al., 2023;
Khan et al., 2025). Toward improved technical practices, Reuel et al. (2024b) outline a set
of 46 best practices for rigorous AI benchmark design. Meanwhile, METR (2024b) work
toward a portable standard for framing and conducting capability evaluations. However,
unlike prior work, here we focus on rigor in reporting both key technical details and context
behind AI audits.
On the inherent sociotechnical nature of evaluations: AI evaluations depend on tech-
nical tools to assess system properties and risks. However, they are never conducted in
a vacuum; they are always embedded in a broader sociotechnical and political context.
Meanwhile, the standards that systems are evaluated against are inherently based on sub-
jective human values. Thus, AI evaluations represent a social science measurement chal-
lenge (Wallach et al., 2025). Aside from lacking technical soundness, evaluations can fail
to be rigorous and serve the public’s interest for a variety of nontechnical reasons. Prior
works have emphasized the role of evaluation integrity and integration in ensuring mean-
ingful oversight (Ojewale et al., 2024; Raji et al., 2022; Sharkey et al., 2024). By surveying
prior literature on evaluation procedures (Section 3), analyzing current evaluation reports
(Section 4), and analyzing evaluation frameworks (Section 5), we make progress toward a
more contextual and critical understanding of AI audits.
3
What information do audit cards need for rigorous reporting?
Methodology: We identify key components of audit cards through an initial literature
review of work on AI audits and evaluations, transparency, technical evaluation design,
and sociotechnical approaches.3 We selected 25 total papers and manually annotated them,
scoring the extent to which the paper recommends that aspect of reporting: is it a central
argument of the paper (2), a minor argument of the paper (1), or not mentioned in the paper (0)
(see further details in Appendix B). In Table 1, we summarize the perspectives from all 25
papers on the three principles and six features. Next, we expand on these principles and
features. See Figure 1 for an overview of an audit card and Appendix C.2 for an actionable
3We also reﬁned our audit card template based on structured interviews with expert stakeholders.
See Section 6.
3
Preprint. Under review.
audit card checklist that discusses the relative importance of principles and features for
different stakeholders and auditing types.
Principles
Features
Paper
Justif. Assum. Limit. Who What How Access Integrity Review
Anderljung et al., 2023
1
2
2
2
2
2
2
2
2
Apollo Research, 2024
2
2
0
0
2
2
0
0
0
Barnett & Thiergart, 2024a
2
2
0
1
0
0
0
0
0
Barnett & Thiergart, 2024b
1
2
2
0
2
2
2
2
0
Birhane et al., 2024
2
2
2
2
2
0
1
2
2
Bucknall & Trager, 2023
0
2
2
0
2
0
2
2
2
Burnell et al., 2023
2
2
0
0
1
2
0
0
2
Casper et al., 2024
1
2
0
1
0
0
2
0
1
Chang et al., 2023
0
2
0
2
0
2
0
0
0
Costanza-Chock et al., 2022
2
2
0
2
1
0
2
1
2
Dobbe et al., 2021
2
2
2
2
1
2
0
0
2
Dow et al., 2024
0
2
0
1
2
2
0
0
0
Eriksson et al., 2025
2
2
1
1
2
2
0
1
0
Gebru et al., 2021
1
2
2
2
1
2
2
2
2
Hendrycks & Woodside, 2024
0
0
1
0
0
2
0
0
1
Liang et al., 2023
1
2
0
0
2
0
0
0
0
METR, b
0
0
0
2
2
2
0
0
2
M¨okander et al., 2024
0
2
2
1
2
1
2
2
2
Mukobi, 2024
0
2
0
2
2
2
2
2
1
Ojewale et al., 2024
1
2
0
2
1
1
2
1
1
Raji et al., 2020
0
2
2
2
2
2
1
0
1
Reuel et al., 2024b
0
2
2
2
2
2
0
0
2
Selbst et al., 2019
0
2
2
2
0
2
0
0
0
Shevlane et al., 2023
0
2
2
2
2
2
2
2
2
Weidinger et al., 2025
2
2
2
0
2
2
0
0
1
Table 1: What contextual details do prior works say are key for audit reporting? We
divide reporting into three overarching principles (justiﬁcations, assumptions, limitations)
and six key features (who, what, how, access, integrity, review). We score each paper’s
extent of recommendation of each these as 2 (central argument), 1 (minor argument), and
0 (no mention). Refer to Table 4 for our complete, more granular analysis of papers.
3.1
Overarching principles for transparent reporting
The following three overarching principles behind audit design are key for transparency
and methodological clarity. These apply across the entire auditing process and the six spe-
ciﬁc features. We design audit cards to make it easier to adopt these principles throughout
the auditing process. This, in return, enables accurate interpretation, facilitates construc-
tive critique, and builds trust in the evaluation process.
Justiﬁcations as explicit arguments to support methodological choices, metric selec-
tion, and interpretative frameworks. Explicit reporting on justiﬁcations enables process
transparency. This justiﬁcation can be through elements of a rigorous scientiﬁc process,
such as explaining proxies and experimental coverage (Apollo Research, 2024), or through
standardized auditing procedures, such as detailed standards identiﬁcation (Birhane et al.,
2024; Costanza-Chock et al., 2022).
Assumptions to articulate the premises underlying the evaluation design and analysis.
Reporting on assumptions clariﬁes the relationship (or lack thereof) between tests and
real-world outcomes, presumptions about what constitutes “good” performance, and de-
tails about the threat models considered (Reuel et al., 2024b; Barnett & Thiergart, 2024a;b;
Raji et al., 2020).
Limitations to explicitly acknowledge the constraints that affect the validity, reliability,
or generalizabilityof ﬁndings. Reporting limitations clariﬁes constraints in data sampling,
potential artifacts in the evaluation process, and boundaries of what auditors can reason-
4
Preprint. Under review.
ably claim based on the evaluation evidence (M¨okander et al., 2024; Barnett & Thiergart,
2024b; Reuel et al., 2024b).
3.2
Speciﬁc features for contextual information
Aside from overarching principles, reporting on more speciﬁc contextual features of au-
dits helps stakeholders assess trustworthiness. We compile recommendations from past
literature into six features for reporting evaluation context in audit cards. For a more gran-
ular analysis, see Appendix C.1, where we further divide these six features into constituent
aspects.
• Who are the auditors: Reporting details of auditor identity facilitates trust and clariﬁes
what potential biases might affect their work. It includes reporting the expertise (rele-
vant domain knowledge, accreditations) (Anderljung et al., 2023; METR, b; Reuel et al.,
2024b) and background (track record of conducting evaluations, positionality) of auditors
(Costanza-Chock et al., 2022; Gebru et al., 2021). Given worries about the privacy of au-
ditors, information about the background can be anonymized and aggregated.
• What is evaluated: Reporting on evaluation goals enables clear interpretation of ﬁnd-
ings.
This includes the scope (e.g., model vs.
scaffolding) and goal (context of use)
(Raji et al., 2020; Birhane et al., 2024; Shevlane et al., 2023), type of evaluation (e.g., capa-
bility vs. propensity), as the scope of the evaluation inﬂuences what the relevant level of
access is (METR, 2024b; Liang et al., 2023), as well as reporting on the evaluation ob-
solescence criteria.4
It also includes reporting whether the evaluation will be repeated,
and if so, at what time or after which conditions are met (Barnett & Thiergart, 2024b;
Anderljung et al., 2023).
• How is it evaluated: Explaining the underlying procedures for the evaluation, such
as descriptions of the evaluation setup with justiﬁcations of how a capability translates to
evaluation goal (Apollo Research, 2024; Raji et al., 2020; Reuel et al., 2024b) and provide
interpretation of scores (Dobbe et al., 2021; Selbst et al., 2019). The speciﬁc relevant aspects
to report for this feature may change depending on the underlying evaluation. We there-
fore refer to the appropriate literature for the underlying evaluation for a complete list
of relevant aspects to report and details on how to report on them, e.g. for benchmarks
(Reuel et al., 2024b).
• Access and resources: Report the constraints under which the evaluation was conducted
to allow readers to critically assess the audit’s limitations. This includes, what access au-
ditors have to a system (black/white box, with/without safeguards) and which resources
available are available (compute, time, funding) as this can have a large impact on the rigor
of evaluations (Barnett & Thiergart, 2024b; Ojewale et al., 2024; Casper et al., 2024).
• Integrity: Accountability for the auditing process establishes trust in auditors and rules
out potential conﬂicts of interest. This includes reporting transparently about the se-
lection process (how auditors were chosen) and compensation (funding sources, incentive
structures). These dynamics can signiﬁcantly affect the evaluation process and are there-
fore crucial context (Birhane et al., 2024; Costanza-Chock et al., 2022; Gebru et al., 2021).
• Review and communication: Reports on if and what independent reviews of audit meth-
ods facilitate accountability5 (Reuel et al., 2024b; Anderljung et al., 2023). This further
includes contact information and avenues for corrections, executive summaries (covering the
evaluation process and result for non-technical audiences), and clear deﬁnitions of key
terms (Gebru et al., 2021; Bucknall & Trager, 2023; METR, 2024b; Reuel et al., 2024b).
4Examples of obsolescence criteria could include substantial amounts of model ﬁne-tuning,
changes in system components, or changes in deployment context. This helps stakeholders under-
stand the shelf life of the ﬁndings.
5We discuss issues of intellectual property and non-disclosure agreements (NDAs) standing in
the way of internal review in Section 6 and Section 7. This review can be performed by experts at the
organization by cross-checking the ﬁnal report, insofar as they were not involved in the evaluation
beforehand.
5
Preprint. Under review.
Some of the above features may be considered conﬁdential in certain contexts, with
academic researchers potentially sharing different details than commercial entities
(Mitchell et al., 2019). Recommendations about how and with whom audit cards should
be shared are beyond the scope of this report. However, redactions offer a simple, well-
precedented approach for protecting sensitive information (Boone et al., 2015).
4
What do existing evaluation reports include?
Methodology: To study the thoroughness of existing evaluation reports, we examined 20
reports produced between 2023 and 2025. We selected these 20 due to their focus on frontier
systems, ensuring to include reports from both developers and third party organizations.
We manually annotated each report using the 3 principles and 6 features from our audit
card template (Section 3). For each audit card component, we scored reports on a 0-2 scale
for comprehensively reporting that component (2), minimally reporting that component
(1), not reporting on this component at all (0). We assessed reporting thoroughness only, not
execution quality, making no judgments about how well evaluations were executed. This
means, for example, transparently reporting that there was a lack of any review/feedback
process and justifying this would receive a full score of 2. We summarize the ﬁndings in
Table 2.
Report
Release date Justif. Assum. Limit. Who What How Access Integrity Review Average
Average
1.10
0.56
1.30
0.75
1.25
1.15
0.35
0.15
1.10
0.86
Mistral
10/23
0
0
0
0
1
1
0
0
0
0.22
Apollo
01/24
2
1
2
0
2
2
0
0
2
1.22
Cohere Aya
02/24
2
1
2
2
2
1
0
1
1
1.33
UK AISI evals
05/24
1
0
1
0
1
1
0
0
2
0.67
Gemini paper
06/24
1
1
1
1
1
1
1
1
1
1.00
Llama 3 paper
07/24
1
1
2
2
1
2
0
0
1
1.11
Llama 3 card
07/24
1
1
1
1
1
1
0
0
1
0.78
METR o1
09/24
1
0
1
1
1
1
1
0
1
0.78
Claude 3 card
10/24
1
1
1
1
1
1
0
0
1
0.78
METR Claude 3.5
10/24
1
0
2
1
1
1
0
0
1
0.78
Gemma 2 paper
10/24
1
0
1
0
2
1
0
0
1
0.67
AISI Sonnet 3.5
11/24
2
1
2
1
2
1
1
0
1
1.25
OpenAI o1 card
12/24
1
1
1
1
1
1
1
0
1
0.89
Gemma 2 card
12/24
1
0
1
0
1
1
0
0
1
0.56
Gemini 1.5 paper
12/24
1
1
2
2
1
2
1
1
2
1.44
AISI o1
12/24
2
1
2
1
2
1
1
0
1
1.25
METR update
01/25
1
1
2
0
1
1
0
0
2
0.89
DeepSeek R1
01/25
0
0
0
0
1
1
0
0
0
0.22
o3 mini card
01/25
1
1
1
1
1
1
1
0
1
0.89
DeepSeek V3
02/25
1
0
1
0
1
1
0
0
1
0.56
Table 2: What contextual details do existing audit reports provide? We score reports as
providing comprehensive (2), minimal (1), or no information (0) for each component of an
audit card. Table ordered by release date of the report.
While all evaluation reports provide at least a general overview of procedures, the level
of detail differs.
We ﬁnd that audit reports consistently offer information on evalua-
tion scope (average score 1.25) and procedures (1.15) used. However, contextual details
are much less consistently reported (average 0.86, σ = 0.65) both across organizations
and sometimes within the same organization. For example, Google DeepMind released
four documents with signiﬁcantly varying levels of contextual detail (average scores of
1.00, 0.67, 0.56, and 1.44 in chronological order). Only some reports (e.g., Meinke et al.;
Gemma Team et al. (2024); ¨Ust¨un et al. (2024)) offered detailed information on the scaffold-
ing used to evaluate the system.
Reports inconsistently address assumptions, limitations, and auditor information.
While justiﬁcations and limitations are more commonly acknowledged (18 of 20) than as-
6
Preprint. Under review.
sumptions (12 of 20), they are typically discussed at a high level rather than relating to
speciﬁc methodological choices. Ten reports discuss limitations minimally, while eight do
so comprehensively. However, there are notable qualitative differences even among more
comprehensive discussions. For example, ¨Ust¨un et al. (2024) discuss limitations in prompt-
ing techniques, language transferability, evaluation tool chaining, and reproducibility in
signiﬁcant detail, whereas the OpenAI system card (OpenAI, 2024) only consider limita-
tions of what was being evaluated (e.g. classiﬁed information or restricted data) rather
than substantial limitations of the procedures. Regarding auditor information, 12 of 20 re-
ports include auditor expertise details, but rarely their background (3 of 20), making an
assessment of potential biases in the audit harder.
Reports rarely disclose information about evaluation integrity, resources, review pro-
cesses, and obsolescence criteria. Only 3 of 20 reports detail organization and auditor
selection, conﬂicts of interest, or contractual arrangements governing the evaluation pro-
cess.
Even in these cases, the report tended to be about the contractual details of hu-
man baselines that are part of the evaluation rather than the core auditing team itself.
Only 7 of 20 reports specify which resources auditors had access to, including model ac-
cess level, computational resources, and time constraints.
Similarly, only six reported
on some form of review and 9 on feedback mechanisms (Meinke et al.; METR, 2024a;
OpenAI, 2024; UK AISI & US AISI, 2024a). Four report whether they underwent any form
of peer review or quality assurance process before publication (e.g. Gemini Team et al.,
2024; UK AISI & US AISI, 2024b). Five speciﬁed an option for public feedback and cor-
rections. Zero reports explicitly present obsolescence criteria.
5
What guidance do current governance frameworks offer?
Methodology: We analyzed key governance frameworks issued by institutions that are
considered inﬂuential regarding technical evaluations for risks from state-of-the-art mod-
els. We selected these frameworks based on jurisdiction, institutional authority, and direct
inﬂuence on model evaluations (see Appendix D for details). This included ofﬁcial reg-
ulations (EU AI Act, US Executive Order), voluntary industry standards (NIST AI Risk
Management Framework), and policies from major AI developers. We scored each frame-
work on whether it either explicitly required or recommended reporting of each feature in our
audit card template, as shown in Table 3.
Existing frameworks offer limited guidance on audit reporting. We ﬁnd existing frame-
works consistently require that models are assessed but do not consistently specify when,
by whom, for what, and how. This is somewhat expected, as governance frameworks
for emerging technologies tend to crystallize gradually (Linkov et al., 2018). High-level re-
quirements in governance documents are often supplemented with more speciﬁc forms of
soft law such as codes of practice, guidance, and industry norms. Notably, these frame-
works rarely require evaluation reports to disclose matters key to the integrity of the pro-
cess (only 8 of 21) and the resources and access given to auditors (only 2 of 21).
6We analyzed the Federal Senate Bill No. 2338/2023 proposed but not (yet) law.
7Our analysis is based on multiple legal documents discussed in Appendix D.
8We analyzed the EU AI Act and the Third Draft of its Code of Practice.
9We analyzed the Model AI Governance Framework, which gives optional guidance.
10We analyzed the “Basic Act on the Development of Artiﬁcial Intelligence and Establishment of
Foundation for Trust”, which comes into force Jan 2026.
11We analyzed President Biden’s Executive Order, now repealed.
7
Preprint. Under review.
Principles
Features
Document
Justif. Assum. Limit. Who What How Access Integrity Review
Regulations
Brazil6
0
0
0
0
0
0
0
0
0
China7
0
0
0
0
0
0
0
0
1
EU8
1
1
1
1
1
1
1
1
1
Singapore9
0
0
0
0
0
0
0
0
1
South Korea10
0
0
0
0
0
0
0
0
0
US11
0
0
0
0
1
0
0
0
1
Other norms
Bletchley
0
0
0
0
0
1
0
0
1
EU Audit
1
1
0
1
1
1
0
1
1
Int’l Report
1
1
1
1
1
1
1
1
1
Japan AISI
0
0
0
0
1
0
0
0
0
METR
1
1
1
1
1
1
0
1
1
NIST AI RMF
1
1
1
1
1
1
0
1
1
Paris
0
0
0
0
0
0
0
0
0
Seoul
0
0
0
0
1
0
0
0
0
UK AISI
0
0
1
1
1
1
0
0
1
Company policies
Anthropic
1
1
1
1
1
1
0
1
1
Google DeepMind
1
0
1
1
1
1
0
1
1
Meta
1
0
1
1
1
1
0
1
1
Microsoft
1
1
1
1
1
1
0
0
1
OpenAI
1
0
0
1
1
1
0
0
1
xAI
1
0
1
0
1
1
0
0
1
Table 3: What guidance do existing governance frameworks provide for reporting? We
give binary scores of 1 (requires or recommends explicit reporting of this feature) or 0 (may
mention issue’s importance but not speciﬁcally its disclosure or reporting). For company
policies, it is scored by whether it is disclosed or not. Citations and details for each frame-
work are in Appendix D.
6
Challenges with auditing according to expert stakeholders
Methodology: To more thoroughly understand audit reporting and perceived gaps be-
tween current and best practices, we interviewed 10 experts. We reached out to these
experts based on their expertise and familiarity across the evaluation ecosystem. The 10
experts come from a diverse range of private companies, non-proﬁt organizations, and
government bodies. Interviewees include: evaluation designers (P1), evaluation develop-
ers (P3, P4, P5), evaluation report writers (P1, P3, P5), evaluation-related policy researchers
(P6, P7), and those supporting evaluation delivery and standards development (P8, P9). To
allow for candid discussions, we committed to working with the level of anonymity that
was comfortable for experts. See Appendix F.1 and Appendix F.2 for interviewees’ back-
grounds and interview questions.
The quality of audits depends greatly on the auditor. Reporting on auditor details may
become more critical as the evaluation market grows. Currently, the quality and trustwor-
thiness of evaluators are primarily assessed informally (P1, P6, P7) through impressions of
auditors’ work and reputation. Many evaluation reports currently lack detailed informa-
tion about evaluator selection, training, and reporting methods for various reasons, includ-
ing time constraints or the belief that technical details about the evaluation sufﬁciently
indicate evaluation quality (P3). However, because third-party auditors’ access to propri-
etary models is voluntary (P1, P6), regulatory gaps and market failures must be addressed
to enable more reporting about auditor selection and engagement terms. Standardized
evaluation reporting may become increasingly important as the market expands. Auditing
regimes from other industries, such as ﬁnance, energy, and medicine, may offer guidance
moving forward (P9; see also Anderson-Samways (2024)).
8
Preprint. Under review.
Reporting about the evaluation timeline can help with industry’s standardizationefforts.
The amount of time spent on designing and executing an evaluation can have a direct
effect on the quality and thoroughness of the evaluation. When audits begin and end,
and the corresponding versions of model access at each checkpoint, can often be in tension
with commercial pressures in companies to develop and deploy quickly (P1, P4, P6). Eval-
uation resources, including time, are often difﬁcult to measure in a chaotic situation (P3).
Standardization and guidance beforehand around auditing timeframes may be greatly ap-
preciated by evaluators (P1, P4, P5, P6). In lieu of laws explicitly specifying timeframes
for different stages of evaluation, reporting the time taken by auditors will be crucial to
facilitate standardization and coordination.
Evaluation reports are currently used more by auditors than policymakers. Currently,
evaluation reports by both internal and external evaluators assume a primarily technical
audience. Policymakers are constrained by time, technical understanding, regulatory state
of affairs, and often read them at a high level (P6, P9). Thus, it seems that evaluation reports
more directly inform the science of evaluations than policy, as they assist with reproducibil-
ity and elicitation. In the future, it will likely be useful for government authorities to check
compliance with standards. Experts have also said that adapting features to a prioritiza-
tion for each user group would be valuable (P1, P5, P6, P8). This may be the subject of
future research.
7
Discussion and recommendations
There are no established standards around how to report the context of audits. The
present lack of standards leads to incompleteness and inconsistency in audit reporting
practices. Certain contextual aspects, such as the integrity of auditing processes, are par-
ticularly neglected. Lacking transparency around audit methodology and context creates
a signiﬁcant barrier to accountability in the AI ecosystem, where trust in evaluations is
essential for informed decision-making by regulators, users, and the public.
Audit cards are a practical solution to reporting that accounts for an audit’s real-world
context and constraints. The limitations and inconsistencies in current reporting can limit
the ability of evaluations to serve their intended role. As the ﬁeld grows, having standards
for reporting is needed to ensure trust between developers, auditors, regulators, and the
public. By offering a checklist for reporting, audit cards are designed to capture essential
contextual information and allow for a meaningful interpretation of results.
Audit cards remain one approach among several to increase transparency between de-
velopers, users, and other stakeholders. Audit cards are designed to offer a structured
checklist applicable to all types of assessment (see Appendix C.2), but should be viewed
as an instrument in a larger AI accountability toolkit. The key context they offer can be
helpful to other existing mechanisms, including model and system cards, datasheets, and
benchmark details (Mitchell et al., 2019; Gebru et al., 2021; Reuel et al., 2024b). Further re-
search may investigate how these cards can enhance and supplement other transparency
methods.
Limitations and concerns:
• Limited sample size and reporting details. To enable consistent high-level analysis of
the current ecosystem, we used simple 2 or 3 point scoring methods. However, this
comes at the expense of granular details and nuance, differentiating between reports
only to a limited degree. This is particularly pronounced for our analysis of governance
frameworks, where we use binary scores. Furthermore, our analysis, while extensive (25
academic papers, 20 evaluation reports, and 21 governance frameworks), is not exhaus-
tive.
• Trade-offs between reporting quality and audit quality: Implementing thorough re-
porting through audit cards requires additional time and effort from auditors who often
already operate under signiﬁcant resource constraints. The additional burden of com-
prehensive reporting may be challenging, particularly for smaller organizations or time-
sensitive evaluations.
9
Preprint. Under review.
• Veriﬁcation challenges: Even with comprehensive reporting guidelines, there remain
challenges in verifying the accuracy of disclosed information. However, fraud is not
unique to AI. Other ﬁelds handle these challenges through formal scrutiny mechanisms
and occasional legal action. Through explicit disclosure, audit cards can be a ﬁrst step
toward a more accountable governance regime.
Key near-term challenges with AI audit reporting include providing rigorous and stan-
dardized guidance. In the absence of external, authoritative standards for reporting, stake-
holders across the AI ecosystem will struggle to effectively compare and interpret audit re-
sults. The EU AI Act (including its Code of Practice) and various contributions from AISIs
and NIST, international consortia, labs, and third-party evaluators have offered progress
toward a shared understanding of rigor. However, future initiatives may beneﬁt from a
higher degree of speciﬁcity (e.g., templates) for reporting on the principles and features
highlighted in this paper.
Standardized reporting frameworks are key for trustworthy AI audits. The path toward
effective AI governance requires both technical innovation and procedural standardization.
By establishing shared expectations for auditing transparency through structured report-
ing mechanisms like audit cards, policymakers can build a more accountable AI gover-
nance landscape. Such frameworks ensure evaluations are rigorous, not only in execution,
but also in reporting. Ultimately, this enables stakeholders to make informed decisions
based on transparent, comparable, and contextually rich information that serves the public
interest.
Acknowledgments
This research was supported by the ML Alignment & Theory Scholars (MATS) Program,
which provided funding for Leon Staufer and Mick Yang through research stipends. We
also thank MATS and our research manager Juan Gil for his organizational assistance and
research support.
We express our gratitude to Michael Aird, Lily Stelling, as well as other members of the
MATS cohort, participants in the FAR Labs discussion group, and others for their valuable
feedback and suggestions throughout the development of this paper.
We are grateful to all participants who contributed through interviews. Their perspectives
were instrumental in developing the frameworks presented.
10
Preprint. Under review.
References
Artiﬁcial intelligence law of the people’s republic of China.
Mike Ananny and Kate Crawford.
Seeing without knowing: Limitations of the trans-
parency ideal and its application to algorithmic accountability. new media & society, 20(3):
973–989, 2018.
Markus Anderljung, Everett Thornton Smith, Joe O’Brien, Lisa Soder, Benjamin Bucknall,
Emma Bluemke, Jonas Schuett, Robert Trager, Lacey Strahm, and Rumman Chowdhury.
Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem
under the ASPIRE framework, November 2023.
Bill Anderson-Samways. AI-relevant regulatory precedents: A systematic search across all
federal agencies. Institute for AI Policy and Strategy, April 2024.
Anthropic. Responsible Scaling Policy. October 2024.
Apollo Research. We need a Science of Evals, January 2024.
Peter Barnett and Lisa Thiergart. Declare and Justify: Explicit assumptions in AI evalua-
tions are necessary for effective regulation, November 2024a.
Peter Barnett and Lisa Thiergart. What AI evaluations for preventing catastrophic risks can
and cannot do, November 2024b.
Yoshua Bengio, S¨oren Mindermann, Daniel Privitera, Tamay Besiroglu, Rishi Bommasani,
Stephen Casper, Yejin Choi, Danielle Goldfarb, Hoda Heidari, Leila Khalatbari, et al.
International scientiﬁc report on the safety of advanced ai (interim report). arXiv preprint
arXiv:2412.05282, 2024.
Nicola Morini Bianzino, Marie-Laure Delarue, Shawn Maher, Ansgar Koene, Katie Kum-
mer, and Fatima Hassan-Szlamka. The Artiﬁcial Intelligence (AI) global regulatory land-
scape. Technical report, EY, 2023.
Abeba Birhane, Ryan Steed, Victor Ojewale, Briana Vecchione, and Inioluwa Deborah Raji.
AI auditing: The Broken Bus on the Road to AI Accountability, January 2024.
Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor
Maslej, Arvind Narayanan, and Percy Liang. Foundation model transparency reports.
In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, volume 7, pp. 181–
195, 2024.
Audra L. Boone, Ioannis V. Floros, and Shane A. Johnson. Redacting proprietary informa-
tion at the initial public offering, May 2015.
Anu Bradford. The Brussels Effect: How the European Union Rules the World. Oxford Univer-
sity PressNew York, 1 edition, February 2020. ISBN 978-0-19-008858-3 978-0-19-008861-3.
doi: 10.1093/oso/9780190088583.001.0001.
Brazilian
Federal
Senate.
Bill
no.
2338/2023:
Provides
for
the
use
of
artiﬁcial
intelligence,
12
2023.
URL
https://www.congressonacional.leg.br/materias/materias-bicamerais/-/ver/pl-2338-2023.
Benjamin S Bucknall and Robert F Trager. Structured Access for Third-Party Research on
Frontier AI Models: Investigating Researchers’ Model Access Requirements. October
2023.
Ryan Burnell, Wout Schellaert, John Burden, Tomer D. Ullman, Fernando Martinez-
Plumed, Joshua B. Tenenbaum, Danaja Rutar, Lucy G. Cheke, Jascha Sohl-Dickstein,
Melanie Mitchell, Douwe Kiela, Murray Shanahan, Ellen M. Voorhees, Anthony G. Cohn,
Joel Z. Leibo, and Jose Hernandez-Orallo. Rethink reporting of evaluation results in AI.
Science, 380(6641):136–138, April 2023. doi: 10.1126/science.adf6369.
11
Preprint. Under review.
Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Ben-
jamin Bucknall, Andreas Haupt, Kevin Wei, J´er´emy Scheurer, Marius Hobbhahn, Lee
Sharkey, Satyapriya Krishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun,
Michael Gerovitch, David Bau, Max Tegmark, David Krueger, and Dylan Hadﬁeld-
Menell. Black-box access is insufﬁcient for rigorous AI audits. In Proceedings of the 2024
ACM Conference on Fairness, Accountability, and Transparency, FAccT ’24, pp. 2254–2272,
New York, NY, USA, June 2024. Association for Computing Machinery.
ISBN 979-8-
4007-0450-5. doi: 10.1145/3630106.3659037.
Stephen Casper, Luke Bailey, Rosco Hunter, Carson Ezell, Emma Cabal´e, Michael
Gerovitch, Stewart Slocum, Kevin Wei, Nikola Jurkovic, Ariba Khan, et al. The ai agent
index. arXiv preprint arXiv:2502.01635, 2025.
Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley,
Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, et al. Visibility into
ai agents. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Trans-
parency, pp. 958–973, 2024.
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,
Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S.
Yu, Qiang Yang, and Xing Xie.
A Survey on Evaluation of Large Language Models,
December 2023.
Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. Who Audits the Au-
ditors? Recommendations from a ﬁeld scan of the algorithmic auditing ecosystem. In
Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT
’22, pp. 1571–1583, New York, NY, USA, June 2022. Association for Computing Machin-
ery. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3533213.
Rogier Creemers, Graham Webster, and Helen Toner. Translation: Internet information
service algorithmic recommendation management provisions, January 2022.
Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz.
Hard choices in artiﬁcial in-
telligence.
Artiﬁcial Intelligence, 300:103555, November 2021.
ISSN 00043702.
doi:
10.1016/j.artint.2021.103555.
P. Alex Dow, Jennifer Wortman Vaughan, Solon Barocas, Chad Atalla, Alexandra Choulde-
chova, and Hanna Wallach. Dimensions of generative AI evaluation design, November
2024.
Maria Eriksson, Erasmo Puriﬁcato, Arman Noroozian, Joao Vinagre, Guillaume Chaslot,
Emilia Gomez, and David Fernandez-Llorca. Can we trust AI benchmarks? An interdis-
ciplinary review of current issues in AI evaluation, February 2025.
EU AI Act. Regulation (EU) 2024/1689 of the European Parliament and of the Council
of 13 June 2024 laying down harmonised rules on artiﬁcial intelligence and amending
Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858,
(EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and
(EU) 2020/1828 (Artiﬁcial Intelligence Act) (Text with EEA relevance), June 2024.
EU monitor. Legal instruments. URL https://www.eumonitor.eu/9353000/1/j9vvik7m1c3gyxp/vh75mdhkg4s0.
European
Commission.
Third
draft
of
the
general-
purpose
AI
code
of
practice,
March
2025.
URL
https://digital-strategy.ec.europa.eu/en/library/third-draft-general-purpose-ai-code-practice-publi
European Union. Types of legislation. URL https://european-union.europa.eu/institutions-law-budget/law/ty
Owain
Evans,
Jade
Leung,
Toby
Shevlane,
and
Carina
Prunkl.
Who
should
develop
which
AI
evaluations?
2023.
URL
https://www.oxfordmartin.ox.ac.uk/publications/who-should-develop-which-ai-evaluations/.
Accessed: [date of access].
12
Preprint. Under review.
Heike Felzmann, Eduard Fosch-Villaronga, Christoph Lutz, and Aurelia Tam`o-Larrieux.
Towards transparency by design for artiﬁcial intelligence. Science and engineering ethics,
26(6):3333–3361, 2020.
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna
Wallach, Hal Daum´e Iii, and Kate Crawford. Datasheets for datasets, December 2021.
Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, and
et. al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of
context, December 2024.
Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,
Surya Bhupatiraju, L´eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre
Ram´e, and Others. Gemma 2: Improving open language models at a practical size. Arxiv
Preprint Arxiv:2408.00118, 2024.
Google. Frontier safety framework 2.0. February 2025.
Aaron Gu. China released new ethics rules requiring company’s internal EC, October 2023.
URL https://www.hankunlaw.com/en/portal/article/index/cid/8/id/13701.html.
Furkan Gursoy and Ioannis A Kakadiaris. System cards for ai-based decision-making for
public policy. arXiv preprint arXiv:2203.04754, 2022.
Pieter Haeck. EU rules for advanced AI are step in wrong direction, google says. Politico,
February 2025.
Amelia Hardy, Anka Reuel, Kiana Jafari Meimandi, Lisa Soder, Allie Grifﬁth, Dylan M
Asmar, Sanmi Koyejo, Michael S Bernstein, and Mykel J Kochenderfer. More than mar-
keting?
on the information value of ai benchmarks for practitioners.
arXiv preprint
arXiv:2412.05520, 2024.
Kashyap Haresamudram, Stefan Larsson, and Fredrik Heintz.
Three levels of ai trans-
parency. Computer, 56(2):93–100, 2023.
Dan Hendrycks and Thomas Woodside. Devising ML metrics, October 2024.
Seaton Huang, Helen Toner, Zac Haluza, Rogier Creemers, and Graham Webster. Transla-
tion: Measures for the management of generative artiﬁcial intelligence services (draft for
comment), April 2023.
Ariba Khan, Stephen Casper, and Dylan Hadﬁeld-Menell. Randomness, not representation:
The unreliability of evaluating cultural alignment in llms. arXiv preprint arXiv:2503.08688,
2025.
Patrick Knerr and Ronald P D’Amelia. Introduction to the ethics of scientiﬁc conﬂict of
interest (coi). Committee on Ethics, American Chemical Society. Ethics Committee Monograph,
2020.
Noam Kolt, Markus Anderljung, Joslyn Barnhart, Asher Brass, Kevin Esvelt, Gillian K Had-
ﬁeld, Lennart Heim, Mikel Rodriguez, Jonas B Sandbrink, and Thomas Woodside. Re-
sponsible reporting for frontier ai development. In Proceedings of the AAAI/ACM Confer-
ence on AI, Ethics, and Society, volume 7, pp. 768–783, 2024.
Kimberly D Krawiec. Cosmetic compliance and the failure of negotiated governance. Wash.
ULQ, 81:487, 2003.
Yueqi Li and Sanjay Goel. Making it possible for the auditing of ai: A systematic review of
ai audits and ai auditability. Information Systems Frontiers, pp. 1–31, 2024.
13
Preprint. Under review.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Ya-
sunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin New-
man, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning,
Christopher R´e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus,
Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam,
Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Ni-
ladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael
Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi
Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and
Yuta Koreeda. Holistic Evaluation of Language Models, October 2023.
Igor Linkov, Benjamin D Trump, Elke Anklam, David Berube, Patrick Boisseasu, Christo-
pher Cummings, Scott Ferson, Marie-Valentine Florin, Bernard Goldstein, Danail Hris-
tozov, et al. Comparative, collaborative, and integrative risk governance for emerging
technologies. Environment Systems and Decisions, 38:170–176, 2018.
Tong Lu. Does opinion shopping impair auditor independence and audit quality? Journal
of Accounting Research, 44(3):561–583, 2006.
Christopher Marquis, Michael W Toffel, and Yanhua Zhou. Scrutiny, norms, and selective
disclosure: A global study of greenwashing. Organization science, 27(2):483–504, 2016.
Nestor Maslej, Loredana Fattorini, Raymond Perrault, Vanessa Parli, Anka Reuel, Erik
Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Car-
los Niebles, Yoav Shoham, Russell Wald, and Jack Clark. Artiﬁcial intelligence index
report 2024, May 2024.
Alexander Meinke, Bronson Schoen, J´er´emy Scheurer, Mikita Balesni, Rusheb Shah, and
Marius Hobbhahn. Frontier models are capable of In-context scheming.
Meta. Frontier AI Framework. February 2025.
METR. Frontier AI safety policies, a. URL https://metr.org/faisc.
METR. Task Development Guide, b. URL http://taskdev.metr.org/desiderata/.
METR. Details about METR’s preliminary evaluation of OpenAI o1-preview, September
2024a. URL https://metr.github.io/autonomy-evals-guide/openai-o1-preview-report/.
METR.
Portable evaluation tasks via the metr task standard,
2024b.
URL
https://metr.org/blog/2024-02-29-metr-task-standard/. Accessed: 2025-03-14.
Microsoft. Frontier Governance Framework, February 2025.
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben
Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for
model reporting. In Proceedings of the Conference on Fairness, Accountability, and Trans-
parency, FAT* ’19, pp. 220–229, New York, NY, USA, January 2019. Association for Com-
puting Machinery. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287596.
Jakob M¨okander. Auditing of ai: Legal, ethical and technical approaches. Digital Society, 2
(3):49, 2023.
Jakob M¨okander, Jonas Schuett, Hannah Rose Kirk, and Luciano Floridi. Auditing large
language models: A three-layered approach. AI and Ethics, 4(4):1085–1115, November
2024. ISSN 2730-5961. doi: 10.1007/s43681-023-00289-2.
Gabriel Mukobi. Reasons to doubt the impact of AI risk evaluations, August 2024.
National Assembly of the Republic of Korea. Basic Act on the Development of Artiﬁcial
Intelligence and Creation of a Trust Base. Technical Report 2206772, National Assembly
of the Republic of Korea, 1 Uisadang-daero, Yeongdeungpo-gu, Seoul, 07233, Republic
of Korea, November 2024.
14
Preprint. Under review.
Victor Ojewale, Ryan Steed, Briana Vecchione, Abeba Birhane, and Inioluwa Deborah Raji.
Towards ai accountability infrastructure: Gaps and opportunities in ai audit tooling.
arXiv preprint arXiv:2402.17861, 2024.
OpenAI. Preparedness Framework (Beta). December 2023.
OpenAI. OpenAI o1 system card. December 2024.
Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful
and transparent dataset documentation for responsible ai. In Proceedings of the 2022 ACM
Conference on Fairness, Accountability, and Transparency, pp. 1776–1826, 2022.
Inioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Ge-
bru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes.
Clos-
ing the AI accountability gap: Deﬁning an end-to-end framework for internal algorith-
mic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Trans-
parency, pp. 33–44, Barcelona Spain, January 2020. ACM. ISBN 978-1-4503-6936-7. doi:
10.1145/3351095.3372873.
Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho. Outsider oversight:
Designing a third party audit ecosystem for ai governance. In Proceedings of the 2022
AAAI/ACM Conference on AI, Ethics, and Society, pp. 557–571, 2022.
Anka Reuel, Ben Bucknall, Stephen Casper, Tim Fist, Lisa Soder, Onni Aarne, Lewis Ham-
mond, Lujain Ibrahim, Alan Chan, Peter Wills, et al.
Open problems in technical ai
governance. arXiv preprint arXiv:2407.14981, 2024a.
Anka Reuel, Amelia Hardy, Chandler Smith, Max Lamparth, Malcolm Hardy, and Mykel J.
Kochenderfer. BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Estab-
lishing Best Practices, November 2024b.
Olawale Salaudeen, Anka Reuel, Ahmed Ahmed, Suhana Bedi, Zachary Robertson, Sud-
harsan Sundar, Angelina Wang, and Sanmi Koyejo. From measurement to meaning: A
validity-centered approach to AI evaluation.
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large lan-
guage models a mirage?
Advances in Neural Information Processing Systems, 36:55565–
55581, 2023.
Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet
Vertesi. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Con-
ference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 59–68, New York, NY,
USA, January 2019. Association for Computing Machinery. ISBN 978-1-4503-6125-5. doi:
10.1145/3287560.3287598.
Lee Sharkey, Cl´ıodhna N´ı Ghuidhir, Dan Braun, J´er´emy Scheurer, Mikita Balesni, Lucius
Bushnaq, Charlotte Stix, and Marius Hobbhahn. A Causal Framework for AI Regulation
and Auditing, January 2024.
Toby Shevlane, Sebastian Farquhar, Ben Garﬁnkel, Mary Phuong, Jess Whittlestone, Jade
Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho,
Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack
Clark, Yoshua Bengio, Paul Christiano, and Allan Dafoe. Model evaluation for extreme
risks, September 2023.
Singapore. Model AI Governance Framework for Generative AI. January 2020.
Stanford HAI staff. Global AI power rankings: Stanford HAI tool ranks 36 countries in AI,
November 2024.
The
White
House.
Executive
order
on
the
safe,
secure,
and
trustwor-
thy
development
and
use
of
artiﬁcial
intelligence,
October
2023.
URL
https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-saf
15
Preprint. Under review.
UK AISI and US AISI. Pre-Deployment Evaluation of Anthropic’s Upgraded Claude 3.5
Sonnet. November 2024a.
UK AISI and US AISI. Pre-Deployment Evaluation of OpenAI’s o1 Model. December
2024b.
Ahmet ¨Ust¨un, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke
Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil
Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and
Sara Hooker. Aya model: An instruction ﬁnetuned open-access multilingual language
model, February 2024.
Jan Philip Wahle, Terry Ruas, Saif M Mohammad, Norman Meuschke, and Bela Gipp. Ai
usage cards: Responsibly reporting ai-generated content. In 2023 ACM/IEEE Joint Con-
ference on Digital Libraries (JCDL), pp. 282–284. IEEE, 2023.
Hanna Wallach, Meera Desai, A Feder Cooper, Angelina Wang, Chad Atalla, Solon Baro-
cas, Su Lin Blodgett, Alexandra Chouldechova, Emily Corvi, P Alex Dow, et al. Posi-
tion: Evaluating generative ai systems is a social science measurement challenge. arXiv
preprint arXiv:2502.00561, 2025.
Laura Weidinger, Inioluwa Deborah Raji, Hanna Wallach, Margaret Mitchell, Angelina
Wang, Olawale Salaudeen, Rishi Bommasani, Deep Ganguli, Sanmi Koyejo, and William
Isaac. Toward an evaluation science for generative AI systems, March 2025.
Amy A Winecoff and Miranda Bogen. Improving governance outcomes through ai docu-
mentation: Bridging theory and practice. arXiv preprint arXiv:2409.08960, 2024.
xAI. xAI risk management framework (2.10.2025 draft). February 2025.
Appendix A
Disambiguating auditing and the evaluations process
AI audits, for the purpose of our analysis, refer to formalized evaluation processes of AI
models or systems. By formalized, we mean that it is performed systematically and by an
organization, i.e., not just being performed ad hoc by individuals. This kind of evaluation
is a model audit, as opposed to a governance audit or an application audit (M¨okander et al.,
2024).
The evaluation is the underlying assessment run on the model during the audit. We ac-
count for the fact that evaluations can occur internally or externally, and pre- or post-
deployment. Evaluations can assess capabilities, propensities, or risks. Evaluations can
be in many forms, such as benchmarks, red-teaming, human uplift studies, and user stud-
ies. Our approach considers all these variants, as the contextual features we have identiﬁed
are relevant to all. It is possible, that in some of these contexts certain features will be more
important and should therefore be reported in more detail. We explore the importance of
certain features further in Section 7 and Appendix C.1.
We conceptualize the evaluation process as comprising three main stages: (1) designing
the underlying evaluation methodology to achieve its aims (this could be a benchmark,
an evaluation suite, or a red-teaming exercise); (2) executing the evaluation and getting
results; (3) publishing these results in some form. Our focus is to elaborate on stage (3),
that is, what is to be included in these evaluation reports.
When speaking of evaluation reports, we refer to internal or external documents including
evaluation results (e.g., a model’s results on a benchmark). Examples of evaluation reports
include: (OpenAI, 2024; Meinke et al.) as well as private evaluation reports produced by
third-parties for AI companies and internal reports created by AI companies themselves.
16
Preprint. Under review.
Appendix B
Literature review
We reviewed past work on “AI auditing”, “AI evaluations”, “best practices”, “science of
evaluations”, “evaluation context” and “reporting”. We then assessed each paper for rel-
evance based off the titles. This only yielded eight papers. We thus expanded to adjacent
ﬁelds such as transparency and AI auditing, technical evaluation design, and sociotechni-
cal approaches. These ﬁelds integrate technical performance metrics with societal assess-
ments and considerations of how technical performance interacts with real-world social,
cultural, and institutional factors. We scored recommendations within each paper as cen-
tral arguments (2), minor arguments (1), and no mention (0). Through four iterations, we
reﬁned which aspects were most salient, focusing particularly on aspects that were action-
able for auditors. We then categorized and prioritized these aspects based on input from
experts. The mapping of aspects to features is shown in Appendix B.2. The ﬁnal audit card
is depicted in Figure 1 and Appendix C.
B.1
List of papers
1. Towards Publicly Accountable Frontier LLMs (Anderljung et al., 2023)
2. Science of Evals (Apollo Research, 2024)
3. Declare and Justify: Explicit assumptions in AI evaluations are necessary for effec-
tive regulation (Barnett & Thiergart, 2024a)
4. What AI evaluations for preventing catastrophic risks can and cannot do
(Barnett & Thiergart, 2024b)
5. AI auditing: The Broken Bus on the Road to AI Accountability (Birhane et al., 2024)
6. Structured Access for Third-Party Research (Bucknall & Trager, 2023)
7. Rethink reporting of evaluation results in AI (Burnell et al., 2023)
8. Black-Box Access is Insufﬁcient for Rigorous AI Audits (Casper et al., 2024)
9. A Survey on Evaluation of Large Language Models (Chang et al., 2023)
10. Who Audits the Auditors? Recommendations from a ﬁeld scan of the algorithmic
auditing ecosystem (Costanza-Chock et al., 2022)
11. Hard choices in artiﬁcial intelligence (Dobbe et al., 2021)
12. Dimensions of Generative AI Evaluation Design (Dow et al., 2024)
13. Can We Trust AI Benchmarks? (Eriksson et al., 2025)
14. Datasheets for Datasets (Gebru et al., 2021)
15. Devising ML Metrics (Hendrycks & Woodside, 2024)
16. Holistic Evaluation of Language Models (Liang et al., 2023)
17. Task Development Guide (METR, b)
18. Auditing Large Language Models: A Three-Layered Approach (M¨okander et al.,
2024)
19. Reasons to Doubt the Impact of AI Risk Evaluations (Mukobi, 2024)
20. Towards AI Accountability Infrastructure (Ojewale et al., 2024)
21. Closing the AI Accountability Gap: Deﬁning an End-to-End Framework for Inter-
nal Algorithmic Auditing (Raji et al., 2020)
22. BetterBench (Reuel et al., 2024b)
23. Fairness and Abstraction in Sociotechnical Systems (Selbst et al., 2019)
24. Model evaluation for extreme risks (Shevlane et al., 2023)
25. Toward an evaluation science for generative AI systems (Weidinger et al., 2025)
17
Preprint. Under review.
Paper
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Anderljung et al., 2023
2 0 0 2 2 2 2 2 2
2
2
2
0
2
0
0
2
2
0
0
2
1
2
2
Apollo Research, 2024
0 0 0 0 0 2 0 0 0
0
2
0
0
0
0
0
0
0
0
0
2
2
0
0
Barnett & Thiergart, 2024a
0 0 0 0 1 0 0 0 0
0
0
1
0
0
2
0
0
0
0
0
1
2
0
0
Barnett & Thiergart, 2024b
2 2 0 1 0 0 0 0 0
0
2
0
0
0
0
2
2
0
0
0
2
1
2
0
Birhane et al., 2024
1 2 0 0 1 2 0 0 0
0
0
2
0
0
2
0
2
2
0
0
2
2
2
2
Bucknall & Trager, 2023
2 2 0 2 0 2 2 0 0
0
0
0
2
0
0
0
2
0
0
0
2
0
2
2
Burnell et al., 2023
0 0 0 0 0 1 2 0 0
1
2
0
0
2
2
2
0
0
1
0
2
2
0
1
Casper et al., 2024
2 0 1 0 0 0 0 0 0
0
0
0
0
0
0
0
0
0
0
0
2
1
0
1
Chang et al., 2023
0 0 0 0 1 0 0 0 2
0
0
2
0
0
2
0
0
0
0
0
1
0
0
0
Costanza-Chock et al., 2022
2 1 0 0 2 1 0 1 0
0
0
1
0
0
0
0
0
2
0
0
2
2
0
2
Dobbe et al., 2021
0 0 0 0 0 0 0 0 0
0
2
0
0
2
2
0
2
2
1
2
2
2
1
2
Dow et al., 2024
0 0 1 0 1 2 0 0 2
0
1
1
0
0
0
0
0
1
0
0
2
0
2
0
Eriksson et al., 2025
0 1 0 0 1 2 0 0 1
0
2
1
0
1
2
2
1
0
2
2
2
2
0
0
Gebru et al., 2021
2 1 1 0 1 1 2 2 2
1
0
1
2
2
2
2
2
2
0
0
1
1
0
2
Hendrycks & Woodside, 2024 0 0 0 0 0 0 0 0 1
1
2
0
0
0
0
0
1
0
0
0
0
0
0
1
Liang et al., 2023
0 0 0 0 0 0 0 0 0
0
0
0
0
0
0
0
0
0
2
0
2
1
0
0
METR, b
0 0 0 0 2 2 2 0 1
2
2
2
2
0
0
0
0
0
0
0
0
0
0
0
M¨okander et al., 2024
2 2 0 0 1 2 2 0 0
0
1
1
0
0
0
0
2
1
2
0
2
0
2
2
Mukobi, 2024
2 2 2 2 0 2 0 2 0
0
2
0
0
1
0
0
0
0
1
0
2
0
2
1
Ojewale et al., 2024
2 1 1 0 1 1 1 0 1
1
1
1
0
0
0
0
0
2
1
0
2
1
0
1
Raji et al., 2020
1 0 0 0 0 2 0 0 2
0
2
1
0
0
2
0
2
2
2
2
2
0
0
1
Reuel et al., 2024b
0 0 0 0 2 0 0 0 0
2
2
2
2
2
2
0
2
0
0
0
1
0
2
2
Selbst et al., 2019
0 0 0 0 0 0 0 0 2
0
2
1
0
2
2
0
2
2
0
2
2
0
0
0
Shevlane et al., 2023
2 2 0 0 2 2 0 0 0
0
2
0
0
0
0
1
2
0
0
0
2
0
2
2
Weidinger et al., 2025
0 0 0 0 0 2 1 0 0
0
2
0
0
2
0
0
2
0
2
1
2
2
1
0
Table 4: Annotations for all 25 papers (see Appendix B.1) across the 24 initial contextual
aspects (see Appendix B.2). We score each paper’s discussion of these as 2 (central argu-
ment), 1 (minor argument), and 0 (no mention).
B.2
Mapping of initial aspects
Our initial literature review yielded 24 aspects listed below that were mapped (→) to the
principles and features:
1. Level of access auditors had →Access and Resources
2. How did they work with the model developers →Integrity
3. Were they trained by the developers to evaluate the model →Who are the auditors
4. How much compute given to auditors, which manufacturers/chip model →Ac-
cess and Resources
5. Expertise / background of people involved →Who are the auditors
6. What was their initial goal for the evaluation →What is evaluated
7. What were their documentation requirements →Review and communication
8. What was the contract structure / How were they paid/rewarded →Integrity
9. What human input was used / who were they →How is it evaluated
10. Peer review process / QA testing →Review and communication
11. How does capability or concept translate to benchmark task →How is it evaluated
12. Involvement of domain experts during design →Who are the auditors
13. Feedback channel (openness + how to contact) →Review and communication
14. How should scores be interpreted or used →How is it evaluated
15. Assumptions of normative properties are documented →Assumptions
18
Preprint. Under review.
16. Non-normative assumptions justiﬁed →Assumptions
17. Evals as a process should consider eval’s limitations (not that evals are limited) →
Limitations
18. Interdisciplinary teams / including multiple perspectives →Who are the auditors
19. What is the technical goal of the model being evaluated →What is evaluated
20. What are the principles behind the development of the model →What is evaluated
21. Consider context in which the AI system will be used (e.g. value hierarchies, di-
verse populations) →Assumptions
22. Justiﬁcation of evaluation methods used / using qualitative evaluations as well →
Justiﬁcation
23. Timely, continuous evals →What is evaluated
24. Interpretable, easy to read, accessible to a wide audience →Review and commu-
nication
Appendix C
Audit cards
C.1
Details and justiﬁcations of features
This section provides an overview of audit card features, including a concise summary of
what to report and why this contextual information matters. A detailed list of aspects for
each feature can be found in the checklist in Appendix C.2.
Who are the auditors?
Summary: Documents auditor qualiﬁcations through domain expertise, experience, tech-
nical familiarity, formal credentials, stakeholder perspectives, and social positionality.
Why it matters: Transparency about qualiﬁcations establishes credibility while revealing
potential biases. Expertise levels directly affect the ability to assess speciﬁc systems, while
diverse backgrounds help identify evaluation blind spots. Without this information, read-
ers cannot judge whether auditors possessed the necessary skills to thoroughly evaluate
the system or what perspectives might be underrepresented in the analysis.
Relevance across contexts: Basic expertise documentation is essential for all evaluation
types. Red-teaming speciﬁcally requires specialized adversarial thinking skills and security
expertise. In user studies, auditor social positionality becomes particularly important as it
may inﬂuence how human-AI interactions are interpreted.
What is evaluated?
Summary:
Deﬁnes evaluation scope, goals, and approach by specifying system ver-
sion, components, scaffolding used, threat models addressed, and whether capabilities or
propensities were evaluated. Includes continuous monitoring plans and obsolescence cri-
teria.
Why it matters: Clear boundaries prevent misinterpretation of results. Understanding
threats, contexts, and capability vs. propensity assessment helps readers gauge relevance
to their speciﬁc concerns. Without speciﬁc scope information, readers might incorrectly
assume the evaluation covers more than it does, while obsolescence criteria help prevent
outdated evaluations from being applied to newer system versions.
Relevance across contexts: Evaluation scope and goal speciﬁcation are critical across all
evaluation types. Benchmarks particularly require clear documentation of obsolescence
criteria as AI capabilities rapidly evolve. Safety assessments beneﬁt from explicit threat
models tailored to deployment context. Human-centered evaluations need clear articu-
lation of intended user groups and use cases, while deployed systems require speciﬁed
intervals and conditions for re-evaluation.
How is it evaluated?
19
Preprint. Under review.
Summary:
Describes methodology, implementation details, and result interpretation
guidance.
Explains connections between evaluation methods and capabilities, justiﬁes
capability-to-goal translations, and outlines ﬁnding limitations. Reporting on this feature
can be very extensive, as the speciﬁc details for reporting depend on the exact evaluation
procedure used. We refer to existing literature on best practices for reporting on evaluation
procedures, such as Reuel et al. (2024b); Apollo Research (2024).
Why it matters: Methodology directly impacts result validity and reliability. Understand-
ing how capabilities become measurable criteria and how scores translate to real-world
performance prevents misapplication of evaluation outcomes. Without methodological
transparency, readers cannot assess whether the evaluation approach actually measures
what it claims to measure or understand the conﬁdence level appropriate for various ﬁnd-
ings.
Relevance across contexts: A description of methodology is essential for all evaluation
types. Benchmarks speciﬁcally require detailed score interpretation guidance to prevent
misapplication of numerical results. Propensity evaluations need robust justiﬁcation for
capability-to-goal translations since they often use proxy measurements. Novel system
types need clearer justiﬁcation for how evaluation approaches measure intended capabili-
ties.
Access and resources
Summary: Details auditor system access level (black/gray/white-box, with/without safe-
guards), provided documentation, and available computational, time, and ﬁnancial re-
sources.
Why it matters: Resource constraints and access limitations impact evaluation thorough-
ness and depth. Limited system access or insufﬁcient resources may prevent auditors from
discovering certain vulnerabilities or testing edge cases. Transparency about these con-
straints helps readers understand which areas received less scrutiny and what types of
issues the evaluation might have missed due to practical limitations.
Relevance across contexts: Basic access level information is relevant to all evaluations.
Red-teaming ﬁndings can differ dramatically between black-box and white-box conditions.
Large-scale benchmarks particularly require computational resource documentation.
Integrity
Summary: Addresses auditor selection process, potential “opinion shopping,” conﬂicts of
interest, compensation structures, contract terms, and NDAs. Documents measures ensur-
ing diverse perspectives.
Why it matters: Selection processes and incentive structures reveal potential inﬂuences on
outcomes. Auditors with ﬁnancial ties to the evaluated organization may face pressure to
produce favorable results, while restrictive NDAs might limit disclosure of critical ﬁndings.
Transparent disclosure of relationships and contractual limitations builds trust in ﬁndings
and allows readers to identify potential sources of bias.
Relevance across contexts: Integrity documentation and conﬂict of interest disclosure is
universally important across all evaluation contexts. Selection process transparency be-
comes critical for high-stakes evaluations where ﬁndings may signiﬁcantly impact deploy-
ment decisions. Non-disclosure agreement details matter most when evaluations involve
proprietary information that might limit the disclosure of certain ﬁndings. Disclosure of
prior relationships between auditors and developers matters more for external evaluations
to establish credibility and independence.
Review and communication
Summary: Outlines evaluation review process, feedback channels, executive summary
quality, key term deﬁnitions, and result distribution. Focuses on unambiguous key ﬁnd-
ings and consistent terminology.
Why it matters: Robust review strengthens ﬁndings’ reliability by catching errors and bi-
ases. Clear feedback channels enable continuous improvement when new information
20
Preprint. Under review.
emerges or vulnerabilities are discovered after publication. Well-deﬁned terminology en-
sures consistent understanding across different stakeholders, preventing miscommunica-
tion about technical concepts and evaluation outcomes that could lead to inappropriate
system deployment decisions.
Relevance across contexts: Robust review processes are essential across all evaluation
types to ensure reliability and catch potential errors or biases. Interdisciplinary evalua-
tions require clear deﬁnitions of domain-speciﬁc terminology. Security-sensitive applica-
tions need explicit documentation of which ﬁndings are shared with which audiences.
C.2
Checklist for audit card
This checklist provides a framework for documenting contextual information about AI sys-
tem audits. Questions are prioritized with the most important appearing ﬁrst in each sec-
tion, though not all questions in a section will be relevant to every evaluation. The checklist
serves as a ﬂexible guide rather than a rigid requirement. More comprehensive informa-
tion generally enhances transparency. Through use of the audit card checklist, it is easier to
document evaluation contexts, limitations, and strengths to facilitate informed discussions
about AI system capabilities and risks.
□Who are the auditors
□Expertise:
• What speciﬁc domain expertise (e.g. cybersecurity knowledge) do the auditors
possess that is relevant to this evaluation?
• What relevant experience do the auditors have in evaluating similar systems?
• What is the auditors’ familiarity with the speciﬁc technology being evaluated?
• Do the auditors hold ofﬁcial accreditations or certiﬁcations relevant to this eval-
uation?
• Have the auditors undergone any training speciﬁc to this evaluation (internal or
external)?
□Background:
• What stakeholder perspectives do the auditors represent or have experience
with?
• What is the social positionality of the auditors (aggregated demographic infor-
mation, cultural background)?
□What is evaluated
□Scope:
• Which speciﬁc version or iteration of the system is being evaluated?
• What speciﬁc components of the system are included in the evaluation?
• What aspects of the system were explicitly excluded from the evaluation scope?
• Does the evaluation cover the full system or only speciﬁc functionalities?
• What scaffolding (e.g. chain-of-thought prompting, few-shot demonstrations)
or additional tools (e.g. web or command line access) were used during the
evaluation?
□Goal:
• What threat models or risk scenarios is the evaluation addressing?
• What speciﬁc harms or misuses is the evaluation attempting to identify?
• What are the speciﬁc contexts (e.g. user groups and use cases) in which the AI
system will be deployed?
• What are the performance expectations in the intended contexts?
□Type of evaluation:
• Is the evaluation assessing capabilities, propensities, or risks?
• What form of evaluation methodology is being used (benchmark, red-teaming,
human uplift studies, or user studies, etc.)?
□Continuous evaluations:
21
Preprint. Under review.
• Will the system be subject to ongoing monitoring after this evaluation?
• At what intervals will follow-up evaluations occur?
□Obsolescence criteria:
• What changes (system updates, deployment context, discoveries about similar
systems, or technological advances) would render this evaluation obsolete?
• How long is this evaluation expected to remain relevant without updates?
□How is it evaluated
□Description of evaluation setup:
• What general evaluation methodology was employed?
• What were the key components of the evaluation design?
• What evaluation framework or established protocol was followed (if applica-
ble)?
□Justiﬁcation of capability-to-goal translation:
• How does the evaluation approach align with the capabilities or risks being as-
sessed?
• How do the evaluation procedures relate to the system’s intended use contexts?
• What is the rationale for how the chosen methods measure the targeted capabil-
ities?
□Interpretation of scores:
• How should the evaluation results be interpreted?
• What limitations should be considered when interpreting the ﬁndings?
• What do different performance levels indicate about the system?
□Access and resources
□Level of access:
• Did the auditor have black-box, gray-box, or white-box access?
• What speciﬁc system documentation was provided to auditors, including access
to system code, architecture, design documents, training data, or previous eval-
uation results?
• Were any aspects of the system explicitly restricted from auditor access?
□Resources available:
• What computational resources were allocated to the evaluation?
• How much time was allocated for the full evaluation cycle?
• Was the evaluation timeline sufﬁcient for comprehensive testing?
• How many auditors were involved and what was their time commitment?
• What specialized tools or software were available to the auditors?
• Were resources sufﬁcient to conduct all planned evaluation activities?
• What resource constraints limited the scope or depth of the evaluation?
□Integrity
□Selection process:
• How were auditors identiﬁed and recruited for this evaluation?
• What criteria were used to select auditors?
• What measures were taken to ensure diversity of perspectives among auditors?
• Was there an open call or nomination process for auditors?
• Were independent third parties involved in the selection process?
□Conﬂicts of interest:
• What relationships exist between auditors and the organization being evalu-
ated?
• Have auditors previously received compensation from the evaluated organiza-
tion?
• Do auditors have ﬁnancial interests or competing interests related to the evalu-
ated system?
• What measures were taken to mitigate potential conﬂicts of interest?
• Were potential conﬂicts of interest publicly disclosed?
22
Preprint. Under review.
□Compensation and incentive structures:
• What was the compensation structure for auditors (ﬁxed fee, hourly, etc.)?
• What contractual limitations were placed on auditors (non-disclosure agree-
ments, etc.)?
• Were there incentives for identifying system ﬂaws or vulnerabilities?
• Were auditors employed internally or contracted externally?
• Were there any performance-based incentives that might bias results?
□Review and communication
□Review:
• What aspects of the evaluation (e.g. methodology, report, ﬁndings) were sub-
jected to review?
• Who reviewed these aspects, and what was their relationship to the auditors and
system developers?
• What expertise did the reviewers bring to the process?
• How were reviewer disagreements resolved and what changes resulted from the
review process?
□Feedback channels:
• What speciﬁc mechanisms exist for providing feedback on the audit?
• How can new information be submitted for consideration after publication?
• Who should be contacted with questions or concerns about the evaluation?
• Is there a plan for addressing follow-up questions about the results?
□Executive summary:
• Does the summary clearly explain what was evaluated, how, and what was
found?
• Are limitations of the evaluation explicitly stated?
• Is the summary accessible to non-technical audiences?
□Deﬁnitions of key terms:
• Are technical terms and evaluation metrics clearly deﬁned?
• Is terminology consistent with industry standards?
□Access to results:
• Who will receive which parts of the evaluation ﬁndings?
• How and when will results be published or distributed?
• What information, if any, will be redacted from public versions?
Appendix D
Selection of governance documents
We chose the relevant governance documents to include in the analysis by considering the
following three factors:
1. Major jurisdiction: broadly selected by geopolitical and economic centrality, espe-
cially in the AI industry (Stanford HAI staff, 2024; Bianzino et al., 2023).
2. Authority of the institution: within each jurisdiction, we chose government and
non-government institutions. METR was the exception because their evaluation
reports have been highly inﬂuential in the ﬁeld, and often involve co-authorship
from researchers in scaling labs and AISIs.
3. Direct inﬂuence on model evaluations. For model developers, these tended to
be their Responsible Scaling Policies or Frontier Safety Frameworks (see METR,
a). For government bodies, we selected documents that tracked with more scale
and inﬂuence (i.e. more national/federal than local, or more binding than not).
The EU Code of Practice is a unique outlier in not being strictly binding but in ef-
fect being as inﬂuential as it enables presumption of conformity with the Act (see
European Union and EU monitor).
To enable the governance documents to ﬁt Table 3, we abbreviated the names of the docu-
ments. Here, we present their ofﬁcial names.
23
Preprint. Under review.
• Ofﬁcial regulations
– Brazil:
We
analyzed the
Federal Senate Bill
No.
2338/2023 (ap-
proved by Senate in 2024, but pending further steps to become law).
(Brazilian Federal Senate, 2023)
– China: Governance of AI models and systems is detailed across various docu-
ments, primarily the: Draft AI Law (with scholars comments, translated May
2024, mur); Internet Information Service Algorithmic Recommendation Man-
agement Provisions (with effect from 2022, Creemers et al. (2022)); Measures
for the Management of Generative Artiﬁcial Intelligence Services (Draft for
Comment) 2023 (Huang et al., 2023)); China 2022 rules for deep synthesis (Gu,
2023)); and China (Draft Measure on Ethical Review).
– EU: We included the keystone AI Act (EU AI Act, 2024) with its accompanying
Code of Practice (“CoP”) (European Commission, 2025): The EU AI Act needs
to be read in conjunction with the Code of Practice. The Code of Practice
is a unique type of document which, while technically not legally binding,
enables presumption of conformity with the EU AI Act. In effect, due to the
legal uncertainty of not knowing what alternatives would comply, it is likely
to be followed by those wishing to be compliant with the EU AI Act, resulting
in effectively a binding effect until “harmonized standards” are published.
– Singapore: We analyzed the Model AI Governance Framework, which is not
technically binding but authoritative guidance. Model AI Governance Frame-
work for Generative AI (Singapore, 2020)
– South Korea:
We analyzed the “Basic Act on the Development of Ar-
tiﬁcial Intelligence and Establishment of Foundation for Trust”, which
comes into force Jan 2026.
Governance of AI models and systems
appears to be done by a keystone AI Act (with effect from 2026)
(National Assembly of the Republic of Korea, 2024).
– US: Given the lack of a US federal-wide AI regulation, we examined President
Biden’s Executive Order (The White House, 2023), which provided the lead-
ing signal as to how the US might think about governing model evaluations
and risk assessments. It is now repealed, but part of its legacy remains with
the highlighted importance of technical assessments and the NIST AI Risk
Management Framework.
• Other norms
– Bletchley Declaration (November 2023)
– EU Auditing Framework
– International AI Safety Report (January 2025)
– Japan AISI Guide to Evaluation Perspectives (September 2024)
– METR (Key Components of an RSP)
– NIST AI Risk Management Framework
– Paris AI Action Summit (February 2025)
– Seoul AI Summit (May 2024)
– UK AISI guidance
• AI company policies
– Anthropic: Responsible Scaling Policy (Anthropic, 2024)
– Google DeepMind: Frontier Safety Framework v2 (Google, 2025)
– Meta: Frontier AI Framework v1.1 (Meta, 2025)
– Microsoft: Frontier Governance Framework (Microsoft, 2025)
– OpenAI: Preparedness Framework Beta (OpenAI, 2023)
– xAI: Draft Risk Management Framework (xAI, 2025)
24
Preprint. Under review.
Appendix E
Features underspeciﬁed by current EU Code of Practice
stipulations about the Model Report
The EU Code of Practice (“CoP”), if ratiﬁed, will be the default way to comply with the AI
Act12. As of its Third Draft version, the Code’s Safety and Security section Commitment
II.8 details what Model Reports to be submitted to the AI Ofﬁce are to include. It covers
many aspects, including what our paper calls the core principles and speciﬁc features (ex-
pertise and background of auditors, scope, methodology, and resources involved in model
evaluations). However, it does not specify that the Model Report itself must contain in-
formation about channels for external review and feedback for the report, or integrity
(conﬂicts of interest) about the process behind the creation of that report.
These exclusions may be understandable due to varying demands and industry concerns
of heavy-handedness (Haeck, 2025). Still, it is worth noting as it allows a clear conversation
of whether these issues should be codiﬁed, where that code should live, and whether more
jurisdictions should adopt this approach (Bradford, 2020). These views will have to be
updated with regulatory shifts.
Feature: Review and communication While the AI Ofﬁce is to have access to key pieces of
information like the Model Report, unredacted frameworks, models and evaluation results,
channels for non-AI Ofﬁce peer review are not spelled out. There are multiple candidates
for who counts as a relevant “peer” (other model auditors, companies, researchers), and
issues, including who would be a trusted coordinator of these interactions, different ac-
tors’ contexts and timelines for work, preventing test data leakages and evaluation-gaming.
Ironing out the difﬁculties of this implementation is the subject of future work.
Feature: Integrity (Conﬂicts of interest) It appears to require appropriate checks for con-
ﬂicts of interest (Safety and Security section glossary deﬁnition of “independent external”)
when external auditors are involved, but not the explicit reporting of this in the submitted
Model Report.
While we have not included this as part of our audit cards, we note other experts call-
ing for good publishing norms for evaluation reports, which includes: evaluations must
be reported per risk and in relation to any risk identiﬁcation and mitigation efforts by
the developer, why certain statistical validity measures were chosen, and how meaning-
ful are the differences between newly tested versus older models or human baselines (see
interpretation and quantitative results in Reuel et al. (2024b); Salaudeen et al.). This non-
conclusive list are ways to help others understand why an evaluation ﬁnding matters,
making it harder to take an evaluation out of context, and easier for a wider community to
understand existing coverage of risk assessments, plan for future assessments and how to
improve them.
Appendix F
Stakeholder interviews
F.1
Interviewee details
F.2
Interview guide
The following is our interview guide and set of questions we used for the 30-minute semi-
structured interview with each interviewee.
Introductions [3 mins]
• Introduce our project: We deﬁned what audit cards are, what we did and did not
include in that scope
12In theory, model providers can showcase compliance through other means that need to be shown
to be equivalent to the Code of Practice. However, given legal uncertainty, this presents a less likely
avenue for model developers.
25
Preprint. Under review.
Participant
Organization
Role
P1
Third-party evaluator
Evaluation designer, developer & writer
P2
[anonymized]
[anonymized]
P3
Scaling lab
Evaluation developer
P4
Third-party evaluator
Evaluation developer
P5
Third-party evaluator
Evaluation developer & writer
P6
[anonymized]
Evaluation policy researcher
P7
Third-party
Evaluation policy researcher
P8
Government
Evaluation policymaker & writer
P9
Third-party evaluator
Evaluation development operations support
P10
[anonymized]
[anonymized]
Table 5: Overview of the stakeholders interviewed for the semi-structured interviews. The
organization and role has been reported up to the level of detail interviewees were com-
fortable with. Note: Participants P2 and P10 were excluded from the ﬁnal analysis as their
interviews were conducted off the record and attributions could not be conﬁrmed in time
for publication.
– Audit cards are a way to standardize reporting of AI evaluation process, with
the goal of contextualizing what evals can do and transparency about the pro-
cess
– We state examples of what we include: why these metrics, who are the au-
ditors, resources available (compute, time) during eval, how they’re selected
and potential COI
– We disambiguate them from examples we don’t include: speciﬁc metrics, tech-
nical implications, elicitation techniques;
• Outline process for interview: We walked interviewees through what to expect of
the interview
– Contribution: Used to color in details about evals process from stakeholders.
– Privacy and permissions: We checked beforehand what level of recording, at-
tribution, and other process checks they would be comfortable with. We con-
ﬁrmed nothing they said to us will be shared to others without it being run
past them and them conﬁrming in writing.
General questions to ask everyone [10 mins max]
1. Describe the work you do in relation to evals, try to be speciﬁc about your respon-
sibilities, identities of people/orgs you relate to, your role in the evals ﬁeld
2. Who is the evaluation report you [build/analyze/use] for? Who is the target audi-
ence? E.g. developers (internal, external), research, governance, or also the general
public.
3. What part of the evaluation process which, if not done well/reported well, would
make you doubt the quality of the eval?
4. How do imagine evaluation best practices best becoming reality?
Do you see
codiﬁcation—whether in law or Industry norm—as a good/valid way to quality
control?
5. In your view, what are the biggest challenge preventing evals from being better?
More useful for improving safety, or whatever their key goal is?
Questions to ask speciﬁc categories of people [12 mins]
Evaluation developer
6. Do you consider the limitations and assumptions of your evals process? Where
does that thinking get captured (if it does)?
26
Preprint. Under review.
7. What is the background/expertise of auditors—does it depend on the type of eval-
uation or something else, and who makes that decision?
8. Can you give us examples of evals where the evals process differed and whether it
affected the quality of eval?
9. Do you follow any internal best practices?
10. What is the current training process both internally (with evaluation developers,
human baseliners) and with your engagement partner?
Evaluation report writer
11. What is the most important information to share in the evaluation report? How do
you decide that?
(a) Can you walk through two situations in which you decided differently (i.e.
type of info/detail level to publish and why)?
(b) What’s the right balance of transparency in reporting evals, and what risks
surround the achieving of that?
12. From our skim of evaluation reports, we found these tend to be underspeci-
ﬁed/not speciﬁed—why do you think that might be? e.g. assumptions, auditors’
background, integrity and resources of process, peer review, commitments to take
action based on evaluation results, and state criteria that make evaluation obsolete
Evaluation development overseer/manager & translate it to policy people
13. Do you consider the limitations and assumptions of your evals? Where does that
thinking get captured (if it does)?
(a) e.g. speciﬁc downstream application context on which the benchmark is con-
tingent;
14. How do you select the right auditors (external / internal)? Walk through a recent
eval’s selection?
15. How much autonomy does your organization get in deciding access/resources
(e.g. time/compute) for evaluations [of a lab’s models]? What does this depend
on?
(a) Do you include different stakeholders in the evals process—who are they, and
how are they engaged?
16. What is the current training process both internally (with evaluation developers,
human baseliners) and with your engagement partner?
17. What is the right balance of ﬂexibility and speciﬁcity in regulation? E.g. what
makes an auditor ’qualiﬁed’ (background/training), conﬂict of interest, fund-
ing/contract structures
18. If you wanted to, how easy would it be for you to be misleading when communi-
cating evaluation results? Why?
Evaluation operations support (decisions getting access to resources that evaluation devel-
opers need)
19. What resources do you usually require for an eval, and what inﬂuences that? Do
you document resource requirements, and where do you share this?
20. What are the conditions of you being able to access them? E.g. Recent Cyber CBRN
Agent evals report used blue/red anonymized models at UK AISI
21. Could you describe the different engagement processes, and why some have been
easier than others?
Evaluation used for recommendations - policy researcher/think tank, funders, lobbying,
advocacy
27
Preprint. Under review.
22. If you have had to use an evaluation report (could be your org or another org.) to
make a recommendation, what features of the report have helped you to do so?
23. Do you have examples of/from evaluation reports you found good and/or bad,
and why?
24. How much do you trust evaluation results depending on the funding source (e.g.
internal different labs OpenAI, external: Apollo, AISI)
AI safety communicators
25. How do you understand evals (as distinct from audits) and what type of evalua-
tion reporting/communication are you aware of?
26. What insights are people most often seeking from evals, and how are evals (not)
meeting that need?
27. Re target audience question: Why do you think [previous answer] is the relevant
audience? Other relevant audience segments, how does the messaging change?
How can evals most strongly communicate their goal?
28

==================================================

==================================================
Paper 2
Title: Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently
demonstrated notable success in enhancing the reasoning capabilities of LLMs,
particularly in mathematics and programming tasks. It is widely believed that
RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning
abilities that exceed corresponding base models' capacity. In this study,
however, we critically re-examines this assumption by measuring the
pass@\textit{k} metric with large values of \textit{k} to explore the reasoning
capability boundary of the models across a wide range of model families and
benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally
new reasoning patterns. While RL-trained models outperform their base models at
smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or
even higher pass@$k$ score compared to their RL counterparts at large $k$
values. The reasoning paths generated by RL-trained models are already included
in the base models' sampling distribution, suggesting that most reasoning
abilities manifested in RL-trained models are already obtained by base models.
Further analysis shows that RL training boosts the performance by biasing the
model's output distribution toward paths that are more likely to yield rewards,
therefore sampling correct responses more efficiently. But this also results in
a narrower reasoning capability boundary compared to base models. Similar
results are observed in visual reasoning tasks trained with RLVR. Moreover, we
find that distillation can genuinely introduce new knowledge into the model,
different from RLVR. These findings underscore a critical limitation of RLVR in
advancing LLM reasoning abilities which requires us to fundamentally rethink
the impact of RL training in reasoning LLMs and the need of a better paradigm.
Project Page: https://limit-of-RLVR.github.io
PDF URL: http://arxiv.org/pdf/2504.13837v1

Full Content:
April 21, 2025
Does Reinforcement Learning Really Incentivize Reasoning
Capacity in LLMs Beyond the Base Model?
Yang Yue 1 ∗† ,
Zhiqi Chen 1 ∗,
Rui Lu 1 ,
Andrew Zhao 1 ,
Zhaokai Wang 2 ,
Yang Yue 1 ,
Shiji Song 1 , and Gao Huang 1 B
1 LeapLab, Tsinghua University
2 Shanghai Jiao Tong University
∗Equal Contribution
† Project Lead
B Corresponding Author
Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable
success in enhancing the reasoning capabilities of large language models (LLMs), particularly
in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to
continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding
base models’ capacity.
In this study, however, we critically re-examines this assumption
by measuring the pass@k metric with large values of k to explore the reasoning capability
boundary of the models across a wide range of model families, RL algorithms and math/coding
benchmarks. Surprisingly, RLVR training does not, in fact, elicit fundamentally new reasoning
patterns. We observed that while RL-trained models outperform their base models at smaller
values of k (e.g., k=1), base models can achieve a comparable or even higher pass@k score
compared to their RL counterparts at large k values. Further analysis shows that the reasoning
paths generated by RL-trained models are already included in the base models’ sampling
distribution, suggesting that most reasoning abilities manifested in RL-trained models are
already obtained by base models. RL training boosts the performance by biasing the model’s
output distribution toward paths that are more likely to yield rewards, therefore sampling
correct responses more efficiently. But this also limits their exploration capacity, resulting in a
narrower reasoning capability boundary compared to base models. Similar results are observed
in visual reasoning tasks trained with RLVR. Moreover, we find that, different from RLVR,
distillation can genuinely introduce new knowledge into the model. These findings underscore
a critical limitation of RLVR in advancing LLM reasoning abilities, which requires us to
rethink the impact of RL training in reasoning LLMs and the need of a better training paradigm.
Project Page: https://limit-of-RLVR.github.io
1. Introduction
The development of reasoning-centric large language models (LLMs), such as OpenAI-o1 (Jaech et al.,
2024), DeepSeek-R1 (Guo et al., 2025), and Kimi-1.5 (Team et al., 2025), has significantly advanced
the frontier of LLM capabilities, particularly in solving complex logical tasks involving mathematics
and programming. In contrast to traditional instruction-tuned approaches that rely on human-curated
annotations (Achiam et al., 2023; Grattafiori et al., 2024), the key driver behind this leap forward is
large-scale Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024; Guo et al.,
2025). RLVR starts with a pretrained base model or one fine-tuned on long chains of thought (CoT)
data, optimizing it via reinforcement learning based on simple, automatically computable rewards.
Correspond to: {le-y22, zq-chen23}@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn.
arXiv:2504.13837v1  [cs.AI]  18 Apr 2025
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
More 
Efficient Sampling
0
0
0
0
0
+1
0
+1
Base Model
0
+1
0
+1
RLVR Model
0
0
0
0
Reduced Scope of 
Reasoning Capacity
+1
0
0
0
Problem A
Problem B
+1
0
0
0
1
2
4
8 16 32 64 128256
Number of Samples k
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Coverage (pass@k)
Omni-MATH-Train
Qwen2.5-7B
GRPO-step150
GRPO-step300
GRPO-step450
Figure 1: (Left) The effect of RLVR on LLM’s reasoning ability. Search trees are generated by repeated sampling from the
base and RLVR-trained models for a given problem. Grey indicates paths that are unlikely to be sampled by the model,
while black indicates paths that are likely to be sampled. Green indicates correct paths, which has positive rewards. Our
key finding is that all reasoning paths in the RLVR model are already present in the base model. For certain problems like
Problem A, RLVR training biases the distribution toward rewarded paths, improving sampling efficiency. However, this
comes at the cost of reduced scope of reasoning capacity: For other problems like Problem B, the base model contains the
correct path, whereas that of the RLVR model does not. (Right) As RLVR training progresses, the average performance
(i.e., pass@1) improves, but the coverage of solvable problems (i.e., pass@256) decreases, indicating a reduction in the
model’s reasoning upper bound.
These rewards are determined by whether the model’s output matches a ground-truth solution in
mathematics or passes unit tests in code, thus enabling scalability without human labeling. This
framework has gained significant attention due to its simplicity and practical effectiveness. It is widely
believed that RLVR enables models to autonomously incentivize advanced reasoning behaviors, such as
enumeration, self-reflection, and iterative refinement, which are not present in base models (Guo et al.,
2025). Consequently, RLVR is viewed as a pathway to self-evolving LLMs with continuously expanding
reasoning capabilities, potentially advancing us closer to stronger intelligence (Guo et al., 2025).
However, despite the empirical successes of RLVR, a critical question remains in the pursuit of continually
self-evolving reasoning abilities:
Does RLVR really bring novel reasoning capabilities to LLMs? If so, what does the model
learn from RLVR training?
In order to rigorously answer this question, we first need to determine the boundary of reasoning
capability of base and RL-trained models. Traditional metrics use single-pass success rates or average
nucleus sampling (Holtzman et al., 2019) to measure average-case performance. However, in our work,
we highlight an important oversight behind such metrics: a model’s true reasoning potential may be
underestimated if it fails on difficult problems after only a few attempts, even if it could succeed with
more sampling. To address this, we employ a simple method, namely the pass@k metric (Brown et al.,
2024), where a problem is considered solved if any of the k samples is correct. The idea is simple:
What if we invest heavily in sampling for the base model with an enormous k? Can its performance
match the RLVR-trained one? By giving the model a large number of trials to solve a problem, we
are able to evaluate the boundary of reasoning capabilities of both base and RL-trained models. This
provides a critical and rigorous test on whether the RLVR training yields fundamentally transcending
capacity—enabling the model to solve problems that the base model cannot.
We conduct extensive experiments across diverse benchmarks, including mathematics, code generation,
and visual reasoning, spanning multiple LLM families, model sizes, and RL algorithms. Using the pass@k
metric, we compare the performance of base models with their RL-trained counterparts. We find several
perhaps surprising discoveries that may alter the conventional understanding of reasoning models and
RLVR training, as follows:
• RLVR-trained models perform worse than base models in pass@k at large k values.
Although RL-trained models consistently outperform their base models in small k as shown in previous
2
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
work (Guo et al., 2025), it is highly surprising that base models consistently surpass RL-trained models
across all benchmarks and LLM families as k increases. More notably, at sufficiently large k, base
models achieve even higher pass@k scores than their RL-enhanced counterparts. This means that the
base model without any RL training can already generate the correct answers to problems that were
previously considered only solvable for RL-trained models by aggressively sampling different approaches.
We manually check the CoTs of these correct answers and find most of problems has at least one CoT to
be correct. This suggests that RL training does not improve, but even downgrade, LLMs’ potential
scope of reasoning capability.
• RLVR boosts sampling efficiency but reduces the scope of reasoning capacity. To further
investigate this phenomenon, we conduct perplexity analysis. Our findings show that reasoning paths
generated by RLVR-trained models already exist within the base models’ output distribution with
considerable probability density, indicating that these reasoning patterns and CoTs are not totally
strange and unachievable for base models. RLVR training contributes to reasoning by biasing the model
toward rewarded reasoning paths, thereby increasing the likelihood of sampling correct reasoning paths.
However, this efficiency gain also comes at a cost: RL training reduces the model’s exploration capacity,
resulting in smaller coverage of solvable problems at large k (see Figure 1 right), suggesting its reduced
scope of reasoning capacity. This challenges the common belief that RLVR elicits transcending reasoning
ability. Instead, the reasoning capacity boundary of RLVR-trained models may be bounded by the
capabilities of base model. The effect of RLVR on LLM’s reasoning ability is illustrated in Figure 1 left.
• RLVR algorithms perform similarly and remain far from optimal. Although different RL
algorithms (e.g., PPO, GRPO, Reinforce++) show slight variations in performance, it does not make
essential difference. As shown in Figure 7 top, we define the difference between the RL model’s pass@1
and the base model’s pass@k (i.e., an potential performance upper-bound; we use k = 256) as the
sampling efficiency gap (∆SE), which quantifies how effectively an RL algorithm approaches optimal
sampling efficiency. We found that different RL algorithms produce only slight variations in ∆SE, yet
the gap remains consistently large across all methods. This suggests that current RL approaches, which
primarily work by improve sampling efficiency, are still far from optimal.
• RLVR and distillation are fundamentally different. While RL improves sampling efficiency,
distillation can genuinely introduce new knowledge into the model. As a result, distilled models often
exhibit an expanded scope of reasoning capability beyond that of the base model by learning from
distilled models, in contrast to RLVR-trained models whose capacity remains bounded by the base.
In conclusion, our results collectively indicate that RLVR, in its current form, is insufficient to incentivize
transcending capabilities beyond the base model’s limits. Our work not only discovers findings that
fundamentally challenge previous understanding on RLVR and its impact in reasoning models, but also
implies that RLVR in itself might be inadequate to push forward the boundary of reasoning abilities.
2. Preliminaries
In this section, we first introduce the fundamentals of RLVR. We then provide a detailed explanation of
pass@k, the metric used to evaluate the reasoning boundary.
2.1. Reinforcement Learning with Verifiable Rewards
Verifiable Rewards. Let πθ be an LLM with parameters θ that generates a token sequence y =
(y1, . . . , yT ) conditioned on a natural-language prompt x. A deterministic verifier V returns a binary
reward: r = V(x, y) ∈{0, 1}, where r = 1 if and only if the model’s final answer is exactly correct (e.g.,
the numeric result matches the ground truth in math, or all unit tests pass in code). A format reward may
also be added to encourage the model to explicitly separate the reasoning process from the final answer.
The goal of RL is to learn a policy to maximize the expected reward: J(θ) = Ex∼D

Ey∼πθ(·|x)[ r ]

,
where D is the distribution of prompts.
RLVR algorithms. Proximal Policy Optimization (PPO) (Schulman et al., 2017) proposed using the
3
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
1
4
16
64
256 1024
Number of Samples k
0
0.2
0.4
0.6
0.8
1
Coverage (pass@k)
Base
RL
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
2
4
8 16 32 64 128
0.8
0.85
0.9
0.95
1
1
2
4
8 16 32 64 128
0.8
0.85
0.9
0.95
1
1
2
4
8 16 32 64 128
0.8
0.85
0.9
0.95
1
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
2
4
8 16 32 64 128
0
0.2
0.4
0.6
0.8
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
AIME24
AMC23
MATH500
Minerva
GSM8K
Olympiad
Qwen-2.5-7B
Qwen-2.5-14B
Qwen-2.5-32B
LLaMA-3.1-8B
Figure 2: Pass@k curves of base models and their zero-RL-trained counterparts across multiple mathematical benchmarks.
When k is small, RL-trained models outperform their base versions. However, as k increases to the tens or hundreds,
base models consistently catch up with RL-trained models across all benchmarks and LLM families without exception.
Eventually, base models surpass RL-trained models.
following clipped surrogate to maximize the objective:
LCLIP = E [min(rt(θ)At, clip(rt(θ), 1 −ϵ, 1 + ϵ)At)] ,
(1)
where rt(θ) = πθ(yt|x, y<t)/πθold(yt|x, y<t), and At is the advantage estimated by a value network Vϕ.
A KL divergence term is also applied, either as a reward penalty or directly in the loss function, to
4
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
constrain the model from deviating too far from the original policy.
To reduce memory and computational overhead, several critic-free variants have been proposed.
GRPO (Shao et al., 2024) estimate the advantage with a normalized reward within a group of re-
sponses to the same question: Ai = [ri −mean(r)]/std(r), where r = {r1, . . . , rG} denotes the set
of rewards for a group of G sampled responses. RLOO (Ahmadian et al., 2024) instead adopts a
leave-one-out baseline within each batch B. Its adavantage is defined as Ai = ri −
1
|B|−1
P
j̸=i rj.
Policy Gradient. PPO and its variants belong to the policy gradient class of reinforcement learning
(Williams, 1992; Sutton et al., 1998). These methods learn exclusively from on-policy samples, i.e.,
samples generated by the current LLM. In the context of verifiable binary rewards, the training objective
is equivalent to maximizing the log-likelihood of samples with correct answers and minimizing the likelihood
of those with incorrect answers.
Zero RL Training applies reinforcement learning directly to the base model without any supervised
fine-tuning (SFT) on long CoT reasoning data (Guo et al., 2025). To clearly study the effect of RLVR,
we follow this zero-RL setting for all math tasks using open-source base models. However, for coding
and visual reasoning tasks, no previous open-source work adopts a strict zero-RL setting, mainly due
to training instability and limited effectiveness. Instead, these tasks typically use instruction-tuned
models as starting points. We follow this convention, which is reasonable for studying the effect of RL on
reasoning ability, as these models are primarily fine-tuned on general instruction-following data rather
than long-form CoT reasoning data.
2.2. Metrics for LLM Reasoning Capacity Boundary
Accurately measuring the reasoning ability boundary of base and RL models is challenging, as methods
like single-pass greedy decoding or the average of a few rounds of nucleus sampling (Holtzman et al.,
2019) reflect average-case performance rather than the model’s upper bound. For example, consider a
traditional avg@8 approach, where 8 responses are sampled for an extremely difficult problem. If none
of the responses are correct, the value is 0/8. However, after more trials such as 128, the model may
solve the problem, indicating that the reasoning boundary can eventually overcome this challenge.
To accurately measure the reasoning ability boundaries of LLMs, we extend the commonly used pass@k
metric from code generation (Chen et al., 2021) to all tasks with verifiable rewards. Given a problem,
we sample k outputs from the model. The pass@k value for this question is 1 if at least one of the k
samples passes verification; otherwise, it is 0. The average pass@k value over the dataset reflects the
proportion of problems in the dataset that the model can solve within k trials, providing a rigorous
evaluation of the reasoning capacity coverage of LLMs.
Directly computing pass@k using only k sampled outputs per problem can lead to high variance. To
mitigate this, we follow the unbiased estimation method proposed by Chen et al. (2021). Specifically, for
each problem xi from the evaluation dataset D, we generate n samples (n ≥k) and count the number of
correct samples as ci. The unbiased estimator of pass@k over the dataset is given by:
pass@k := Exi∼D
"
1 −
 n−ci
k

 n
k

#
(2)
With this formulation, we can easily estimate pass@k with low variance across all k ≤n.
For coding tasks, where a compiler and predefined unit test cases are used as verifiers, the pass@k value
can accurately reflect whether the model can solve the problem. However, the issue of “hacking” can
become pronounced in mathematics as k increases. In math problems, a model may generate an incorrect
chain of thought but still accidentally arrive at the correct answer in one of many samplings, a scenario
that is often overlooked by previous metrics. To address this, we filter out problems that are easily
“hacked” and then manually check the correctness of CoT for a subset of model outputs. By combining
these measures, we rigorously evaluate the limit of LLM’s reasoning capacity as detailed in Section 3.1.
5
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
Table 1: Experimental setup for assessing RLVR’s effect on the reasoning boundaries of LLMs across
different tasks.
Task
Start Model
RL Framework
RL Algorithm(s)
Benchmark(s)
Mathematics
LLaMA-3.1-8B
Qwen-2.5-7B/14B/32B-Base
Qwen-2.5-Math-7B
SimpleRLZoo
Oat-Zero
GRPO
GSM8K, MATH500
Minerva, Olympiad
AIME24, AMC23
Code Generation
Qwen-2.5-7B-Instruct
Code-R1
GRPO
LiveCodeBench
HumanEval+
Visual Reasoning
Qwen-2.5-VL-7B
EasyR1
GRPO
MathVista
MathVision
Deep Analysis
Qwen-2.5-7B-Base
Qwen-2.5-7B-Instruct
DeepSeek-R1-Distill-Qwen-7B
VeRL
PPO, GRPO
Reinforce++
RLOO, ReMax, DAPO
Omni-Math-Rule
MATH500
3. RLVR’s Effect on Reasoning Capacity Boundary
With the evaluation metrics for reasoning boundaries established, we now conduct a comprehensive
evaluation of the base and RLVR models through extensive experiments. Our analysis is organized
by task category, covering three representative domains: mathematics, code generation, and visual
reasoning. The overall experimental setup is summarized in Table 1.
Evaluation Protocal. For all sampling procedures involving both base and RL-trained models, we use
a temperature of 0.6 and a top-p value of 0.95, allowing a maximum generation of 16,384 tokens. We
also show the effect of different temperature settings in Figure 10. For evaluation of the base model, a
common practice is to include few-shot examples in the prompt to guide both the output formatting
and the reasoning process (Grattafiori et al., 2024; Yang et al., 2024; Liu et al., 2024). However, to
ensure a fair and unbiased comparison, we deliberately avoid using few-shot prompts for base models,
eliminating any potential confounding effects on reasoning that might be introduced by in-context
examples. For evaluating both the base and RLVR models, we use the same zero-shot prompt as in
RLVR training, or the default prompt provided by the benchmark, ensuring a consistent setup across
both models. Interestingly, although base models often produce unformatted or non-sensical responses
without few-shot guidance, we observe that with sufficient sampling, they are still capable of generating
correctly formatted outputs and successfully solving complex problems. Prompt templates for training
and evaluation are provided in Appendix B.
3.1. RLVR for Mathematical Reasoning
Models and Benchmarks. In math problems, models are required to generate a reasoning process
(i.e., CoT) along with the final answer. To ensure the robustness of our conclusions, we experiment
with multiple LLM families, primarily Qwen-2.5 (7B/14B/32B base variants) (Yang et al., 2024) and
additionally LLaMA-3.1-8B (Grattafiori et al., 2024). We adopt RLVR-trained models released by
SimpleRLZoo (Zeng et al., 2025), which train zero-RL models using GRPO on GSM8K and the MATH
training set, with correctness reward only, excluding any format-based reward. We compare the pass@k
curves of base and zero-RL models on benchmarks of varying difficulty: GSM8K (Cobbe et al., 2021),
MATH500 (Hendrycks et al., 2021), Minerva (Lewkowycz et al., 2022), Olympiad (He et al., 2024),
AIME24, and AMC23. Additionally, we include the RL model Oat-Zero-7B, based on Qwen-2.5-7B-Base
and trained using the Oat-Zero framework, for further comparison (Liu et al., 2025b). This model is
characterized by strong performance on the challenging AIME24 benchmark, achieving an average of
43.4% points, substantially higher than the base model’s performance of below 10%.
The Effect of RLVR: Increased Likelihood of Correct Samples, Decreased Coverage of
Solvable Problems. As shown in Figure 2, we consistently observe a contrasting trend between small
and large k values. When k is small (e.g., k = 1, equivalent to average-case accuracy), RL-trained
models outperform their base counterparts. This aligns with the common observation that RL improves
performance, suggesting that RLVR makes models significantly more likely to sample correct responses.
However, as k increases to the tens or hundreds, base models consistently catch up with RL-trained
6
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
models across all benchmarks and LLM families without exception. Notably, the pass@k curves of base
models exhibit a steeper upward trend compared to their RL-trained versions. Ultimately, the base
model surpasses the RL-trained model at sufficiently large k, indicating that the coverage of solvable
problems by the base model is broader than that of the RL-enhanced version. For example, on the
Minerva benchmark with a 32B-sized model, the base model outperforms the RL-trained model by
approximately 9% at k = 128, implying that it can solve around 9% more problems in the validation set.
1
4
16
64
256 1024
Number of Samples k
0
0.2
0.4
0.6
0.8
1.0
Coverage (pass@k)
AIME24
Qwen-2.5-Math-7B
Qwen-2.5-Math-7B-Oat-Zero
Figure 3: Oat-Zero for AIME24.
We further examine RL models trained with Oat-Zero. As shown
in Figure 3, although the RL model initially demonstrates a strong
performance, nearly 30% higher than the base model, it is eventually
surpassed by the base model. Based on these results, we conclude
that RLVR increases the likelihood of sampling correct responses at
low k, but narrows the model’s overall coverage. We further analyze
the root cause of this phenomenon in Section 4.1.
CoT Case Analysis. We present the sampled correct CoTs from the
base model in Figure 18 and Figure 19, manually selected from 2048
samplings for the hardest questions in AIME24. The responses from
the base model tend to be long CoTs and exhibit reflective behavior,
highlighting the strong reasoning ability inherent in the base model.
Validity of Chain-of-Thought. For mathematical problems, the
common evaluation is based solely on the correctness of the final
answer, without considering the validity of the intermediate CoT.
However, as k increases, the risk of “hacking” may become more pronounced when a model accidentally
arrives at the correct answer through an incorrect CoT. To accurately reflect the reasoning ability
boundary using pass@k, it is important to assess how many of the solved problems result from sampling
genuinely correct CoTs, rather than from lucky guesses.
Following (Brown et al., 2024), we manually inspect all CoTs that led to correct answers on the most
challenging solvable problems in the GSM8k dataset—those with an average accuracy below 5% but
above 0%. The base model answered 25 such questions, with 24 containing at least one correct CoT.
Similarly, the RL-trained model answered 25 questions, 23 of which included at least one correct CoT.
These results suggest that even on the hardest questions in GSM8k, problem-solving primarily results
from sampling valid reasoning paths rather than relying on lucky guesses, implying that pass@k at large
k values can serve as a relatively accurate indicator of a model’s reasoning boundary.
We also manually check the CoTs for the most challenging AIME24 benchmark. To begin, we introduce
a filtering mechanism designed to eliminate easily guessable problems. Specifically, we prompt Qwen-
7B-Base to answer questions directly, without using chain-of-thought reasoning, and sample answers
multiple times. If a problem can be answered correctly with a low but non-zero probability (e.g., <5%),
we consider it guessable and remove it. Problems that can be directly answered correctly with a high
probability are retained, as they are likely easier and solvable using valid CoTs. The base and RL
model pass@k curves on this filtered AIME24 can be found in Figure 9, showing similar trending with
previous results. While this filtering method is heuristic, it proves effective. Applying it to AIME24
(30 questions) results in a subset of 18 questions. We then prompt the models to answer these filtered
questions using CoT reasoning. Then we perform a manual inspection of all CoTs that led to correct
answers on the hardest problems—those with an average accuracy below 5%. The base model answered
7 such questions, with 5 out of 6 containing at least one correct CoT (excluding one ambiguous case of
correctness due to skipped reasoning steps.). Similarly, the RL-trained model answered 6 questions, 4 of
which included at least one correct CoT. These results suggest that, even for the hardest questions in
the challenging AIME24, base model can sampling valid reasoning paths to solve the problems.
3.2. RLVR for Code Generation
Models and Benchmarks. We adopt the open-source Code-R1 (Liu & Zhang, 2025) and evaluate
its RLVR-trained model, CodeR1-Zero-Qwen2.5-7B, which trains zero-RL models on 12K LeetCode
7
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
and TACO samples over 832 steps, based on Qwen2.5-7B-Instruct-1M (Yang et al., 2025). A binary
correctness reward is provided by the compiler based on predefined test cases. For evaluation, Models
are assessed on LiveCodeBench v5, comprising 880 problems that span from May 2023 to January
2025 (Jain et al., 2024), as well as HumanEval+ and MBPP+ (Liu et al., 2023). Since passing all unit
tests is nearly impossible to achieve by guesswork, pass@k provides a reliable measure of a model’s
reasoning boundary.
1
2
4
8 16 32 64 128
Number of Samples k
0.1
0.2
0.3
0.4
0.5
Coverage (pass@k)
LiveCodeBench
Qwen-7b-Instruct-1M
Coder-R1-Zero-Qwen-2.5-7B
Figure 4: RLVR for Coding.
The Effect of RLVR. As shown in Figure 4 and Figure 5 left,
the effects of RLVR on three code generation benchmarks exhibit
trends that are highly consistent with those observed in mathematical
benchmarks. For instance, on LiveCodeBench, the original model
achieves a pass@1 score of 23.8%, while the RLVR-trained model
achieves 28.1%. However, when sampling 128 times, approximately
50% of coding problems can be solved by the original model, whereas
only 42.8% are solved by the RLVR model. Furthermore, we observe
that the slope of the original model’s curve remains steep at k = 128,
suggesting that pass@k will continue to increase with larger k values.
In contrast, the curve for Code-R1 gradually converges. This suggests
that while RLVR shows improvements in single-sample performance,
it results in a diminishing coverage boundary and has less potential
than the original model.
3.3. RLVR for Visual Reasoning
Models and Benchmarks. In visual reasoning, models must jointly interpret visual and textual
inputs to solve complex reasoning problems. This has gained significant attention in the computer vision
and multimodal communities since the rise of LLM reasoning (Chen et al., 2025; Shen et al., 2025;
Zheng et al., 2025). For our experiments, we select mathematical reasoning within visual contexts as a
representative task. We use the EasyR1 framework (Zheng et al., 2025) to train Qwen-2.5-VL-7B (Bai
et al., 2025) on Geometry3K (Lu et al., 2021), and evaluate its visual reasoning capabilities on filtered
MathVista-TestMini (Lu et al., 2024) and MathVision-TestMini (Wang et al., 2024). Specifically, both
MathVista and MathVision contain multiple-choice questions, which can be easily guessed through
repeated sampling. Therefore, we remove multiple-choice questions from the evaluation. After filtering,
460 problems remain out of 1000 in MathVista, and 114 problems remain out of 460 in MathVision.
The Effect of RLVR. As shown in Figure 5 (right), the effects of RLVR on visual reasoning are highly
consistent with those observed in math and coding benchmarks. This suggests that the original model
has broader coverage of solvable questions even in multimodal tasks.
Validity of Chain-of-Thought. To ensure that the increase in coverage is not due to random guessing
with incorrect CoT, we manually inspect a subset of the most challenging problems, i.e.those with an
average accuracy below 5%. For each inspected problem, we review all CoTs that led to correct answers.
We find that for both the original and RL models, 7 out of 8 problems have at least one correct CoT.
These results suggest that even on the hardest questions in the filtered set, the increase of coverage
primarily results from sampling valid reasoning paths, rather than relying on lucky guesses.
4. Deep Analysis
In this section, we investigate the root cause of the phenomenon where RLVR improves sample efficiency
but reduces the reasoning boundary. We also highlight the distinct properties of distillation compared to
RLVR. In addition, we design controlled experiments to examine the effects of different RL algorithms.
4.1. Reasoning Patterns Already Present in Base Models
The RLVR Model’s Solvable-Problem Coverage is an Approximate Subset of the Base
Model’s Coverage. Experiments in Section 3 surprisingly show that the base model has a larger
8
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
1
2
4
8 16 32 64 128
Number of Samples k
0.75
0.8
0.85
0.9
0.95
Coverage (pass@k)
HumanEval+
Qwen-7b-Instruct-1M
Coder-R1-Zero-Qwen-2.5-7B
1
2
4
8 16 32 64 128
Number of Samples k
0.7
0.8
0.9
MBPP+
1
2
4
8
16 32 64 128
Number of Samples k
0.9
0.8
0.7
0.6
0.5
Coverage (pass@k)
MathVista
Qwen2.5-VL-7b-Instruct
Qwen2.5-VL-7b-Instruct-RL
1
2
4
8
16 32 64 128
Number of Samples k
0.2
0.4
0.6
0.8
MathVision
Figure 5: Pass@k curves of base models and zero-RL counterparts. (Left) Code Generation. (Right) Visual Reasoning.
coverage of solvable problems than the RLVR-trained model. To further investigate, we compare the set
of solvable questions for both the base model and its corresponding RL-trained version on AIME24. We
find that the set of problems solved by the RL-trained model is nearly a subset of the solvable problems
of the base model, as shown in Table 4. A similar trend is observed in coding tasks as shown in Table 5.
This raises the natural question: Do all reasoning paths generated by RL-trained models already exist
within the output distribution of their base models?
Perplexity Analysis. To answer this question, we utilize the metric perplexity. Given a model m, a
problem x, and a response Y = (y1, . . . , yT ) (can be generated by the same model, another model, or
human), the perplexity is defined as the exponentiated average negative log-likelihood of a sequence:
PPLm(Y|x) = exp
 
−1
T
T
X
t=1
log P(yt|x, y1, . . . , yt−1)
!
,
which reflects the model’s ability to predict the given response Y conditioned on the prompt x. Lower
perplexity indicates that the model has a higher likelihood of generating this response.
We randomly sample two problems from AIME24 and employ Qwen-7B-Base and SimpleRL-Qwen-7B-
Base to generate 16 responses for each problem, denoted as Ybase and YRL, respectively. We also let
OpenAI-o1 (Jaech et al., 2024) generate 8 responses, denoted as YGT. As shown in Figure 6 left, the
distribution of PPLBase(YRL|x) closely matches the lower portion of the PPLBase(YBase|x) distribution,
corresponding to responses that the base model tends to generate. This suggests that the responses
from RL-trained models are highly likely to be generated by the base model. Combining the results
from Section 3, we draw the following conclusions:
• RLVR does not elicit novel reasoning abilities beyond the base model. The trends of
pass@k at large values of k and the distribution of perplexity indicate that the reasoning coverage of
the RL model remains entirely within that of the base model. All reasoning paths exploited by the RL
model already exist within the base model. Thus, RL training does not introduce any fundamentally
new reasoning capability, and the RL model remains bounded by the capabilities of its base model.
• RLVR improves sampling efficiency. Although the reasoning paths in the RL model already
exist within the base model, RL training improves pass@1 performance, as shown in Section 3. This
indicates that by biasing the output distribution toward high-reward responses, RL increases the
likelihood of sampling correct reasoning paths that are already encoded in the base model.
• RLVR narrows the reasoning boundary. The efficiency gain of RLVR comes at the expense of
coverage: pass@k decreases at larger k values compared to the base model. We attribute this to the
tendency of RL to reduce output entropy, which limits exploration (Yu et al., 2025).
4.2. Distillation Expands the Reasoning Boundary
In addition to direct RL training, another effective approach to improving the reasoning ability of small
base models is distillation from a powerful reasoning model such as DeepSeek-R1 (Guo et al., 2025). This
9
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
PPLBase(YBase)
PPLBase(YRL)
PPLRL(YRL)
PPLBase(YGT)
1.00
1.05
1.10
1.15
1.20
1.25
1.30
Perplexity
PPLBase(YBase)
PPLBase(YRL)
PPLRL(YRL)
PPLBase(YGT)
1.0
1.2
1.4
1.6
1.8
2.0
2.2
Figure 6: (Left) Perplexity distribution of responses from different sources, evaluated by the base and RL models. The
conditioning problem x is omitted in the figure. (Right) Coverage comparison of base, Instruct, RL, and distilled models.
process is analogous to instruction-following fine-tuning in post-training, converting a base model into
an instruct model. However, instead of using short instruction-response pairs, the training data consist
of long CoT reasoning traces generated by DeepSeek-R1. Given the limitations of RLVR in expanding
reasoning capabilities, it is natural to ask whether distillation exhibits similar behavior. We focus on a
representative model, DeepSeek-R1-Distill-Qwen-7B, which distills DeepSeek-R1 into Qwen-2.5-Math-7B.
We compare it with the base model Qwen-2.5-Math-7B and its RL-trained counterpart Qwen-2.5-Math-
7B-Oat-Zero and include Qwen-2.5-Math-7B-Instruct as an additional baseline. As shown in Figure 6
(right), the pass@k curve of the distilled model is consistently and significantly above that of the base
model. This indicates that, unlike RL that is fundamentally bounded by the reasoning capacity of the
base model, distillation introduces new reasoning patterns learned from a stronger teacher model. As a
result, the distilled model is capable of surpassing the reasoning boundary of the base model.
4.3. Effects of Different RL Algorithms
As discussed previously, the primary effect of RL is to enhance sampling efficiency rather than to expand
a model’s reasoning capacity. To quantify this, we propose the Sampling Efficiency Gap (∆SE), defined
as the difference between the RL-trained model’s pass@1 and the base model’s pass@k (we use k = 256
in our evaluation). Lower ∆SE is better. Here we conduct clean experiments to study the effect of
different RL algorithms in enhancing sampling efficiency.
Experiment Setup. We re-implement popular RL algorithms using the VeRL framework (Sheng
et al., 2024) for fair comparison, including PPO (Schulman et al., 2017), GRPO (Shao et al., 2024),
Reinforce++ (Hu, 2025), RLOO (Ahmadian et al., 2024), ReMax (Li et al., 2023), and DAPO (Yu et al.,
2025). To assess in-domain and out-of-domain generalization under RLVR, we construct splits from
Omni-MATH-Rule, a subset of Omni-MATH (Gao et al., 2024) containing verifiable problems. We split
it into a training set (2,000 samples) and an in-domain test set (821 samples), and use MATH500 as the
out-of-domain benchmark.
As shown in Figure 7 (top), although different RL algorithms exhibit slight variations in both pass@1
and pass@256, these differences are not fundamental. Different RL algorithms yield slightly different ∆SE
values (i.e., ranging from GRPO’s 43.9 to RLOO’s best 42.6 on the in-domain test set). Furthermore,
we observe that ∆SE remains consistently above 40 points across different algorithms, highlighting that
existing RL methods are still far from achieving optimal sampling efficiency. This suggests that novel
RL algorithms or entirely new paradigms may be necessary to approach the upper bound.
We also report several additional observations. First, DAPO achieves slightly higher pass@1 scores across
all three datasets; however, its dynamic sampling strategy requires approximately 3 ∼6× more samples
10
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
∆𝐒𝐄=0.358
∆𝐒𝐄=0.410
∆𝐒𝐄=0.206
0.6
0.64
0.68
Omni-MATH-Train
1
2
4
8 16 32 64 128256
Number of Samples k
0.1
0.2
0.3
0.4
0.62
0.66
0.7Omni-MATH-Test (In Domain)
1
2
4
8 16 32 64 128256
Number of Samples k
0.1
0.2
0.3
0.4
0.9
0.94
0.98 MATH500 (Out of Domain)
1
2
4
8 16 32 64 128256
Number of Samples k
0.5
0.6
0.7
0.8
Qwen2.5-7B
GRPO-step150
GRPO-step300
GRPO-step450
Figure 7: (Top) Different RL algorithms. (Bottom) Different RL training steps. We use a folded y-axis range to better
highlight the details at k = 1 and 256. Unfolded version can be found in Figure 8. The detailed values for each point at
pass@1 and pass@256 are provided in Table 2 and Table 3.
per batch during training compared to other algorithms. Moreover, its performance drops significantly
at k = 256. Second, RLOO and Reinforce++ perform consistently well across the entire k range (from 1
to 256), while maintaining efficient training costs, achieving a good balance between effectiveness and
efficiency. Third, ReMax shows lower performance at both pass@1 and pass@256. We hypothesize that
this is due to its use of the greedy response reward as the advantage baseline, which in the RLVR setting
is binary (0 or 1) and highly variable. This likely results in unstable gradient updates during training.
4.4. Asymptotic Effects of RL Training
Based on the setup in Section 4.3, we investigate the effect of the training steps on the asymptotic
performance of the model. As shown in Figure 7 (bottom), as RL training progresses, pass@1 on the
training set consistently improves from 26.1 to 42.5. However, several observations suggest that longer
training may not yield substantial benefits. First, we observe that pass@1 after 450 steps on both
in-domain and out-of-domain test sets shows only a marginal improvement compared to 150 steps,
which is much smaller than the gain observed in the training set. This suggests that RL training might
be overfitting the training set to some degree. Second, we find that as the number of training steps
increases, pass@256 decreases across all three sets. Specifically, training for 450 steps results in the lowest
pass@256, indicating a minimal reasoning boundary. This is likely because, as RL training progresses,
the model’s output entropy and exploration ability decrease.
5. Discussion
In Section 3 and Section 4, we identified key limitations of RLVR in enhancing the LLM reasoning
capabilities. In this section, we explore possible underlying factors that may explain why RLVR remains
bounded by the reasoning capacity of the base model.
Discussion 1: Key Differences Between Traditional RL and RLVR for LLMs are Vast Action
Space and Pretrained Priors. Traditional RL such as AlphaGo Zero and the DQN series (Silver
11
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
et al., 2017; Mnih et al., 2015; Yue et al., 2023) can continuously improve the performance of a policy in
environments like Go and Atari games without an explicit upper bound. There are two key differences
between traditional RL and RLVR for LLMs. First, the action space in language models is exponentially
larger than that of Go or Atari games (Ramamurthy et al., 2023). RL algorithms were not originally
designed to handle such a vast action space, which makes it nearly impossible to explore the reward
signal effectively if training starts from scratch. Therefore, the second distinction is that RLVR for LLMs
starts with a pretrained base model with useful prior, whereas traditional RL in Atari and GO games
often begins from scratch. This pretrained prior guides the LLM in generating reasonable responses,
making the exploration process significantly easier, and the policy can receive positive reward feedback.
Discussion 2: Priors as a Double-Edged Sword in This Vast Action Space. Since the sampling
of responses is guided by the pretrained prior, the policy may struggle to explore new reasoning patterns
beyond what the prior already provides. Specifically, in such a complex and highly combinatorial
space, most responses generated during training are constrained by the base model’s prior. Any sample
deviating from the prior is highly likely to produce invalid or non-sensical outputs, leading to negative
reward. As discussed in Section 2.1, policy gradient algorithms aim to maximize the log-likelihood of
responses within the prior that receive positive rewards, while minimizing the likelihood of responses
outside the prior that receive negative rewards. As a result, the trained policy tends to produce responses
already present in the prior, constraining its reasoning ability within the boundaries of the base model.
From this perspective, training RL models from a distilled model may temporarily provide a beneficial
solution, as distillation helps inject a better prior. In the future, exploration approaches that can explore
beyond the confines of the prior in such a vast action space hold promise for creating more powerful
reasoning models. Furthermore, alternative paradigms beyond pure RLVR could also be explored to
overcome these limitations and enhance the model’s reasoning capabilities.
6. Related Works
Reinforcement Learning for LLM Reasoning. Since the emergence of LLMs, the post-training
phase has proven crucial to enhance problem-solving and reasoning abilities (Ouyang et al., 2022). This
stage typically falls into three main categories: supervised fine-tuning using human-curated or distilled
data (Wang et al., 2023), self-improvement iteration (Zelikman et al., 2022; Gulcehre et al., 2023),
and reinforcement learning (Ouyang et al., 2022). Previously, a reward model or preferences between
responses were employed for reward modeling (Ouyang et al., 2022; Rafailov et al., 2023). Recently,
Reinforcement Learning with Verifiable Rewards (RLVR) has gained significant traction as a method to
improve the reasoning capabilities of LLMs in domains such as mathematics and programming (Lambert
et al., 2024; Shao et al., 2024). An encouraging landmark work is OpenAI’s o1 model (Jaech et al., 2024),
which was among the first large-scale applications of RL for reasoning, achieving state-of-the-art results
at the time of its release. Following this, Deepseek-R1 (Guo et al., 2025) became the first open-weight
model to match or surpass the performance of o1. A significant innovation introduced with R1 is
the “zero” setting, where reinforcement learning is applied directly to the base LLM, bypassing any
intermediate supervised tuning. This approach inspired a wave of open-source efforts to replicate or
extend R1’s methodology and improve RL algorithms (Zeng et al., 2025; Liu et al., 2025b; Yu et al., 2025;
Liu & Zhang, 2025). In parallel, reinforcement learning has also gained attention in the multimodal
domain, driving advancements in multimodal reasoning (Chen et al., 2025; Shen et al., 2025; Zheng
et al., 2025).
Although there are many excellent open-source works and algorithmic designs in the field of RLVR, there
remains a lack of deep understanding regarding the root effects of RLVR on LLM reasoning abilities and
its limitations when starting from the base model. Several studies (Liu et al., 2025a; Zhao et al., 2025;
Shah et al., 2025) highlight that the reflective behaviors observed in R1-like models actually emerge
from the base models, rather than being introduced by RLVR training. Dang et al. (2025) observed a
phenomenon similar to our findings: Pass@k deteriorates rapidly and fails to recover with reinforcement
learning, but this was seen only in a limited experimental setup with Qwen-2.5-0.5B on GSM8K. More
importantly, they did not explore the relationship between the base model and the RL model. In contrast,
our paper conducts systematic and rigorous experiments to show that not only reflective behaviors but
12
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
all reasoning paths are already embedded in the base model. We further demonstrate that RLVR does
not elicit any new reasoning abilities beyond the base model.
7. Conclusion
RLVR is widely regarded as a promising approach to enable LLMs to continuously self-improve and
acquire novel reasoning capabilities. In this paper, we systematically investigate the reasoning capability
boundaries of both base and RLVR models across a wide range of LLM families and benchmarks,
using the pass@k metric coupled with manually checked CoTs. Surprisingly, our findings demonstrate
that RLVR does not elicit fundamentally new reasoning patterns. Instead, RL primarily enhances the
efficiency of LLMs in sampling existing correct reasoning paths encoded in the base model. Consequently,
the reasoning boundary remains limited by the base model’s capabilities. Furthermore, our in-depth
analysis reveals that current RL algorithms are far from achieving the optimal sampling efficiency, defined
by the reasoning boundary of the base model. We also show that distillation plays a significant role in
introducing new reasoning patterns and expanding the reasoning boundary. These findings highlight a
critical limitation of RLVR in advancing LLM reasoning abilities, suggesting that a new paradigm may
be necessary to fully surpass the capabilities of base models.
13
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt,
J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1
Ahmadian, A., Cremer, C., Gallé, M., Fadaee, M., Kreutzer, J., Pietquin, O., Üstün, A., and Hooker,
S. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms.
arXiv preprint arXiv:2402.14740, 2024. 5, 10
Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al.
Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 8
Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Ré, C., and Mirhoseini, A. Large language
monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024.
2, 7
Chen, L., Li, L., Zhao, H., Song, Y., and Vinci. R1-v: Reinforcing super generalization ability in
vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed:
2025-02-02. 8, 12
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y.,
Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374, 2021. 5
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton,
J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv
preprint arXiv:2110.14168, 2021. 6
Dang, X., Baek, C., Kolter, J. Z., and Raghunathan, A. Assessing diversity collapse in reasoning.
In Scaling Self-Improving Foundation Models without Human Supervision, 2025. URL https://
openreview.net/forum?id=AMiKsHLjQh. 12
Gao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li, L., Ma, C., Chen, L., Xu, R., Tang,
Z., Wang, B., Zan, D., Quan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T., and Chang, B.
Omni-math: A universal olympiad level mathematic benchmark for large language models, 2024. URL
https://arxiv.org/abs/2410.07985. 10
Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A.,
Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
1, 6
Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A.,
Ahern, A., Wang, M., Gu, C., et al. Reinforced self-training (rest) for language modeling. arXiv
preprint arXiv:2308.08998, 2023. 12
Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al.
Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint
arXiv:2501.12948, 2025. 1, 2, 3, 5, 9, 12
He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al.
Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal
scientific problems. arXiv preprint arXiv:2402.14008, 2024. 6
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J.
Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. 6
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration.
arXiv preprint arXiv:1904.09751, 2019. 2, 5
14
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
Hu, J. Reinforce++: A simple and efficient approach for aligning large language models. arXiv preprint
arXiv:2501.03262, 2025. 10
Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel,
A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 1, 9, 12
Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and
Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code.
arXiv preprint, 2024. 8
Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A.,
Dziri, N., Lyu, S., et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv
preprint arXiv:2411.15124, 2024. 1, 12
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil,
C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models.
Advances in Neural Information Processing Systems, 35:3843–3857, 2022. 6
Li, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., and Luo, Z.-Q. Remax: A simple, effective, and efficient
reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505,
2023. 10
Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al.
Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 6
Liu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. https://github.com/
ganler/code-r1, 2025. GitHub repository. 7, 12
Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous
evaluation of large language models for code generation. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023. URL https://openreview.net/forum?id=1qvx610Cu7. 8
Liu, Z., Chen, C., Li, W., Pang, T., Du, C., and Lin, M. There may not be aha moment in r1-zero-like
training — a pilot study. https://oatllm.notion.site/oat-zero, 2025a. Notion Blog. 12
Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like
training: A critical perspective. arXiv preprint arXiv:2503.20783, 2025b. 6, 12
Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., and Zhu, S.-C. Inter-gps: Interpretable
geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting
of the Association for Computational Linguistics (ACL), 2021. 8
Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and
Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In
International Conference on Learning Representations (ICLR), 2024. 8
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller,
M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning.
nature, 518(7540):529–533, 2015. 12
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in
neural information processing systems, 35:27730–27744, 2022. 12
Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference
optimization: Your language model is secretly a reward model. Advances in Neural Information
Processing Systems, 36:53728–53741, 2023. 12
15
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage, C., Hajishirzi, H., and
Choi, Y. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and
building blocks for natural language policy optimization. In ICLR, 2023. 12
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017. 3, 10
Shah, D. J., Rushton, P., Singla, S., Parmar, M., Smith, K., Vanjani, Y., Vaswani, A., Chaluvaraju, A.,
Hojel, A., Ma, A., et al. Rethinking reflection in pre-training. arXiv preprint arXiv:2504.04022, 2025.
12
Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al.
Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint
arXiv:2402.03300, 2024. 5, 10, 12
Shen, H., Zhang, Z., Zhao, K., Zhang, Q., Xu, R., and Zhao, T. Vlm-r1: A stable and generalizable
r1-style large vision-language model.
https://github.com/om-ai-lab/VLM-R1, 2025.
Accessed:
2025-02-15. 8, 12
Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow:
A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 10
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L.,
Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. nature, 550(7676):
354–359, 2017. 11
Sutton, R. S., Barto, A. G., et al. Reinforcement learning: An introduction, volume 1. MIT press
Cambridge, 1998. 5
Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi
k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 1
Wang, K., Pan, J., Shi, W., Lu, Z., Ren, H., Zhou, A., Zhan, M., and Li, H.
Measuring
multimodal mathematical reasoning with math-vision dataset.
In The Thirty-eight Conference
on Neural Information Processing Systems Datasets and Benchmarks Track, 2024.
URL https:
//openreview.net/forum?id=QWTCcxMpPA. 8
Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K., Wadden, D., MacMillan, K., Smith,
N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open
resources. Advances in Neural Information Processing Systems, 36:74764–74786, 2023. 12
Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine learning, 8:229–256, 1992. 5
Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al.
Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 6
Yang, A., Yu, B., Li, C., Liu, D., Huang, F., Huang, H., Jiang, J., Tu, J., Zhang, J., Zhou, J., et al.
Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. 8
Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., et al. Dapo:
An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 9,
10, 12
Yue, Y., Kang, B., Xu, Z., Huang, G., and Yan, S. Value-consistent representation learning for data-
efficient reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 37, pp. 11069–11077, 2023. 12
Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances
in Neural Information Processing Systems, 35:15476–15488, 2022. 12
16
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and
taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892,
2025. 6, 12
Zhao, R., Meterez, A., Kakade, S., Pehlevan, C., Jelassi, S., and Malach, E.
Echo chamber: Rl
post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025. 12
Zheng, Y., Lu, J., Wang, S., Feng, Z., Kuang, D., and Xiong, Y. Easyr1: An efficient, scalable,
multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. 8, 12
17
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
A. Detailed Experimental Results
The unfolded y-axis version of Figure 7 is provided in Figure 8, and the detailed values of pass@1 and
pass@256 are given in Table 2 and Table 3. Indices of solvable problems in AIME24 and LiveCodeBench
are provided in Table 4 and Table 5. Pass@k curves in the filtered AIME24 are shown in Figure 9. The
effects of different temperature settings are shown in Figure 10.
1
2
4
8 16 32 64 128256
Number of Samples k
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Omni-MATH-Train
1
2
4
8 16 32 64 128256
Number of Samples k
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Omni-MATH-Test (In Domain)
1
2
4
8 16 32 64 128256
Number of Samples k
0.4
0.6
0.8
1.0MATH500 (Out of Domain)
Qwen2.5-7B
GRPO
PPO
ReMax
RLOO
Reinforce++
DAPO
1
2
4
8 16 32 64 128256
Number of Samples k
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Omni-MATH-Train
1
2
4
8 16 32 64 128256
Number of Samples k
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Omni-MATH-Test (In Domain)
1
2
4
8 16 32 64 128256
Number of Samples k
0.4
0.6
0.8
1.0MATH500 (Out of Domain)
Qwen2.5-7B
GRPO-step150
GRPO-step300
GRPO-step450
Figure 8: Unfolded y-axis version of Figure 7.
Table 2: Detailed values for each point at pass@1 and pass@256 across different RL algorithms in Figure 7.
Model
Omni-MATH-Train
Omni-MATH-Test
MATH500
pass@1
pass@256
pass@1
pass@256
pass@1
pass@256
Qwen2.5-7B
9.9
67.2
10.2
69.1
34.5
96.2
GRPO
26.1
66.3
25.1
68.3
74.4
97.2
PPO
27.2
65.8
26.8
69.2
75.2
97.2
ReMax
24.4
65.5
23.8
67.5
73.5
96.6
RLOO
28.6
66.4
28.1
69.2
75.0
97.4
Reinforce++
28.2
67.7
28.0
69.7
75.4
96.8
DAPO
31.4
66.1
26.5
67.0
75.6
96.4
Table 3: Detailed values at pass@1 and pass@256 across different RL training steps in Figure 7.
Model
Omni-MATH-Train
Omni-MATH-Test
MATH500
pass@1
pass@256
pass@1
pass@256
pass@1
pass@256
Qwen2.5-7B
9.9
67.2
10.2
69.1
34.5
96.2
GRPO-step150
26.1
66.3
25.1
68.3
74.4
97.2
GRPO-step300
33.6
65.3
27.1
66.6
75.4
96.0
GRPO-step450
42.5
64.3
28.3
63.9
76.3
95.4
18
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
Table 4: Indices of solvable problems in AIME24 (starting from 0). An approximate subset relationship
can be observed: most problems solved by the RL model are also solvable by the base model.
Models
Problem Indices
Qwen-7B-Base
0, 1, 4, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29
SimpleRL-Qwen-7B
0, 1, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29
Table 5: Indices of solvable problems in LiveCodeBench (ranging from 400 to 450, starting from 0).
Model
Solvable Problem Indices
Qwen-7B-Instruct-1M
400, 402, 403, 407, 409, 412, 413, 417, 418, 419, 422, 423,
427, 432, 433, 436, 438, 439, 440, 444, 445, 448, 449
Coder-R1
400, 402, 403, 407, 412, 413, 417, 418, 419, 422, 423,
427, 430, 433, 438, 439, 440, 444, 445, 449
1
4
16
64
256
1024
Number of Samples k
0
0.2
0.4
0.6
0.8
1
Coverage (pass@k)
AIME24
Base
RL
Figure 9: Pass@k curves of the base and RL models in the filtered AIME24.
1
4
16
64
256 1024
Number of Samples k
0
0.2
0.4
0.6
0.8
Coverage (pass@k)
Base
RL
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
4
16
64
256 1024
0
0.2
0.4
0.6
0.8
1
2
4
8 16 32 64 128
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0.2
0.4
0.6
0.8
1
1
2
4
8 16 32 64 128
0.2
0.4
0.6
0.8
1
AIME24
MATH500
T = 0.6
T = 1.0
T = 1.2
Figure 10: We found that the base model’s performance drops when the temperature exceeds 1.0, as it tends to generate
more random and less coherent tokens. In contrast, the RL model’s performance remains relatively stable across different
temperature settings. Therefore, we use T = 0.6 in the main experiments, as it allows both models to demonstrate their
best reasoning performance.
19
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
B. Prompt Templates
We provide the prompt templates used for training and evaluation in our experiments. The prompt
for SimpleRL training and evaluation is shown in Figure 11, while the prompt for Oat-Zero is shown
in Figure 12. For Code-R1 training, prompt in Figure 13 is adopted. For Code-R1 evaluation, we follow
the original codebase and adopt the default templates from the benchmarks, including LiveCodeBench
prompt (Figure 14), HumanEval+, and MBPP+ prompt (Figure 15). The prompt used for EasyR1
training and evaluation is shown in Figure 16. For VeRL-trained RL models, as discussed in Section 4.3
and Section 4.4, the training and evaluation prompts are provided in Figure 17. To ensure a fair
comparison, the base models use the same prompts as their corresponding RL-trained counterparts
during evaluation.
C. Broader Impacts
The potential negative social impacts of our method align with those typically associated with general
LLM reasoning technologies. We emphasize the importance of adhering to the principles of fair and safe
deployment in LLM systems.
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{question}
Please reason step by step, and put your final answer within\\boxed{{}}.<|im_end|>
<|im_start|>assistant
SimpleRL Prompt
Figure 11: Prompt for SimpleRL Training and Evaluation. The base model uses the same prompt as the RL model during
evaluation.
<|im_start|>system
Please reason step by step, and put your final answer within \\boxed{}.<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
Oat Prompt
Figure 12: Prompt for Oat-Zero training and evaluation.
20
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
<|im_start|>system
You are a helpful programming assistant. The user will ask you a question and you 
as the assistant solve it. The assistant first thinks how to solve the task through 
reasoning and then provides the user with the final answer. The reasoning process 
and answer are enclosed within <think>...</think> and <answer>...</answer> tags, 
respectively.<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
Code-R1 Prompt
Figure 13: Prompt for Code-R1 training.
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
You will be given a question (problem specification) and will generate a correct 
Python program that matches the specification and passes all tests. You will NOT 
return anything except for the program.
Question: {question.question_content}
{ if question.starter_code }
### Format: 
{PromptConstants.FORMATTING_MESSAGE_WITH_STARTER_CODE}
```python
{question.starter_code}
```
<|im_end|>
{ else }
### Format: 
{PromptConstants.FORMATTING_MESSAGE_WITHOUT_STARTER_CODE}
```python
# YOUR CODE HERE
```
<|im_end|>
<|im_start|>assistant
LiveCodeBench (Code Generation) Prompt
Figure 14: Prompt for Code-R1 Evaluation on LiveCodeBench.
21
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
Please provide a self-contained Python script that solves the following problem in a 
markdown code block:
```
{python_task_prompt}
```
<|im_end|>
<|im_start|>assistant
Below is a Python script with a self-contained function that solves the problem and 
passes corresponding tests:
```python
HumanEval+ & MBPP+ Prompt
Figure 15: Prompt for Code-R1 Evaluation on HumanEval+ and MBPP+.
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant. You FIRST 
think about the reasoning process as an internal monologue and then provide the 
final answer. The reasoning process MUST BE enclosed within <think> </think> 
tags. The final answer MUST BE put in \boxed{}.<|im_end|>
<|im_start|>user
<|vision_start|>{image_token}<|vision_end|>
{question}<|im_end|>
<|im_start|>assistant
EasyR1 Prompt
Figure 16: Prompt for EasyR1 training and evaluation.
<|im_start|>system
You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You 
should think step-by-step and put your final answer within \\boxed{}. <|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
VeRL Training and Evaluation Prompt
Figure 17: (1) Prompt for VeRL training and evaluation.
22
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
 
Question 
(AIME24 Question No.16) 
Among the 900 residents of Aimeville, there are 195 who own a diamond ring, 367 who own a set of golf clubs, and 562 who own a garden 
spade. In addition, each of the 900 residents owns a bag of candy hearts. There are 437 residents who own exactly two of these things, and  
234 residents who own exactly three of these things. Find the number of residents of Aimeville who own all four of these things.  
Qwen-2.5-7B Answer                                                        Response Length: 1058 
To solve this problem, we can use the principle of inclusion-exclusion. We will denote the number of residents who own diamond rings, golf 
clubs, garden spades, and candy hearts as D, G, S, and C, respectively. We are given the following information:  
- D = 195  
- G = 367 
- S = 562 
- C = 900 (since every resident owns a bag of candy hearts) 
We are also given that 437 residents own exactly two of these things and 234 residents own exactly three of these things. 
Let's denote the number of residents who own all four of these things as X. We need to find X.  
First, we need to find the number of residents who own at least one of these things. This can be calculated using the principle of inclusion- 
exclusion:  
Total = 𝐷+ 𝐺+ 𝑆+ 𝐶−(number of residents who own exactly two things)
−2 × (number of residents who own exactly three things) −3 × (number of residents who own all four things)
 
Since there are 900 residents in total, we can write the equation as: 
900 = 195 + 367 + 562 + 900 −437 −2 × 234 −3 × 𝑋 
Now we need to solve for X:  
900 = 195 + 367 + 562 + 900 −437 −468 −3 × 𝑋
900 = 1717 −905 −3 × 𝑋
900 = 812 −3 × 𝑋
3 × 𝑋= 812 −900
3 × 𝑋= −88
𝑋= −88
3
 
Since the number of residents cannot be negative, there is an error in our calculations. Let's re-evaluate the problem. 
We know that there are 437 residents who own exactly two things and 234 residents who own exactly three things. We can subtract these 
from the total number of residents who own at least one thing to find the number of residents who own exactly one thing:  
Number of residents who own exactly one thing = (𝐷+ 𝐺+ 𝑆+ 𝐶) −(number of residents who own exactly two things)
−2 × (number of residents who own exactly three things) −3 × (number of residents who own all four things)
 
Let's denote the number of residents who own exactly one thing as Y: 
𝑌= (195 + 367 + 562 + 900) −437 −2 × 234 −3 × 𝑋
𝑌= 1924 −905 −3 × 𝑋
𝑌= 1019 −3 × 𝑋
 
Since every resident owns at least one thing, we can write the equation: 
900 = 𝑌+ 437 + 2 × 234 + 3 × 𝑋 
Now we need to solve for X:  
900 = (1019 −3 × 𝑋) + 437 + 468 + 3 × 𝑋
900 = 1019 + 437 + 468
900 = 1924
 
This equation is not true, which means there is still an error in our calculations. Let's try a different approach. 
We can use the fact that the total number of residents who own at least one thing is 900. We can write the equation: 
900 = (number of residents who own exactly one thing) + (number of residents who own exactly two things)
+(number of residents who own exactly three things) + (number of residents who own all four things)
 
Let's denote the number of residents who own exactly one thing as Y:  
900 = 𝑌+ 437 + 2 × 234 + 𝑋
900 = 𝑌+ 805 + 𝑋
 
Now we need to find Y. We can use the fact that the total number of residents who own at least one thing is equal to the sum of the residents 
who own each item minus the residents who own at least two items:  
900 = (195 + 367 + 562 + 900) −(number of residents who own exactly two things)
−2 × (number of residents who own exactly three things) −3 × (number of residents who own all four things) 
900 = 1924 −437 −2 × 234 −3 × 𝑋
900 = 1924 −805 −3 × 𝑋
900 = 1119 −3 × 𝑋
 
Now we can solve for X:  
3 × 𝑋= 1119 −900
3 × 𝑋= 219
𝑋= 73
 
 
So, the number of residents of Aimeville who own all four of these things is 73 . 
 
Figure 18: Qwen-2.5-Base-7B Correct Response - Case 1.
23
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
 
Question 
(AIME24 Question No.24) 
Find the number of ways to place a digit in each cell of a 2x3 grid so that the sum of the two numbers 
formed by reading left to right is 999, and the sum of the three numbers formed by reading top to bottom is 
99. The grid below is an example of such an arrangement because 8 + 991 = 999 and 9 + 9 + 81 = 99.  
Qwen-2.5-7B Answer                                                          Response Length: 967 
To solve this problem, we need to determine the number of ways to place digits in a 2x3 grid such that the sum 
of the two numbers formed by reading left to right is 999, and the sum of the three numbers formed by reading 
top to bottom is 99. Let's break this down step by step. 
 
1. Define the grid: 
   Let the grid be represented as:  
𝑎
𝑏
𝑐
𝑑
𝑒
𝑓 
   The numbers formed by reading left to right are 100𝑎+ 10𝑏+ 𝑐 and 100𝑑+ 10𝑒+ 𝑓. The numbers formed 
by reading top to bottom are 10𝑎+ 𝑑, 10𝑏+ 𝑒, and 10𝑐+ 𝑓. 
 
2. Set up the equations: 
   - The sum of the two numbers formed by reading left to right is 999: 
100𝑎+ 10𝑏+ 𝑐+ 100𝑑+ 10𝑒+ 𝑓= 999 
   - The sum of the three numbers formed by reading top to bottom is 99: 
10𝑎+ 𝑑+ 10𝑏+ 𝑒+ 10𝑐+ 𝑓= 99 
 
3. Simplify the equations: 
   - From the first equation: 
100𝑎+ 10𝑏+ 𝑐+ 100𝑑+ 10𝑒+ 𝑓= 999 
     This implies: 
𝑎+ 𝑑= 9, 𝑏+ 𝑒= 9, 𝑐+ 𝑓= 9 
   - From the second equation: 
10𝑎+ 𝑑+ 10𝑏+ 𝑒+ 10𝑐+ 𝑓= 99 
     This is consistent with the previous simplification since: 
10(𝑎+ 𝑏+ 𝑐) + (𝑑+ 𝑒+ 𝑓) = 99 
     Given 𝑎+ 𝑑= 9, 𝑏+ 𝑒= 9, and 𝑐+ 𝑓= 9, it follows that: 
10(𝑎+ 𝑏+ 𝑐) + (9 −𝑎) + (9 −𝑏) + (9 −𝑐) = 99 
     Simplifying, we get: 
10(𝑎+ 𝑏+ 𝑐) + 27 −(𝑎+ 𝑏+ 𝑐) = 99 
9(𝑎+ 𝑏+ 𝑐) + 27 = 99 
9(𝑎+ 𝑏+ 𝑐) = 72 
𝑎+ 𝑏+ 𝑐= 8 
     Since 𝑎+ 𝑑= 9, 𝑏+ 𝑒= 9, and 𝑐+ 𝑓= 9, and 𝑎+ 𝑏+ 𝑐= 8, it follows that: 
𝑑= 9 −𝑎, 𝑒= 9 −𝑏, 𝑓= 9 −𝑐 
 
4. Determine the possible values: 
   - 𝑎, 𝑏, 𝑐 must be digits (0-9) such that 𝑎+ 𝑏+ 𝑐= 8. 
   - For each valid combination of 𝑎, 𝑏, 𝑐, 𝑑, 𝑒, 𝑓 are uniquely determined as 𝑑= 9 −𝑎, 𝑒= 9 −𝑏, 𝑓= 9 −𝑐. 
 
5. Count the combinations: 
   - We need to count the number of non-negative integer solutions to 𝑎+ 𝑏+ 𝑐= 8 with 0 ≤𝑎, 𝑏, 𝑐≤9. 
   - This is a classic "stars and bars" problem with an additional constraint. The number of solutions is given by 
the binomial coefficient: 
(8 + 3 −1
3 −1
) = (10
2 ) = 45 
 
Therefore, the number of ways to place the digits in the grid is 45 . 
 
Figure 19: Qwen-2.5-Base-7B Correct Response - Case 2.
24

==================================================

==================================================
Paper 3
Title: Outlier-Robust Multi-Model Fitting on Quantum Annealers
Abstract: Multi-model fitting (MMF) presents a significant challenge in Computer
Vision, particularly due to its combinatorial nature. While recent advancements
in quantum computing offer promise for addressing NP-hard problems, existing
quantum-based approaches for model fitting are either limited to a single model
or consider multi-model scenarios within outlier-free datasets. This paper
introduces a novel approach, the robust quantum multi-model fitting (R-QuMF)
algorithm, designed to handle outliers effectively. Our method leverages the
intrinsic capabilities of quantum hardware to tackle combinatorial challenges
inherent in MMF tasks, and it does not require prior knowledge of the exact
number of models, thereby enhancing its practical applicability. By formulating
the problem as a maximum set coverage task for adiabatic quantum computers
(AQC), R-QuMF outperforms existing quantum techniques, demonstrating superior
performance across various synthetic and real-world 3D datasets. Our findings
underscore the potential of quantum computing in addressing the complexities of
MMF, especially in real-world scenarios with noisy and outlier-prone data.
PDF URL: http://arxiv.org/pdf/2504.13836v1

Full Content:
Outlier-Robust Multi-Model Fitting on Quantum Annealers
Saurabh Pandey1,2
Luca Magri3
Federica Arrigoni3
Vladislav Golyanik1
1MPI for Informatics, SIC
2Saarland University
3Politecnico di Milano
Abstract
Multi-model fitting (MMF) presents a significant challenge
in Computer Vision, particularly due to its combinatorial
nature.
While recent advancements in quantum comput-
ing offer promise for addressing NP-hard problems, exist-
ing quantum-based approaches for model fitting are either
limited to a single model or consider multi-model scenar-
ios within outlier-free datasets.
This paper introduces a
novel approach, the robust quantum multi-model fitting (R-
QuMF) algorithm, designed to handle outliers effectively.
Our method leverages the intrinsic capabilities of quan-
tum hardware to tackle combinatorial challenges inherent
in MMF tasks, and it does not require prior knowledge of
the exact number of models, thereby enhancing its practical
applicability. By formulating the problem as a maximum
set coverage task for adiabatic quantum computers (AQC),
R-QuMF outperforms existing quantum techniques, demon-
strating superior performance across various synthetic and
real-world 3D datasets. Our findings underscore the poten-
tial of quantum computing in addressing the complexities
of MMF, especially in real-world scenarios with noisy and
outlier-prone data1.
1. Introduction
Model fitting is a fundamental and challenging problem in
computer vision, with applications such as 3D reconstruc-
tion, scene layout estimation, motion segmentation, and im-
age stitching. Its objective is to explain input data (e.g., 2D
or 3D point sets) using a non-redundant number of paramet-
ric models. However, challenges arise with multiple mod-
els, whose exact number is typically unknown, and the need
for robustness against outliers. These issues are particularly
critical in homography and fundamental matrix estimations,
where errors can significantly impact downstream tasks. Ul-
timately, multi-model fitting is an ill-posed problem with a
combinatorial nature, where data clustering and model esti-
mation must be solved simultaneously.
Multi-model fitting (MMF) has been actively researched
1Project page: https://4dqv.mpi-inf.mpg.de/RQMMF/
Figure 1. Overview of our R-QuMF, a multi-model fitting ap-
proach that is robust to outliers and admissible to modern quantum
annealers. We first sample models that along with the data define
the preference matrix P. Next, a QUBO problem is prepared that
can be minimised by quantum annealing (after a minor embedding
of the logical problem on quantum hardware) or other solvers. Fi-
nally, the best solution is selected. R-QuMF outperforms previous
quantum-admissible model fitting approaches.
during the last decades, following many different princi-
ples (e.g., optimisation and clustering-based approaches,
see Sec. 2). One of the latest and newest research direc-
tions in the field focuses on adopting quantum computa-
tional paradigms, either gate-based or quantum annealing
[11, 13, 15]. While gate-based quantum machines are uni-
versal in the sense they complement classical computers
with a set of additional operations, quantum annealers can
be thought of as samplers of a specific type of optimisa-
tion problems, i.e., quadratic unconstrained binary optimi-
sation (QUBO) objectives. The latter has recently gained
a lot of attention in the community since many computer
vision tasks can be rephrased as a QUBO, including match-
ing problems [8, 9, 31, 35, 36], object detection [24], multi-
object tracking [45], motion segmentation [3] and neural
network weight optimisation [23, 33]. The main motivation
for using quantum computers lies in their promise to ac-
celerate the solution search for combinatorial optimisation
problems while returning globally optimal solutions with
a certain non-zero probability [14]. Advantages in adopt-
ing a quantum approach have also been shown for non-
combinatorial problems like point-set registration [17, 30].
1
arXiv:2504.13836v1  [cs.CV]  18 Apr 2025
Method
Multi-Model
Outlier-Robust
HQC-RF [13]
×
✓
QuMF [15]
✓
×
Ours
✓
✓
Table 1. Comparison of method characteristics.
In the context of model fitting, Chin et al. [11] in-
troduced a method based on gate-based quantum hard-
ware, while other works rely on the quantum annealing
paradigm [13, 15].
These methods were shown to pro-
vide improvements compared to classical methods due to
the quantum effects both from the theoretical and practical
perspectives; they are compatible with current and upcom-
ing generations of quantum hardware.
Among these approaches, [11, 13] address the case of a
single model. The QuMF method of Farina et al. [15] for
quantum annealers, instead, not only outperforms previous
quantum single-model fitting approaches but also supports
multiple models and competes with classical state-of-the-
art. However, the main drawback of QuMF [15] is that it
assumes outlier-free data, which is unrealistic in most prac-
tical scenarios. The naive way to extend such an approach
to managing outliers is by post-processing, i.e., only the k
largest models are selected at the end among all candidate
models obtained by random sampling (with k equal to the
true number of models explaining the data). Besides requir-
ing the knowledge of k, or equivalent ancillary information,
this approach is sub-optimal and prone to inaccuracies, as
demonstrated by our experiments on standard datasets.
This paper addresses the challenge of outlier handling in
multi-model fitting and tailors for quantum annealers a new
multi-model fitting approach, which we call Robust Quan-
tum Multi-Model Fitting (R-QuMF); see Fig. 1. In contrast
to previous quantum work [15], it accounts for outliers ex-
plicitly in the formulation, resulting in a more general ap-
proach while exhibiting superior results on real data. An-
other advantage of our method is that it does not require
any prior information about the optimal number of models
explaining the data, which is convenient in practical appli-
cations. More details on the differences with respect to pre-
vious quantum papers are reported in Tab. 1. To summarise,
the contributions of this paper are two-folds:
i) R-QuMF, a new approach for outlier-robust fitting of mul-
tiple models;
ii) A formulation compatible with quantum annealers that
accounts for outliers explicitly: Our method does not
need any post-processing steps or a number of optimal
models in advance as input, which are highly advanta-
geous properties in practice.
We also apply to R-QuMF the decomposition principle
similar to De-QuMF [15]. It addresses the limitations of
current AQC hardware by iteratively decomposing the orig-
inal large problem into smaller QUBO sub-problems until
the final sub-problem selects the solutions among the most
promising models. Our approach significantly outperforms
previous quantum techniques in the experiments with var-
ious multi-model fitting scenarios with different outlier ra-
tios such as geometric model fitting, homography estima-
tion and fundamental matrix estimation. The source code
of our method for all solver versions (including D-Wave and
demo examples) will be made available.
2. Related Work
Classical Approaches.
Multi-model fitting has been ad-
dressed since the 1960s, with effective techniques ranging
from the Hough transform [43] to more recent approaches
based either on clustering or optimising an objective func-
tion. Clustering-based methods [2, 6, 10, 18, 21, 26, 28, 29,
32, 37, 38, 40, 46] focus on data segmentation and offer pro-
cedural, easy-to-implement solutions that produce promis-
ing results in most cases. However, hard clustering of data
does not always produce optimal results when models over-
lap. On the contrary, optimisation methods prioritise the
refinement of a precise objective function, offering a quan-
titative measure to assess the quality of the derived solu-
tion. The most common objective functions are typically
based on consensus, i.e., they aim at maximising the num-
ber of inliers of each model, so optimisation-based methods
[4, 5, 20, 27, 39, 47] can be considered as sophisticated ex-
tensions of the popular RanSaC paradigm [16] to the case
of multiple models.
In this respect, the classical work that is mostly related
to our approach is RanSaCov [27], which casts multi-model
fitting as a coverage problem. Given a collection of mod-
els with their consensus sets, RanSaCov extracts either the
minimum number of models that explain all the points (set
cover formulation) or, if the number k of the sought mod-
els is known in advance, it selects the k models that explain
most of the data (maximum coverage formulation). The lat-
ter is effective in dealing with data contaminated by outliers
that can be recognised as uncovered points.
We borrow from RanSaCov the maximum coverage for-
mulation. However, RanSaCov [27] solves the coverage
problems via integer linear programming and branch and
bound, hence, it either resorts to approximations or falls
back to enumerating all candidate solutions. Instead, our
approach exploits quantum effects to optimise the objective
directly in the space of qubits, where global optimality is
expected with high probability after multiple anneals. In ad-
dition, we directly minimize the number of models as done
in several traditional MMF frameworks [4, 5, 20], without
requiring the knowledge of the true number of models in ad-
vance. Contrary to RanSaCov, we do not deal natively with
intersecting models, but inliers belonging to multiple mod-
els can be identified after the models have been extracted by
2
inspecting the point-model residuals.
Approaches Compatible with Quantum Hardware.
While many classical methods have been developed, the
first methods based on quantum computing have only re-
cently attempted to exploit the capabilities of quantum hard-
ware to tackle the combinatorial nature of the problem.
The quantum solutions presented so far do not address
the multi-model fitting problem under outliers. Moreover,
other approaches—both theoretical and practical—start to
address closely related problems such as linear regression
[12, 34], clustering and segmentation [3, 41] to capitalise
on the advantages of quantum computing.
The first attempts to address model fitting with the help
of quantum hardware concentrate on single-model fitting
[11, 13]. Chin et al. and Yang et al. [11, 44] introduced
a single-model fitting method based on gate-based quantum
hardware, while Doan et al. [13] rely on quantum anneal-
ing. Although both Doan et al. [13] and our method are
based on linear programming, their approaches diverge sig-
nificantly. Doan et al. [13] employ a hypergraph formalism
that relies on multiple QUBOs within an iterative frame-
work, while our method is more streamlined, using a single
QUBO. Additionally, while they focus on single-model fit-
ting, our approach extends to multi-model fitting.
Farina et al. [15] take this further with their QuMF
method for quantum annealers, which considers multiple
models and achieves results on par with classical state-of-
the-art techniques. However, QuMF is primarily limited by
its reliance on outlier-free data. While post-processing can
improve outlier robustness, it usually requires prior knowl-
edge of the number k of models and often leads to sub-
optimal results, as evidenced by our experiments.
3. Background on Quantum Annealers
Modern quantum annealers (QAs) can sample Quadratic
Unconstrained Binary Optimisation (QUBO) problems,
which in a general form can be written as
arg min
yϵBd
yT Qy + sT y +
X
i
λi||Aiy −bi||2
2,
(1)
where y ∈Bd is a vector of d binary variables, Q ∈Rd×d
is a real symmetric matrix, Ai ∈Rd×d are real matrices,
s, bi ∈Rd, and λi are scalar weights. The terms under
the ℓ2-norm are rectifiers expressing soft linear constraints.
They preserve the QUBO problem type since Eq. (1) can be
written without constraints as follows:
arg min
yϵBd
yT eQy + ˜sT y,
(2)
with eQ = Q + P
i AT
i Ai and ˜s = s −2 P
i Aiλibi.
Eq. (2) is the combinatorial QUBO form admissible to mod-
ern QAs; the elements of eQ and ˜s along with the number of
binary variables to be optimised have to be provided.
During quantum annealing, yi are modelled as qubits
weighted by ˜si with the strength of the mutual influ-
ence defined by eQ.
The optimisation takes place in
the 2d-dimensional Hilbert space and involves quantum-
mechanical effects of qubit superposition, entanglement and
quantum tunnelling.
We will call the connectivity pattern between the binary
variables in Eq. (2) the logical problem graph (i.e. in which
qubits are represented by vertices and edges between the
vertices are present if Qi,j entries are non-zero). Since di-
rect interactions between only a small subset of all possi-
ble d(d −1)/2 pairs of binary variables are enabled by the
hardware, mapping the logical problem graph into the phys-
ical hardware is necessary for most QUBO problems. This
mapping is called minor embedding, as the logical problem
graph is interpreted as a graph minor of a larger graph, i.e.
hardware graph of physical qubits. This means the hard-
ware graph contains qubit chains representing a single log-
ical qubit from the logical problem graph.
4. The Proposed R-QuMF Method
This section presents our robust quantum multi-model fit-
ting (R-QuMF) approach and details of its implementation.
The method can be summarised as in Fig. 1: Steps 1 and
2 are described in Secs. 4.1 and 4.2. Steps 3 and 5 corre-
spond to Sec. 4.3, whereas details of Step 4 (implementa-
tion/solvers) are in Sec. 4.4.
4.1. Preliminaries, Definitions and Notations
We frame the multi-model fitting problem in the presence
of outliers as follows.
We are given in input a set of
points X = (x1, x2, ..., xn) and a collection of models
Θ = (θ1, θ2, ..., θm) generated through random sampling
akin to those of the RANSAC algorithm [16]. The desidered
output is a subset {θi1, . . . θik} ⊂Θ of non-redundant mod-
els that explain the data. Having non-redundant models is
a form of regularisation to make the ill-posed multi-model
fitting problem tractable. Echoing Occam Razors’ princi-
ple, we favour the interpretation of the data that minimizes
the number of models required. Moreover, we assume that:
i) X can be corrupted by high outlier percentages (up to
50%), and that ii) the number k of true models explaining
the underlying data is unknown. The above-mentioned as-
sumptions are very desirable in practical applications.
The problem can be equivalently formulated in terms of
a preference-consensus matrix defined as
P[i, j] =
(
1
if err(xi, θj) < ϵ,
0
otherwise,
(3)
where P ∈Bn×m is a n × m binary matrix; n and m be-
3
ing the number of points and models, respectively. The i-th
data point xi is assigned to the j-th sampled model θj if its
residual is below an inlier threshold ϵ. Operator error(·, ·)
quantifies the point-to-model distance. Following [27], the
rows of P can be interpreted as preference sets, while the
columns of P—denoted by S1, . . . , Sm—represent consen-
sus sets (w.r.t. ϵ). MMF reduces to selecting from P the
columns that correspond to the sought models {θi1, . . . θik}.
4.2. Revisiting Maximum-Set Coverage Objective
In order to gain robustness against outliers, we generalise
the QUBO formulation presented in QuMF [15] approach –
which, in turn, is based on the Set Cover formulation pre-
sented in [27]. Specifically, we revisit the maximum-set
coverage (MSC) objective for MMF [27]. The MSC task is
to select at most k subsets from Θ such that the coverage
of the data points contained in the set X is maximum (or as
complete as possible); all the uncovered points are consid-
ered outliers. Intuitively, with reference to the matrix P, we
want to select k columns that explain most of the points.
Let us introduce n binary variables y1, . . . , yn such that:
yi = 1 if xi is covered by one of the θj, or, in other terms,
it is part of the selected subsets (i.e. the point is an in-
lier); yi = 0 otherwise (i.e. the point is an outlier). Let
us consider additional auxiliary variables z1, . . . , zm such
that: zj = 1 if model θj is selected; zj = 0 otherwise.
Using this notation, the MSC problem is formulated as an
integer linear programming:
max
n
X
i=1
yi
s.t.
m
X
j=1
zj ≤k,
X
j:Sj∋xi
zj ≥yi
∀xi ∈X
(4)
where both yi ∈{0, 1} and zj ∈{0, 1}. In this context, the
first constraint imposes that at most k models are selected
(with k known in advance); the second constraint ensures
that, if yi = 1, then at least one set θj containing xi must be
chosen. Recall that n corresponds to the cardinality of the
set X and m denotes the number of candidate models. Note
that, within this formulation, uncovered points are consid-
ered outliers. Therefore, outliers and uncovered points are
used interchangeably and no artificial models for outliers
need to be introduced.
The combinatorial nature of the problem makes it a suit-
able choice for designing a QUBO method compatible with
a quantum annealer. To accomplish such a task, we make
some changes with respect to the MSC objective in (4).
First, we replace the inequality in the second constraint with
an equality, therefore looking for disjoint models. Secondly,
instead of demanding the approach to select k models, we
opt for directly minimising the number of selected models.
This choice is motivated by the fact that, in many practical
situations, assuming known k is restrictive. Therefore our
final objective, converted into a minimisation problem, is:
min −
n
X
i=1
yi + λ1
m
X
j=1
zj
s.t.
X
j:Sj∋xi
zj = yi
∀xi ∈X
(5)
where λ1 is a regularisation parameter.
4.3. MSC Reformulated as QUBO
The first step to reformulate MSC as QUBO is to vectorise
the objective in Eq. (5) and convert the constraint into a ma-
trix form. More precisely, we rewrite it as follows:
min
y∈Bn, z∈Bm −1T
ny + λ1(1T
mz)
s.t
Pz = y
(6)
where 1 denotes a vector of ones (whose length is given as a
subscript), P is the preference matrix of size n×m, y and z
are binary vectors collecting the yi and zj variables, respec-
tively. To simplify the formulation into a single variable
optimisation problem, we can combine the two unknowns
into a single variable w:
w =

y
z

∈
0, 1	n+m ,
(7)
and rewrite the main objective of (6) in terms of variable w:
−1T
ny + λ1(1T
mz) = [−1T
n, OT
m]w + λ1[OT
n, 1T
m]w
= [−1T
n, λ11T
m]w
(8)
where O represents a vector of zeros, with its length indi-
cated as a subscript. Furthermore, the constraint in (6) can
also be reformulated in terms of the newly introduced vari-
able w as follows:
Pz −y = On ⇔[−In×n, P]w = On
(9)
where In×n is an identity matrix of size n × n.
After incorporating the constraint from Eq. (9) as a
penalty term into equation (8) we obtain
min
w∈Bn+m[−1T
n, λ11T
m]w + λ2||[−In×n, P]w||2
2.
(10)
Now, when comparing (10) with (1), the following corre-
spondences emerge:
Q = 0,
sT = [−1T
n, λ11T
m],
A = [−In×n, P],
b = On.
(11)
Note that Q equals zero matrix as there are no quadratic
terms involved. Finally, we obtain the target QUBO objec-
tive admissible on quantum hardware that can be written in
the form of Eq. (2):
min
w
wT eQw + esT w,
(12)
4
where
w =
y
z

∈
0, 1	n+m ,
eQ = λ2
In×n
−P
−P T
P T P

,es =

−1n
λ11m

.
(13)
The QUBO formulation in Eq. (12) can be optimised by
classical global optimisation algorithms such as simulated
annealing, or sampled on a quantum annealer. Note that
the number of unknowns scales linearly with the number of
points and models. Finally, we point out that the decom-
position principle introduced by Farina et al. [15] can be
applied to our QUBO as well, to manage large-scale ap-
plications. The core principle is to iteratively break down
the input problem into smaller (tractable) sub-problems and
aggregate the respective results. Algorithm 1 in our supple-
ment provides a summary of our method.
4.3.1. Selection of λ’s
The lambda parameters from Eq. (13) are decisive for the
performance of our method. To find suitable values, we
use Tree-structured Parzen Estimator (TPE) [7] which em-
ploys a Bayesian optimisation strategy utilizing a proba-
bilistic model to steer the search process towards hyperpa-
rameter configurations that are more likely to improve the
performance metric of interest. This model-based approach
contrasts sharply with exhaustive grid search, which op-
erates without leveraging prior knowledge or outcomes of
evaluations. TPE optimises by constructing and refining a
probabilistic model based on past evaluation results, thereby
smartly converging to optimal hyperparameters through se-
quential model fitting and utility-based sampling.
4.4. Implementation Details
Our QUBO objective can be optimized either on CPU us-
ing classical solvers, like Simulated Annealing (SA) [22]
and Gurobi [19], or on QPU (Quantum Processing Unit)
via Quantum Annealing (QA).
For simulated annealing, we use D-Wave’s neal package
(version 0.6.0), and we fix the number of samples for SA to
100 (the same used in Farina et al. [15]), and we adopt this
configuration also for the competing methods.
As regard Gurobi, we rely on version 10.0.3 under aca-
demic license with a time limit of 120 seconds for both
RQuMF and the decomposed version De-RQuMF. The
same configuration was used for QuMF and De-QuMF [15].
Experiments with Quantum Anneling are performed on
D-Wave quantum annealer Advantage 5.4. We set the num-
ber of anneals to 5k for the one-shot version and 2.5k for
the decomposed version (the same values used in Farina et
al. [15]). In total, we used approximately 16 minutes of
QPU time for our experiments. The subproblem size in De-
RQuMF is set to 40 (the same as in Farina et al. [15]). We
adopt the maximum chain length criterion for all conducted
experiments to calculate chain length for D-Wave experi-
ments. This involves initially mapping a logical graph onto
a physical graph through the process of minor embedding.
After determining the final embedding, we calculate the
length (l) of the longest chain of qubits. Subsequently, the
chain strength parameter is established by adding a small
offset to l specifically, an offset of 0.5 inline with the pre-
vious works [3, 9]. See further implementation details of
R-QuMF in the released source code.
5. Experiments
We assess the effectiveness of the proposed method on
both synthetic and real datasets.
Specifically, we com-
pare our approach with RanSaCov [27], a classical method,
and a quantum apporach, namely the recently introduced
QuMF [15], as these methods are the closest competitors
(see Sec. 2). To evaluate the performance of the analysed
methods we adopt the misclassification error, denoted as
Emis, which judges the quality of MMF in terms of the seg-
mentations attained. Specifically, the misclassification error
counts the number of misclassified points as follows: first,
each point is assigned to a label corresponding to the model
it belongs to (outliers are assigned to the label 0); then,
the map between ground-truth labels and estimated ones
that minimises the overall number of misclassified points
is found; a point is deemed as correct if one of its labels
corresponds to the ground truth. The evaluated multi-model
fitting tasks include fitting lines to 2D points (synthetic), fit-
ting planes to 3D points (real) and two-view segmentation
on the AdelaideRMF dataset [42] for fitting fundamental
matrices or homographies.
5.1. Line Fitting on Synthetic Data
We evaluate our method on line fitting problems by creating
a synthetic test-bed that comprises five lines arranged into
a pentagon (see Fig. 2 for some visualisations). Each line
fitting problem comprises 30 data points, divided into out-
liers and inliers. Outliers are uniformly distributed, whereas
inliers (i.e., points belonging to the lines) are perturbed us-
ing Gaussian noise with a standard deviation of 0.01, and
they are equally distributed among the lines. Each test is
repeated 20 times and the mean misclassification error is
reported. The preference matrix is the same for all com-
pared methods and true models have always been sampled
in the pool of provisional models Θ (a standard practice).
Robustness to Outliers. First, in order to evaluate the ro-
bustness of the methods, we gradually increase the outlier
percentage from 0% to 50% while keeping fixed the total
number of points (i.e., 30 points). The number of sampled
models is set equal to 40 models. Results of this experiment
are reported in Top Fig. 3. QuMF and De-QuMF are not
designed to deal with outliers and, as expected, they do not
5
Figure 2. A sample visualization of the synthetic dataset (Ground
Truth) and results for various methods for 50 models and 33%
outliers (i.e. 10 outliers out of 30 points).
# of Models
QuMF[15]
De-QuMF[15]
RanSaCov[27]
RQuMF
De-RQuMF
20
1.00
1.00
0
0
0
50
16.16
9.50
0.66
0.66
0.66
100
44.83
18.66
0
1.33
0
500
86.66
36.66
14.66
30.99
0
1000
89.50
49.83
15.99
35.99
3.32
Table 2. Misclassification Error [%] on synthetic data with vary-
ing problem size (i.e. number of sampled models); the outlier per-
centage is fixed to 17% (i.e. five outliers); data size is fixed to
30 points. SA is used for quantum methods. This experiment is
the classical counterpart of the evaluations reported in the bottom
Fig. 3. All variants of quantum-enhanced methods in this table
(Farina et al. [15] and ours) use simulated annealing.
provide accurate results as the number of outliers increases.
This phenomenon can also be observed in Fig. 2, where out-
liers are erroneously detected as belonging to models sup-
ported by very few points.
On the contrary, RanSaCov,
RQuMF and De-RQuMF, which deal with outliers by de-
sign, are able to cope with a higher percentage of outliers
without suffering a huge performance degradation.
Scalability. In our second experiment, we focus on how
the performance scales with respect to the dimension of the
multi-model fitting problem at hand. Thus, having fixed
the outlier ratio at 17%, we vary the number m of sam-
pled models between 20 and 1000 (with a step size of 10
until 200 models are reached, and a step size of 100 after
that). Results of this experiment are reported in Tab. 2. It
can be noted that, when the number m of sampled models
increases, the misclassification error achieved by RQuMF
worsens, as the problem becomes more difficult to solve.
On the contrary, De-RQuMF can handle a higher number
of sampled models without impacting the overall misclas-
sification error, thanks to the decomposition principle. It
can also be observed that our approach outperforms previ-
ous quantum work (QuMF and De-QuMF), similarly to the
previous experiment.
Figure 3. Top: Misclassification Error [%] on synthetic data for
40 sampled models with increasing outliers (0-50%); the prob-
lem size is fixed to 70 (30 data points + 40 models). SA is used
for quantum methods. Bottom: Misclassification Error [%] for
synthetic data on quantum hardware with increasing problem size;
outlier percentage is fixed to 17%. Note that R-QuMF’s Emis
breaks starting from 120 qubits. Non-robust quantum methods
(i.e. QuMF and De-QuMF) are omitted because they fail in this
scenario.
Experiments on Quantum Hardware. In order to assess
the performance on Quantum Hardware, we also repeat the
previous experiment on D-Wave quantum annealer Advan-
tage 5.4. We consider a constant outlier ratio of 17% and
problem size varying from 50 to 120 qubits (which is the
sum of the number of points and models, namely 30 data
points plus 20 to 140 sampled models). Results are reported
in Bottom Fig. 3. We observe that RQuMF cannot handle
problems starting from 80 models (i.e., 110 qubits)–when
its error goes beyond 50%, while De-RQuMF remains ro-
bust for the entire range of problem sizes with Emis pre-
dominantly staying under 10%. This confirms the advan-
tages of a decomposed and iterative approach for handling
high-dimensional problems. The fact that RQuMF can not
manage large-scale problems is not surprising since quan-
tum hardware is far from being mature, in agreement with
previous work on quantum computer vision [3, 9]. It is
worth noticing that results on quantum hardware are worse
than the ones with SA reported in Tab. 2, as expected. For
example, SA can successfully manage problems with 100
sampled models with 1.33% error whereas QA fails.
To further analyze this aspect, we visualize in Left Fig. 4
6
Figure 4. Analysis of R-QuMF runs on our synthetic dataset. Left:
The number of physical qubits as a function of the number of log-
ical problem qubits for data points varying in the range [2; 32];
sampled models are 6 times the data size. Right: The sparsity of
eQ in % as the function of the input data size.
how the number of physical qubits (that reflects the effective
allocation of QPU resources) scales with respect to the in-
creasing problem size (represented by the number of logical
qubits) on sample problems. Although eQ in our QUBO is
significantly sparse (see Fig. 4-(right)), the number of phys-
ical qubits still increases superlinearly with respect to the
logical qubits, approaching the maximum size that can typ-
ically be handled by an adiabatic quantum computer. These
results are in line with previous quantum work [15].
5.2. Motion Segmentation on Real Data
We consider the AdelaideRMF dataset [42], which encom-
passes two distinct types of multi-model fitting tasks: fit-
ting fundamental matrices (15 image pairs with at least two
moving objects) and homographies (16 image pairs with at
least two planes). Our evaluation specifically targets the
multi-model sequences associated with both types of fitting
problems and we do not take into account single-model fit-
ting. The outlier percentages for these data are depicted in
the supplementary material, with nearly all sequences ex-
hibiting an outlier rate exceeding 30%, and some reaching
as high as 68% (for more detail about the outlier distribu-
tion see Fig. 9 from supplementary material): this presents
a substantial challenge for accurate model fitting.
The number of sampled models for each instance is six
times the number of points. As before, the preference ma-
trices used as input remain consistent across all evaluated
methods. We conducted each experiment 20 times, report-
ing the average Emis.
Unlike the synthetic experiments, where computational
resources are less constrained, for real data we do not re-
port results obtained using QA due to limited QPU time.
In addition to SA, we also consider the Gurobi solver in
order to enrich the evaluation.
In addition to the origi-
nal outlier-contaminated sequences, we also consider the
same sequences where outliers have been removed, to study
the impact of outliers and diverse behaviour of the con-
sidered methods.
We also analyse the efficacy of post-
processing, especially related to QuMF. Specifically, we ex-
amine whether 1) selecting the top k models identified by a
method and 2) designating all points not accounted for by
these k models as outliers, would yield robustness. Note that
this post-processing does not make sense for RanSaCov, for
it enforces hard constraints in its formulation and returns at
most k models. Aggregated results for fitting fundamental
matrices are given in Tab. 3, whereas those for homogra-
phies are reported in Tab. 4. See also Fig. 5 and 6 for sample
qualitative results. (More visualizations can be referenced
from the supplementary material in Fig. 16 and 17)
Similar conclusions can be drawn for fitting fundamental
matrices and homographies. It is not surprising that QuMF
outperforms our approach in the outlier-free scenario (as
seen in the first row of the aforementioned tables), given
that QuMF is specifically designed for such settings. Addi-
tionally, the lambda parameters in RQuMF have been fine-
tuned for scenarios involving outlier contamination.
In the case with outliers (second row of the tables), how-
ever, QuMF is significantly worse than RQuMF, therefore
showing that explicitly modeling outliers is indispensable
for achieving robustness. Using the post-processing largely
improves the performance of QuMF, which, however, is
still not comparable to RQuMF. Note also that the post-
processing assumes the knowledge of the number of true
models k which is typically unavailable in practice. Our
method is not influenced by post-processing, thereby show-
ing that it selects the right number of models in most cases,
and it is better than RanSaCov in many scenarios. Concern-
ing quantum methods, there are no significative differences
between using Gurobi or SA solvers. The fact that the de-
composed version of our approach does not improve upon
using the full QUBO, could be due to the task difficulty in
terms of outlier corruption compared to the simplified sce-
narios with synthetic data. From Fig. 5 it becomes apparent
that QuMF can segment the entire model accurately only
with the aid of the post-processing phase. Without post-
processing, QuMF struggles to segment the entire model
accurately, often choosing multiple models to explain what
essentially constitutes a single true model (see Fig. 6).
5.3. Plane Fitting on 3D Point Clouds
Finally, we illustrate the versatility and practicality of our
approach in a 3D plane fitting scenario. We consider a 3D
point cloud obtained through image-based 3D reconstruc-
tion [1]. The dataset comprises 10812 points; we sample
2000 models from those, focusing exclusively on planar
structures, with an inlier threshold set to 0.5 and use the
SA solver. Fig. 7 provides a visual example of a fitting per-
formed using our decomposed method, De-RQuMF. The re-
sults demonstrate that our method identifies distinct planes
within the point cloud. As expected, the fitting accuracy for
cylindrical sections of the building is lower as our method
supports plane sampling exclusively per design.
7
Outliers Settings
QuMF(SA)[15]
De-QuMF(SA)[15]
RanSaCov[27]
RQuMF(SA)
De-RQuMF(SA)
QuMF(G)[15]
De-QuMF(G)[15]
RQuMF(G)
De-RQuMF(G)
No Outliers
Mean
3.61
0.84
9.79
6.46
11.47
3.14
1.29
12.56
11.96
Median
2.68
0.28
7.97
2.41
10.55
1.87
0.93
11.35
10.55
With Outliers
Mean
40.37
26.19
7.22
10.46
12.69
45.81
26.21
13.14
12.84
Median
39.82
26.94
5.76
8.33
11.18
46.58
27.35
10.96
11.33
With Outliers +
Mean
19.76
8.89
NA
9.70
12.48
25.61
8.94
12.67
12.59
Post Processing
Median
19.67
6.55
NA
8.02
11.09
27.92
7.12
10.75
11.33
Table 3. Misclassification Error [%] for the 15 multi-model fundamental matrix sequences from AdelaideRMF [42] using SA or Gurobi
(for quantum methods). Results for RanSaCov without and with outliers are taken from [15] and [25], respectively.
Outliers Settings
QuMF[15]
De-QuMF[15]
RanSaCov[27]
RQuMF
De-RQuMF
No Outliers
Mean
54.10
13.94
-
20.77
19.11
Median
55.44
16.20
-
25.11
24.96
With Outliers
Mean
86.23
49.32
14.72
17.01
14.33
Median
86.25
46.35
14.38
16.72
15.76
With Outliers +
Mean
51.22
26.57
-
16.75
14.21
Post Processing
Median
50.76
20.46
-
16.63
16.05
Table 4. Misclassification Error [%] for the 16 multi-model ho-
mography sequences from AdelaideRMF [42] using SA (for quan-
tum methods).
Results for RanSaCov without outliers are not
available whereas those with outliers are taken from [25]. De-
QuMF with post-processing fails on at least one sequence in 16
out of 20 trials.
(a)
QuMF
(Post
Processing),
Emis = 16.72%
(b) DeQuMF (Post Processing),
Emis = 6.16%
(c) RQuMF, Emis = 5.87%
(d) De-RQuMF, Emis = 5.57%
Figure 5. Sample results of fundamental matrix fitting on biscuit-
book using SA. Our method performs as well as QuMF and De-
QuMF which use the information about the number of ground-
truth models.
(a) QuMF, Emis = 93.00%
(b) DeQuMF, Emis = 41.69%
(c) RQuMF, Emis = 2.9%
(d) De-RQuMF, Emis = 0.53%
Figure 6. Sample result of homography fitting on oldclassicswing
(32% outliers) using SA. In the absence of the true number of
models both QuMF and De-QuMF fail. Both our proposed meth-
ods achieve a near-perfect score.
Figure 7.
A plane fitting example using the De-RQuMF(SA)
method. The encircled dark red points are uncovered and treated
as outliers. The inset view on the top-right shows the same result
from a different virtual camera perspective.
6. Conclusion
Based on experimental evidence, we conclude that explic-
itly accounting for outliers in the model significantly lowers
the misclassification error across a wide variety of scenar-
ios, compared to all competing quantum-admissible meth-
ods. With respect to QuMF [15], the price to pay in RQuMF
for outlier robustness is an increased dimensionality of the
Q matrix, which is, however, a minor factor in our itera-
tive version and not a major limitation. More importantly,
RQuMF does not assume the true number of models ex-
plaining the data, which is highly advantageous in practice.
Although the attained results are promising, one of the
limitations of our approach is that performance is unpre-
dictable when outliers exceed 50% of the data.
While
RQuMF already works on real quantum hardware for small
problems, we believe its usefulness will increase as the
quantum hardware is improving.
Managing heteroge-
neous models (e.g. both planar and cylindrical models) is
a promising direction for future extensions.
Acknowledgements.
This paper is partially supported by
the PNRR-PE-AI FAIR project funded by the NextGenera-
tion EU program and by Geopride (ID: 2022245ZYB CUP:
D53D23008370001) funded by PRIN 2022.
8
References
[1] 3dflow srl. Samantha. https://www.3dflow.net/,
2024. online; accessed on the 01.08.2024. 7
[2] Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-manor, Pietro
Perona, David Kriegman, and Serge Belongie. Beyond pair-
wise clustering. In CVPR, pages 838–845, 2005. 2
[3] Federica Arrigoni,
Willi Menapace,
Marcel Seelbach
Benkner, Elisa Ricci, and Vladislav Golyanik. Quantum mo-
tion segmentation. In ECCV, pages 506–523. Springer, 2022.
1, 3, 5, 6
[4] Daniel Barath and Jiri Matas. Multi-class model fitting by
energy minimization and mode-seeking.
In ECCV, pages
229–245. Springer, 2018. 2
[5] Daniel Barath and Jiri Matas. Progressive-x: Efficient, any-
time, multi-model fitting algorithm. In CVPR, pages 3780–
3788, 2019. 2
[6] Daniel Barath, Denys Rozumny, Ivan Eichhardt, Levente Ha-
jder, and Jiri Matas. Finding geometric models by clustering
in the consensus space. In CVPR, pages 5414–5424, 2023. 2
[7] James Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs
K´egl.
Algorithms for hyper-parameter optimization.
In
NIPS, 2011. 5
[8] Harshil Bhatia, Edith Tretschk, Zorah L¨ahner, Marcel
Benkner, Michael M¨oller, Christian Theobalt, and Vladislav
Golyanik.
Ccuantumm: Cycle-consistent quantum-hybrid
matching of multiple shapes. In CVPR, 2023. 1
[9] Tolga Birdal, Vladislav Golyanik, Christian Theobalt, and
Leonidas Guibas. Quantum permutation synchronization. In
CVPR, 2021. 1, 5, 6
[10] Tat-Jun Chin, Hanzi Wang, and D. Suter. Robust fitting of
multiple structures: The statistical learning approach.
In
ICCV, pages 413–420, 2009. 2
[11] Tat-Jun Chin, David Suter, Shin-Fang Ch’ng, and James Q.
Quach. Quantum robust fitting. In ACCV, 2020. 1, 2, 3
[12] Prasanna Date and Thomas Potok. Adiabatic quantum linear
regression. Scientific Reports, 11:21905, 2021. 3
[13] Anh-Dzung Doan, Michele Sasdelli, David Suter, and Tat-
Jun Chin. A hybrid quantum-classical algorithm for robust
fitting. In CVPR, pages 417–427, 2022. 1, 2, 3
[14] Edward Farhi, Jeffrey Goldstone, Sam Gutmann, Joshua La-
pan, Andrew Lundgren, and Daniel Preda. A quantum adia-
batic evolution algorithm applied to random instances of an
np-complete problem. Science, 292(5516):472–475, 2001. 1
[15] Matteo Farina, Luca Magri, Willi Menapace, Elisa Ricci,
Vladislav Golyanik, and Federica Arrigoni. Quantum multi-
model fitting. In CVPR, pages 13640–13649, 2023. 1, 2, 3,
4, 5, 6, 7, 8, 11, 14
[16] Martin A. Fischler and Robert C. Bolles. Random sample
consensus: a paradigm for model fitting with applications to
image analysis and automated cartography. Commun. ACM,
24(6):381–395, 1981. 2, 3
[17] Vladislav Golyanik and Christian Theobalt. A quantum com-
putational approach to correspondence problems on point
sets. In CVPR, pages 9182–9191, 2020. 1
[18] Venu Madhav Govindu. A tensor decomposition for geomet-
ric grouping and segmentation. In CVPR, pages 1150–1157,
2005. 2
[19] Gurobi Optimization, LLC.
Gurobi Optimizer Reference
Manual, 2023. 5
[20] Hossam Isack and Yuri Boykov.
Energy-based geometric
multi-model fitting. In IJCV, pages 123–147, 2012. 2
[21] Suraj Jain and Venu Madhav Govindu. Efficient higher-order
clustering on the grassmann manifold. In ICCV, pages 3511–
3518, 2013. 2
[22] Scott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vecchi.
Optimization by simulated annealing. science, 220(4598):
671–680, 1983. 5
[23] Maximilian
Krahn,
Michelle
Sasdelli,
Fengyi
Yang,
Vladislav Golyanik, Juho Kannala, Tat-Jun Chin, and Tolga
Birdal. Projected Stochastic Gradient Descent with Quantum
Annealed Binary Gradients. In BMVC, 2024. 1
[24] Junde Li and Swaroop Ghosh. Quantum-soft qubo suppres-
sion for accurate object detection. In ECCV, 2020. 1
[25] Luca Magri.
Multiple Structure Recovery via Preference
Analysis in Conceptual Space. PhD thesis, University of Mi-
lan (Italy), 2015. 8
[26] Luca Magri and Andrea Fusiello. T-Linkage: A continuous
relaxation of J-Linkage for multi-model fitting. In CVPR,
pages 3954–3961, 2014. 2
[27] Luca Magri and Andrea Fusiello. Multiple models fitting as
a set coverage problem. In CVPR, pages 3318–3326, 2016.
2, 4, 5, 6, 8
[28] Luca Magri and Andrea Fusiello. Multiple structure recovery
via robust preference analysis. Image and Vision Computing,
67:1–15, 2017. 2
[29] Luca Magri, Filippo Leveni, and Giacomo Boracchi. Multi-
link: Multi-class structure recovery via agglomerative clus-
tering and model selection.
In CVPR, pages 1853–1862,
2021. 2
[30] Natacha Kuete Meli, Florian Mannel, and Jan Lellmann.
An iterative quantum approach for transformation estimation
from point sets. In CVPR, pages 529–537, 2022. 1
[31] Natacha Kuete Meli, Vladislav Golyanik, Marcel Seelbach
Benkner, and Michael Moeller. Qucoop: A versatile frame-
work for solving composite and binary-parametrised prob-
lems on quantum annealers. In CVPR, 2025. 1
[32] Pulak Purkait, Tat-Jun Chin, Hanno Ackermann, and David
Suter. Clustering with hypergraphs: the case for large hyper-
edges. In ECCV, pages 672–687, 2014. 2
[33] Michele Sasdelli and Tat-Jun Chin. Quantum annealing for-
mulation for binary neural networks. In DICTA, pages 1–10,
2021. 1
[34] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione.
Prediction by linear regression on a quantum computer.
Phys. Rev. A, 94:022342, 2016. 3
[35] Marcel Seelbach Benkner, Vladislav Golyanik, Christian
Theobalt, and Michael Moeller. Adiabatic quantum graph
matching with permutation matrix constraints. In 3DV, pages
583–592, 2020. 1
[36] Marcel
Seelbach
Benkner,
Zorah
L¨ahner,
Vladislav
Golyanik, Christof Wunderlich, Christian Theobalt, and
Michael Moeller.
Q-match: Iterative shape matching via
quantum annealing. In ICCV, pages 7586–7596, 2021. 1
9
[37] Mariano Tepper and Guillermo Sapiro.
Nonnegative ma-
trix underapproximation for robust multiple model fitting. In
CVPR, pages 2059–2067, 2017. 2
[38] Roberto Toldo and Andrea Fusiello. Robust multiple struc-
tures estimation with J-Linkage. In ECCV, pages 537–547,
2008. 2
[39] Esther Vincent and Robert Laganiere. Detecting planar ho-
mographies in an image pair. In ISPA, pages 182–187, 2001.
2
[40] Hanzi Wang, Guobao Xiao, Yan Yan, and David Suter.
Searching for representative modes on hypergraphs for ro-
bust geometric model fitting. IEEE TPAMI, 41(3):697–711,
2018. 2
[41] Dennis Willsch, Willsch Madita, Hans De Raedt, and Kris-
tel Michielsen.
Support vector machines on the D-Wave
quantum annealer. Computer physics communications, 248:
107006 –, 2020. 3
[42] Hoi Sim Wong, Tat-Jun Chin, Jin Yu, and David Suter. Dy-
namic and hierarchical multi-structure geometric model fit-
ting. In ICCV, pages 1044–1051, 2011. 5, 7, 8, 12, 13, 15,
16, 17
[43] Lei Xu, Erkki Oja, and Pekka Kultanen. A new curve detec-
tion method: randomized Hough transform (RHT). Pattern
recognition letters, 11(5):331–338, 1990. 2
[44] Frances Fengyi Yang, Michele Sasdelli, and Tat-Jun Chin.
Robust fitting on a gate quantum computer. In ECCV, 2024.
3
[45] Jan-Nico Zaech, Alexander Liniger, Martin Danelljan,
Dengxin Dai, and Luc Van Gool. Adiabatic quantum com-
puting for multi object tracking. In CVPR, pages 8811–8822,
2022. 1
[46] Ron Zass and Amnon Shashua. A unifying approach to hard
and probabilistic clustering. In ICCV, pages 294–301, 2005.
2
[47] Marco Zuliani, Charles S Kenney, and BS Manjunath. The
multiRANSAC algorithm and its application to detect planar
homographies. In ICIP, pages III–153–6, 2005. 2
10
Outlier-Robust Multi-Model Fitting on Quantum Annealers
Supplementary Material
This document provides additional details on our Ro-
bust Quantum Multi-Model Fitting algorithm and its de-
composed version (Sec. 7). It also provides further details
and visualizations for the experiments (Sec. 8) presented in
the main paper.
7. Algorithmic Details
Decomposed R-QuMF.
Real-world size problems are
currently intractable on a modern AQC, since the amount
of physical qubits required to map logical qubits increases
super-linearly. Our decomposed approach, following Fa-
rina et al. [15], mitigates this issue by decomposing the
preference-consensus matrix P (i.e., consensus set of sam-
pled models) into manageable sub-matrices with at most s
columns (i.e. sub-problem size) that can be confidently sam-
pled on modern quantum hardware using RQuMF. Alg. 1
summarises the decomposed version of our approach that
we call De-RQuMF.
The algorithm takes in input a dataset X, a sub-problem
size s, and inlier thresholds ϵ. It returns a set of labels l cor-
responding to a cover of X according to the retrieved mod-
els. The parameter s controls how many sampled models
are processed in each iteration of the decomposed method.
The first step (lines 3−10) consists of generating a pool of
M tentative models via random sampling and of computing
their consensus set. The number of hypothesis M is defined
as a multiple of the number of input points (line 3). Minimal
sample sets are sampled from X using localized sampling
(line 5) and used to fit a geometric model (line 6). Hence
residuals are computed (line 7). Residuals smaller than the
inlier threshold ϵ define the consensus sets of each sampled
model, which are stored as columns in the preference ma-
trix (line 8). The process is repeated until M models are
sampled (line 4).
The second step (lines 11 −25) involves the decomposi-
tion of the preference matrix P to define the logical graph
mapping. Specifically, P is partitioned into L sub-block Pj
having at most s columns each (line 12). Each sub-problem
Pj is converted to a QUBO form (lines 13−21) with its log-
ical graph and hence solved using RQuMF (line 19). Each
solution z represents the selected models, indicated by the
corresponding columns of Pj. In the pruning phase (line
22), the process retains only the chosen models, while the
rest are eliminated, thereby reducing the dimensionality of
Pj. Once the overall number of retained models falls below
s, a final execution of the RQuMF method is carried out
(line 26) to derive the final solution. The models selected
in this last iteration of RQuMF (line 26) undergo label as-
Algorithm 1 De-RQuMF Method
1: Input: Point set X, problem size s, inlier threshold ϵ
2: Output: Predicted labels l
Generate preference consensus matrix
3: M ←k × |X|, initialize P as a |X| × M zero matrix
4: for i = 0 to M −1 do
5:
Sample points from X
6:
fit geometric model
7:
compute residuals R∀{(xi, yi)} ∈X
8:
Update P[:, i] ←[r < ϵ?1 : 0 ∀r ∈R]
9:
i ←i + 1
10: end for
Logical graph mapping
11: while |columns(P)| > s do
12:
Partition P into L subproblems {Pj} of size s
13:
for j = 0 to L do
14:
z = RQuMF(Pj)
15:
procedure RQUMF(Pj)
16:
A ←concatenate [−I; Pj]
17:
eQ ←λ2 × AT A
18:
es ←concatenate [−1N; λ1 × 1M]
19:
z = solveQUBO( eQ, es)
20:
return z
21:
end procedure
22:
retain z from Pj
23:
j ←j + 1
24:
end for
25: end while
26: z = RQuMF(P)
Final model selection
27: return z
Generate label assignment
28: for each model zi in z do
29:
for each point xj in consensus set of zi do
30:
Assign label i to xj
31:
end for
32: end for
33: Solve a linear assignment problem to maximize label
coverage across X
34: return predicted labels l
signment (line 29) where the consensus set corresponding
to each model is labelled with the ith index of the model in
question. Subsequently, to optimise the coverage of these
labels across all data points, a linear assignment problem is
tackled (line 33) yielding the final labels l.
11
8. Additional Details On Experiments
We provide here additional details about the experiments
performed in Sec. 5 of the main paper.
Experiments Overview.
In Fig. 8 we report a general
overview of the experiments conducted on real and syn-
thetic datasets, highlighting the different solvers used (i.e.,
QA, SA etc) and the different configurations adopted to test
scalability and robustness of our proposed methods.
Figure 8. Overview of our experiments. “FM” refers to datasets
based on the fundamental matrix model whereas “HM” refers to
the homography model.
Outliers Percentage In Real Data Multi-Model Fitting
Tasks.
The plots in Fig. 9 show the outlier percentage
in multiple fundamental matrices and homography fitting
problems respectively, as referred in 5.2. Outliers typically
correspond to wrong key-point matches that cannot be de-
scribed by any model. It can be appreciated that in most of
the pairs related to fundamental matrices (which are related
to motion segmentation in two images) the outlier ratio is
greater than 30%, while for homographies (which are re-
lated to plane fitting) we have that in 5 pairs out of 16, there
are more than 50% of outliers, making the problem partic-
ularly challenging. This justifies the higher errors reported
in plane fitting with respect to the one attained on motion
segmentation.
Logical and physical graphs.
In Fig. 10 we report the
logical and physical graphs corresponding to sample prob-
lems related to the scalability experiment on the synthetic
dataset, where we maintain a constant outlier ratio of 17%
while expanding the sampled model size from 20 to 140.
The left-side images showcase the logical graph representa-
tion of the problem, where each node corresponds to a log-
ical qubit and the edges depict the coupling between these
qubits. Through minor embedding, these logical qubits are
mapped onto physical qubits within the quantum hardware.
Figure 9. Outlier Percentage of each sequence in AdelaideRMF
dataset [42]. Left: 15 image pairs for Fundamental Matrices fit-
ting, Right: 16 image pairs for homographies.
On the right side, the physical representation of these map-
pings is displayed, with each node representing a physi-
cal qubit. The colour inside the node reveals the measured
value in its most stable energy state, while the colour of the
node’s outer ring indicates the direction of bias, specifically
pointing out whether the coefficient of the linear component
in the optimization is positive or negative.
(a) 50 Logical Qubits
(b) 62 Physical Qubits
(c) 90 Logical Qubits
(d) 273 Physical Qubits
(e) 120 Logical Qubits
(f) 602 Physical Qubits
Figure 10. Left: These images depict logical graphs for different
problem sizes, reported in Fig. 3 of the main manuscript. Right:
The corresponding physical qubit embedding of logical graphs (a,
c, d) respectively using Pegasus topology on DWave Advantage5.4
as mentioned in Sec. 4.4 of the main paper.
Hyperparameter Tuning.
We studied the effect of hy-
perparameter tuning in our experiments. Specifically, we
studied how the objective value defined in Eq. (12) changes
with respect to λ1, λ2. In Fig. 11 we plot the objective land-
12
scape for fundamental matrix (FM) fitting problems, us-
ing the Tree-structured Parzen Estimator (TPE) discussed
in Sec. 4.4. It can be appreciated that the TPE strategy dis-
cards suboptimal values of the parameters and concentrates
more on the ones that result in lower values (i.e, a cluster
of points near the best objective values), contrary to grid
search which doesn’t consider the previous results for se-
lecting parameters for the future. Additionally, in relation
with the Tab. 3 the performance gap between QuMF and
our method in the outlier-free setting can be narrowed by
adjusting the lambda parameters to suit this specific case.
For instance, when optimized lambda values (λ1 = 4.8 and
λ2 = 0.6) are used for the fundamental matrix estimation
task in the absence of outliers, the misclassification error for
RQuMF decreases to 2.05, while for De-RQuMF, it drops to
6.18. Conversely, these parameters are not ideal for outlier-
prone scenarios, where misclassification rates increase from
10.46 to 16.95 for RQuMF and from 12.69 to 15.75 for De-
RQuMF. Therefore, we recommend adjusting the lambda
parameters based on the specific conditions to achieve opti-
mal results.
Figure 11. Lambda optimisation contour plot of RQuMF for fun-
damental matrix data (λ1 = 1.7 , λ2 = 0.1) using TPE.
Number Of Selected Models.
Our QUBO formulation
does not require knowing in advance the number of mod-
els, thus we assess whether the number of estimated mod-
els matches the ground truth ones.
Results are reported
for fundamental matrix fitting problems in different setups,
i.e., without outliers in Fig. 12 and with outliers in Fig. 13.
We compare the estimated number of models attained by
QuMF, DeQuMF, RQuMF, and De-RQuMF. The plots show
that our methods mostly select the true number of models,
in contrast to previous methods which estimate the right
number of models in the outlier-free scenario, but, without
any kind of post-processing are prone to over-estimation in
the presence of outliers. This is expected as, being based
on set-cover rather than on maximum coverage, QuMF and
DeQuMF try to maximize the number of inliers at the cost
of hallucinating more models in the solution. It is worth
noting, that even when coupled with post-processing, i.e.,
after providing the right number of models, QuMF failed to
achieve competitive results compared to our methods, high-
lighting the fact that these methods, in the presence of out-
liers, can not segment the data at the first place. See also
Tab. 4 from the main paper.
Figure 12. Average number of models selected by different meth-
ods for 15 multimodel sequences from AdelaideRMF [42] dataset
for fundamental matrix fitting in the absence of outliers. The mid-
dle bar in the grouped bars represents the ground truth number of
models, the left two bars represent QuMF, DeQuMF respectively
and the right bars represent our proposed methods.
Figure 13. Average number of models selected by different meth-
ods for 15 multimodel sequences from AdelaideRMF [42] dataset
for fundamental matrix fitting in the absence of outliers. The mid-
dle bar in the grouped bars represents the ground truth number of
models, the left two bars represent QuMF, DeQuMF respectively
and the right bars represent our proposed methods.
Execution Times.
Tab. 5 presents the average execution
time per sample of different methods on the AdelaideRMF
[42] dataset. It can be noticed that DeQuMF by far outper-
forms in terms of execution time among all the methods
confirming the advantages of the decomposed approach.
Our formulation is less efficient as the dimension of the Q
matrix has to encode also the number of points, while in
the previous approach, Q scales with the number of sam-
pled models. For reference, biscuitbook of the fundamen-
tal matrix fitting dataset has 341 data points.
In one of
the runs, the corresponding Q matrix dimension and node
count in the logical graph are (2046, 2046), 1793282 for
QuMF, (40, 40), 744 for DeQuMF, (2387, 2387), 1890341
for RQuMF, (381, 381), 3059 for DeRQuMF respectively.
Thus De-RQuMF has to solve a problem of almost ×5 big-
ger than DeQuMF (in terms of the logical graph). It’s cru-
cial to emphasise that the extended execution time of our
method significantly enhances the reliability of the results,
13
contrary to the DeQuMF approach, which exhibits limita-
tions as detailed in Tab. 5 of the main paper, our proposed
method delivers reliable performance without failure. Ad-
ditionally, we omit the reporting of methods execution time
on quantum hardware, as the anneal time remains constant
at 20µs, independent of problem size. Note that, with the
advent of stable Adiabatic Quantum Computers (AQC), our
approach is not only expected to become significantly faster
but also stay reliable.
Method
FM
HM
QuMF(SA) [15]
45.88
81.11
DeQuMF(SA) [15]
4.40
4.93
RQuMF(SA)(ours)
51.61
130.10
DeRQuMF(SA)(ours)
21.48
77.99
Table 5. Execution time (in seconds) of methods on different real
datasets using Apple Silicon M1 machine with 8GB RAM.
Qualitative Results.
We report the best and worst results
for our RQuMF method for a few sequences of the Adelaide
dataset in Fig. 14 and 15. Notably, our RQuMF method
consistently surpasses the previous QuMF method, its non-
decomposed counterpart, in performance across all scenar-
ios, including both best and worst cases. Moreover, it excels
beyond all other methods in three out of the four instances
as depicted in Fig. 14 and 15. Additional visualizations are
given in Fig. 16 and 17, confirming previous considerations.
14
Figure 14. Left: A sample of the best-case for RQuMF on the cubetoy (average Emis = 3.73%) sequence of the AdelaideRMF [42]
dataset for fundamental matrix (for the same sample average Emis for De-RQuMF, QuMF and DeQuMF is 4.13%, 42.95%, 23.71%
respectively). Right: A sample of the worst-case for RQuMF on the breadtoycar (average Emis = 21.02%) sequence of the AdelaideRMF
[42] dataset for fundamental matrix (for the same sample average Emis for De-RQuMF, QuMF and DeQuMF is 26.23%, 35.81%, 21.95%
respectively).
Figure 15. Left: A sample of the best-case for RQuMF on the nese (average Emis = 1.92%) sequence of the AdelaideRMF [42] dataset
for homography matrix (for the same sample average Emis for De-RQuMF, QuMF and DeQuMF is 2.14%, 77.70%, 28.29% respectively).
Right: A sample of the worst-case for RQuMF on the bonhall(average Emis = 41.60%) sequence of the AdelaideRMF [42] dataset for
homography matrix (for the same sample average Emis for De-RQuMF, QuMF and DeQuMF is 23.13%, 74.20%, 25.63% respectively).
15
(a) QuMF, Emis = 53.35%
(b) DeQuMF, Emis = 35.58%
(c) RQuMF, Emis = 6.12%
(d) De-RQuMF, Emis = 6.95%
(e) QuMF, Emis = 45.48%
(f) DeQuMF, Emis = 26.94%
(g) RQuMF, Emis = 4.86%
(h) De-RQuMF, Emis = 8.20%
(i) QuMF, Emis = 34.09%
(j) DeQuMF, Emis = 12.87%
(k) RQuMF, Emis = 7.79%
(l) De-RQuMF, Emis = 16.91%
Figure 16. Average Emis on some samples of AdelaideRMF [42] dataset for fundamental matrix fitting in the presence of outliers. one of
our proposed methods outperforms the previous methods every time.
16
(a) QuMF, Emis = 83.99%
(b) DeQuMF, Emis = 49.88%
(c) RQuMF, Emis = 2.86%
(d) De-RQuMF, Emis = 2.80%
(e) QuMF, Emis = 86.51%
(f) DeQuMF, Emis = 52.59%
(g) RQuMF, Emis = 20.53%
(h) De-RQuMF, Emis = 24.75%
(i) QuMF, Emis = 80.27%
(j) DeQuMF, Emis = 27.40%
(k) RQuMF, Emis = 35.67%
(l) De-RQuMF, Emis = 22.49%
Figure 17. Average Emis on some samples of AdelaideRMF [42] dataset for homography fitting in the presence of outliers. One of our
proposed methods outperforms the previous methods every time in addition to being reliable.
17

==================================================
