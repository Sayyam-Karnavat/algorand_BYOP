
==================================================
Paper 1
Title: Survey of Video Diffusion Models: Foundations, Implementations, and Applications
Abstract: Recent advances in diffusion models have revolutionized video generation,
offering superior temporal consistency and visual quality compared to
traditional generative adversarial networks-based approaches. While this
emerging field shows tremendous promise in applications, it faces significant
challenges in motion consistency, computational efficiency, and ethical
considerations. This survey provides a comprehensive review of diffusion-based
video generation, examining its evolution, technical foundations, and practical
applications. We present a systematic taxonomy of current methodologies,
analyze architectural innovations and optimization strategies, and investigate
applications across low-level vision tasks such as denoising and
super-resolution. Additionally, we explore the synergies between diffusionbased
video generation and related domains, including video representation learning,
question answering, and retrieval. Compared to the existing surveys (Lei et
al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which
focus on specific aspects of video generation, such as human video synthesis
(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on
diffusion-based approaches with a special section for evaluation metrics,
industry solutions, and training engineering techniques in video generation.
This survey serves as a foundational resource for researchers and practitioners
working at the intersection of diffusion models and video generation, providing
insights into both the theoretical frameworks and practical implementations
that drive this rapidly evolving field. A structured list of related works
involved in this survey is also available on
https://github.com/Eyeline-Research/Survey-Video-Diffusion.
PDF URL: http://arxiv.org/pdf/2504.16081v1
Submission Date: 2025-04-22 17:59:17+00:00

Full Content:
Survey of Video Diffusion Models:
Foundations, Implementations, and Applications
Yimu Wang∗
yimu.wang@uwaterloo.ca
University of Waterloo
Xuye Liu∗
xuye.liu@uwaterloo.ca
University of Waterloo
Wei Pang∗
w3pang@uwaterloo.ca
University of Waterloo
Li Ma∗
li.ma@scanlinevfx.com
Netflix Eyeline Studios
Shuai Yuan∗
shuai@cs.duke.edu
Duke University
Paul Debevec
debevec@netflix.com
Netflix Eyeline Studios
Ning Yu†
ning.yu@scanlinevfx.com
Netflix Eyeline Studios
Abstract
Recent advances in diffusion models have revolutionized video generation, offering supe-
rior temporal consistency and visual quality compared to traditional generative adversarial
networks-based approaches. While this emerging field shows tremendous promise in appli-
cations, it faces significant challenges in motion consistency, computational efficiency, and
ethical considerations. This survey provides a comprehensive review of diffusion-based video
generation, examining its evolution, technical foundations, and practical applications. We
present a systematic taxonomy of current methodologies, analyze architectural innovations
and optimization strategies, and investigate applications across low-level vision tasks such
as denoising and super-resolution. Additionally, we explore the synergies between diffusion-
based video generation and related domains, including video representation learning, ques-
tion answering, and retrieval. Compared to the existing surveys (Lei et al., 2024a;b; Melnik
et al., 2024; Cao et al., 2023; Xing et al., 2024c) which focus on specific aspects of video
generation, such as human video synthesis (Lei et al., 2024a) or long-form content genera-
tion (Lei et al., 2024b), our work provides a broader, more updated, and more fine-grained
perspective on diffusion-based approaches with a special section for evaluation metrics, in-
dustry solutions, and training engineering techniques in video generation. This survey serves
as a foundational resource for researchers and practitioners working at the intersection of
diffusion models and video generation, providing insights into both the theoretical frame-
works and practical implementations that drive this rapidly evolving field. A structured list
of related works involved in this survey is also available on GitHub.
∗These authors contributed equally to this work.
†Corresponding author.
1
arXiv:2504.16081v1  [cs.CV]  22 Apr 2025
Contents
1
Introduction
4
2
Foundation
5
2.1
Video generative paradigm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.1.1
GAN video models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.1.2
Auto-regressive video models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.1.3
Video diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.1.4
Auto-regressive video diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
Learning foundation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2.1
Denoising diffusion probabilistic models (DDPM) . . . . . . . . . . . . . . . . . . . . .
7
2.2.2
Denoising diffusion implicit models (DDIM) . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.3
Elucidated diffusion models (EDM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.4
Flow matching and rectified flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2.5
Learning from feedback and reward models . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2.6
One-shot and few-shot learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.2.7
Training-free methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.2.8
Token learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3
Guidance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3.1
Classifier guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3.2
Classifier-free guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.4
Diffusion model frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.4.1
Pixel diffusion and latent diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.4.2
Optical-flow-based diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.4.3
Noise scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.4.4
Agent-based diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.5
Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.5.1
UNet
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.5.2
Diffusion transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.5.3
VAE for latent space compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.5.4
Text encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3
Implementation
20
3.1
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.2
Training engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3.3
Evaluation metrics and benchmarking findings
. . . . . . . . . . . . . . . . . . . . . . . . . .
23
2
3.4
Industry solutions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
4
Applications
26
4.1
Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
4.1.1
Image condition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
4.1.2
Spatial condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
4.1.3
Camera parameter condition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
4.1.4
Audio condition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.1.5
High-level video condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
4.1.6
Other conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.2
Enhancement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.2.1
Video denoising and deblurring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
4.2.2
Video inpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
4.2.3
Video interpolation and extrapolation/prediction . . . . . . . . . . . . . . . . . . . . .
38
4.2.4
Video super-resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
4.2.5
Combining multiple video enhancement tasks . . . . . . . . . . . . . . . . . . . . . . .
41
4.2.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
4.3
Personalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
4.4
Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
4.5
Long video
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4.6
3D-aware video diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
4.6.1
Training on 3D dataset
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
4.6.2
Architecture for 3D diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
4.6.3
Camera conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
4.6.4
Inference-time tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
5
Benefits to other domains
49
5.1
Video representation learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
5.2
Video retrieval
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
5.3
Video QA and captioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
5.4
3D and 4D generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
5.4.1
Video diffusion for 3D generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
5.4.2
Video diffusion for 4D generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
6
Conclusion and outlook
53
3
Video generation models
Applications
Personalization
Consistency
Long video
3D aware generation
Video inpainting
Denoising/Deblurring
Inter- and extra-polation
Super-resolution
Benefits to other domains
Representation learning
Retrieval/Classification
QA/Understanding
3D and 4D generation
Training data
Real-world data
Synthetic data
Training engineering
Evaluation and
benchmarking
Implementations
Conditions
Noise
Images
Videos
Audios
Text
3D
Spatial
Camera Information
Architectures
UNet
VAE
Transformers
Paradigms
GAN
DDPM
DDIM
EDMs
Flow matching
Rectified flow
Foundations
Auto-regressive models
Text-encoders
Figure 1: Overview of video generation methods. Generally, the input conditions can be noises, images,
videos, audios, texts, and 3D point clouds.
The architectures (UNet, VAE, and/or Transformers) are
trained using GAN or diffusion models with training data of real-world data or synthetic data with different
paradigms. The applications are in several folds, e.g., video personalization, consistency-aware generation,
and long video generalization. On the other side, video generation models can also benefit other video tasks,
such as, video retrieval, understanding, and representation learning.
1
Introduction
Video generation (Ren et al., 2024; Zheng et al., 2024c; Davtyan & Favaro, 2022) has emerged as a critical and
transformative technology in recent years. The ability to generate high-quality, realistic videos has numerous
applications, from entertainment and advertising (Wang & Shi, 2023) to virtual reality (Hu et al., 2021) and
autonomous systems (Zhou et al., 2024c), further enabling enhanced user experiences, cost-effective content
creation, and new avenues for creative expression.
In the past few years, video generation (Clark et al., 2019; Aldausari et al., 2022; Hong et al., 2022) has
experienced remarkable progress, particularly with the adoption of Generative Adversarial Networks (GANs,
(Goodfellow et al., 2014)). Researchers have implemented various strategies to improve the temporal coher-
ence (Chai et al., 2023), realism, and diversity of generated videos. Despite these advancements, GAN-based
methods often face challenges in training stability and achieving high-quality outputs consistently. The in-
troduction of diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021; Sohl-Dickstein et al., 2015) has
revolutionized this area by offering a probabilistic framework that overcomes many limitations of GANs.
Diffusion-based models (Kwak et al., 2024; Chai et al., 2023; Wang & Yang, 2024; Ho et al., 2022c) have
demonstrated superior performance in generating temporally consistent and visually compelling videos, mo-
tivating further research in the domain.
However, diffusion-based video generation presents several fundamental challenges. A primary concern lies in
ensuring motion consistency across frames, a critical factor for producing temporally consistent and realistic
videos. Moreover, generated videos must adhere to physical rules, such as accurate object dynamics and
environmental interactions, to maintain realism. Long video generation poses another challenge, requiring
4
models to handle complex temporal dependencies over extended sequences. Furthermore, the computational
demands of training diffusion-based models are substantial, often leading to inefficiencies that limit scalability.
Similarly, inference acceleration remains a severe issue, as generating high-quality videos in real-time is
critical for many applications. Beyond these technical challenges, ethical considerations also play a pivotal
role, including mitigating biases in generated content and preventing the creation of harmful or misleading
visual outputs.
In response to the rapid advancements and emerging challenges of diffusion-based video generation, this
survey provides a systematic analysis of current methodologies, recent advances, and future directions in the
field.
Our contributions. This survey presents a comprehensive analysis of diffusion-based video generation,
focusing on both technical foundations and practical applications. While existing surveys (Lei et al., 2024a;b;
Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) have addressed specific aspects of video generation,
such as human video synthesis (Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on diffusion-based approaches.
Compared to related surveys (Xing et al., 2024c; Melnik et al., 2024), we offer more extensive coverage of
diffusion models and their applications with a detailed overview of datasets, evaluation metrics, industry
solutions, and training engineering techniques for video generation. The main contributions of this paper
are as follows:
• To the best of our knowledge, this is the most comprehensive survey on diffusion-based video gen-
eration, including model paradigms, learning foundation, implementation details, applications, and
relations to other domains.
• Compared to related surveys, our survey covers a broader scope of diffusion models and their ap-
plications with a detailed overview of datasets, evaluation metrics, industry solutions, and training
engineering techniques for video generation.
The rest of the survey is organized as follows: Section 2 lays the groundwork by exploring foundational
concepts, including video generation paradigms such as GAN-based models, auto-regressive models, and dif-
fusion models. Section 3 focuses on implementation, addressing practical aspects such as datasets, training
engineering techniques, and evaluation metrics, along with benchmarking findings to highlight model perfor-
mance. Section 4 covers diverse applications, including conditional generation tasks, enhancement methods
such as video denoising, inpainting, interpolation, extrapolation, and super-resolution, as well as personaliza-
tion, consistency, long-video generation, and emerging 3D-aware diffusion models. Finally, Section 5 discusses
the benefits of diffusion-based video generation to other domains, such as video representation learning, re-
trieval, QA, and 3D and 4D generation, emphasizing its broader impact in advancing related fields. This
structure provides a comprehensive and cohesive understanding of diffusion-based video generation, from
foundational principles to advanced applications.
2
Foundation
2.1
Video generative paradigm
2.1.1
GAN video models
GANs consist of a generator and a discriminator. The generator aims to produce realistic samples, while
the discriminator seeks to differentiate between the generated samples and the ground truth samples. For
video generation, GANs have been extended to model temporal consistency and generate realistic video
frames. For example, TGAN (Saito et al., 2017) uses two generators, with a temporal generator creating
motion features and an image generator producing video frames. TGAN-v2 (Saito et al., 2020) improves
efficiency by generating videos at multiple temporal resolutions using a cascade of generator modules. MoCo-
GAN (Tulyakov et al., 2018) separates latent space into motion and content parts, predicting motion vectors
auto-regressively, while DVD-GAN (Clark et al., 2019) generates all frames in parallel and conditions on
5
class labels. MoCoGAN-HD (Tian et al., 2020) disentangles content and motion differently, with a motion
generator predicting a latent motion trajectory. DIGAN (Yu et al., 2021) further leverages implicit neu-
ral representations to efficiently generate longer video sequences with a motion discriminator focused on
frame pairs rather than entire sequences. StyleGAN2 (Karras et al., 2020) introduced a highly expressive
and controllable latent space for photorealistic image synthesis, laying a foundational framework for video
synthesis through sparse motion cues and efficient control mechanisms. Building on this, StyleGAN-V (Sko-
rokhodov et al., 2022) advanced the field by modelling videos as continuous-time signals, improving temporal
coherence while supporting high-resolution and long-duration generation. Extending these developments,
StyleInv (Wang et al., 2023i)) proposed a non-autoregressive GAN inversion network, achieving smoother
motion transitions and enabling style transfer through fine-tuning. AniFaceGAN (Wu et al., 2022c) adapted
StyleGAN2 into an animatable 3D-aware GAN, enabling multiview-consistent face animation generation,
particularly for face-centric applications. Recently, SIDGAN (Muaz et al., 2023) tackled high-resolution
dubbed video generation by introducing shift-invariant learning and a pyramid-based decoder, enabling ac-
curate lip-syncing and high-quality textures even under challenging conditions. Further advancing lip-sync
technology, StyleLipSync (Ki & Min, 2023) leveraged pre-trained StyleGAN to generate temporally con-
sistent lip-synced videos with pose-aware masking and moving-average-based latent smoothing, enabling
high-fidelity results for arbitrary identities. Details about the GAN application on video generation can be
found (Aldausari et al., 2022).
2.1.2
Auto-regressive video models
Auto-Regressive models generate data by modelling the conditional distribution of each data point given its
preceding data points. In the context of video generation, Auto-Rregressive models generate realistic and
coherent videos by predicting each frame based on previously generated frames. Generally, three strategies
have been employed. The first is pixel-level auto-regression. Traditional representative methods, such as
Video Pixel Networks (VPN) (Kalchbrenner et al., 2017), used LSTMs to capture temporal dependencies
and PixelCNN (Reed et al., 2017) to model spatial and color dependencies within each frame. The second
is frame-level auto-regression. (Huang et al., 2022a) proposed auto-regressive GAN to predict frames based
on a single still frame. The third is latent-level auto-regression, which significantly saves processing time due
to reduced data redundancy and achieves a good time quality trade-off (Rakhimov et al., 2020; Seo et al.,
2022; Yan et al., 2021; Ge et al., 2022; Hong et al., 2023a).
2.1.3
Video diffusion models
GAN-based and Auto-Regressive models often face challenges in maintaining temporal consistency and com-
putational efficiency when generating videos. While GANs struggle with maintaining temporal consistency
and often produce unstable or inconsistent frames, diffusion models excel at generating smooth, coherent
sequences, particularly for long videos, because their iterative denoising process ensures frame-to-frame con-
sistency. Also, despite Auto-Regressive model can handle both continuous and discrete data, it normally
requires high computational cost. In terms of frame quality, diffusion models generate more detailed and
higher-resolution outputs compared to GANs, which are prone to artifacts from adversarial training, and
autoregressive models, which face resolution limitations due to the compounding of errors in sequential pre-
diction[cite]. Diffusion models also offer superior controllability and local editing capabilities because they
can refine specific parts of the video during the denoising process without affecting other regions while GANs
and autoregressive methods generate frames in a more holistic or sequential manner[cite].
Diffusion models offer a more efficient solution by framing video generation as a denoising process. Early
models, such as the Video Diffusion Model (VDM) (Ho et al., 2022c), adapted the U-Net architecture to 3D
for spatio-temporal video generation but faced high computational costs and limited resolution.
Later, Imagen-Video (Ho et al., 2022a) uses a cascaded diffusion process (Ho et al., 2022b) to generate
high-resolution videos. It introduces temporal attention layers between spatial layers to capture motion
information effectively.
Make-a-Video (Singer et al., 2022) is another powerful competitor in text-video
synthesis by conditioning on the CLIP semantic space. It generates keyframes based on text input and uses
a cascade of interpolation and upsampling diffusion models to ensure high consistency and fidelity. However,
both of these pioneering models come with high computational costs. To address this, MagicVideo (Zhou
6
et al., 2022) introduced low-dimensional latent embedding space in their diffusion process by a pretrained
variational-auto-encoder (VAE), significantly reducing computational demands. Video LDM (Blattmann
et al., 2023b) also extended the adaptation of the Latent Diffusion Models (Rombach et al., 2022) to text-to-
video generation tasks by adding temporal attention layers to a pre-trained text-to-image diffusion model and
fine-tune them on labeled video data. Their model is capable of generating long-driving car video sequences
auto-regressively and producing videos of personalized characters. Recently, CCEdit (Feng et al., 2024a)
leverages diffusion models with a trident network architecture that decouples structure and appearance
control, ensuring precise and creative editing capabilities.
2.1.4
Auto-regressive video diffusion models
Some recent works further exploit auto-regressive framework in the context of video generation with diffusion
models (Weng et al., 2024; Jin et al., 2024; Xie et al., 2024a). ART•V (Weng et al., 2024) first utilized
autoregressive diffusion to generate frames sequentially, employing masked diffusion to enhance coherence
and reduce inconsistencies. Subsequently, Pyramidal Flow Matching (Jin et al., 2024) introduced spatial
and temporal pyramids to optimize autoregressive generation, reducing computational costs and improving
scalability.
Progressive Autoregressive Video Diffusion Models
(Xie et al., 2024a) refined this by using
a progressive noise schedule, enabling smoother transitions in long videos.
CausVid
(Yin et al., 2024)
advanced the field by converting bidirectional diffusion models into causal autoregressive generators through
asymmetric distillation, achieving real-time frame-by-frame generation with low latency and reduced error
accumulation.
2.2
Learning foundation
2.2.1
Denoising diffusion probabilistic models (DDPM)
Denoising diffusion probabilistic models (DDPM) Ho et al. (2020); Nichol & Dhariwal (2021); Sohl-Dickstein
et al. (2015) consist of two interconnected Markov chains: a forward diffusion process that gradually adds
noise to the data, and a reverse process that removes noise to recover the data.
The forward process
transforms complex data distributions into a simpler prior, typically Gaussian noise, while the reverse process
learns how to reconstruct the data from this noisy representation by modeling the reverse transitions. Data
generation involves first sampling a random noise vector from the prior, and then applying the reverse chain,
step-by-step, to iteratively refine the noise into meaningful data.
The key challenge lies in training the
reverse Markov chain so that it effectively mimics the true reversal of the forward process.
Mathematically, let the data be sampled from a distribution x0 ∼q(x0).
The forward Markov process
produces a sequence of progressively noisier variables x1, x2, ..., xT through a transition kernel q(xt|xt−1).
The joint distribution of these variables, conditioned on the original data x0, can be expressed as:
q(x1, ..., xT |x0) =
T
Y
t=1
q(xt|xt−1).
(1)
Typically, the transition kernel is defined as a Gaussian distribution:
q(xt|xt−1) = N(xt;
p
1 −βtxt−1, βtI),
(2)
where βt ∈(0, 1) is a pre-determined noise schedule controlling the amount of noise added at each step.
In the reverse process, the goal is to recover the data by sampling from a reverse Markov chain. The chain
starts with a noise sample xT ∼p(xT ) = N(0, I) from a simple Gaussian prior, and transitions through a
learnable kernel pθ(xt−1|xt), parameterized by neural networks. This kernel takes the form:
pθ(xt−1|xt) = N(xt−1; µθ(xt, t), Σθ(xt, t)),
(3)
where µθ(xt, t) and Σθ(xt, t) represent the learned mean and variance, respectively. By iteratively applying
this reverse kernel, we generate the data x0 through a sequence of updates starting from the noise sample
xT and progressing through xT −1, xT −2, . . . , x0. Figure 2 illustrates the graphical model for DDPM.
7
!
xT
−! · · · −!
xt
−−−−−! xt−1 −! · · · −! x0
p✓(xt−1|xt)
q(xt|xt−1)
Figure 2: The directed graphical model for DDPM(Ho et al., 2020)
This iterative sampling process captures the reverse dynamics of the original forward diffusion, gradually
refining random noise into a meaningful data point, such as an image or text.
2.2.2
Denoising diffusion implicit models (DDIM)
This original DDPM formulation of the generation process in the form of a reverse Markov chain has more
recently been complemented by a non-Markovian alternative denoted as denoising diffusion implicit models
(DDIM), which offers a deterministic and more efficient generation process. In DDPM, the sampling process
typically requires a large number of iterations to achieve satisfactory results.
In contrast, the DDIM is
designed to accelerate the sampling process by reducing the number of iterations needed. Here, a backward
denoising step can be computed with
xt−1 =
√¯αt−1xt −√1 −¯αtϵθ(xt, t)
√¯αt
+
p
1 −¯αt−1ϵθ(xt, t).
(4)
One distinct advantage of this formulation of the denoising process is that it allows for accurate reconstruction
of the original input video x0 from the noise at time step T. This technique, called DDIM inversion, can be
utilized for applications such as image and video editing.
2.2.3
Elucidated diffusion models (EDM)
EDM (Karras et al., 2022) introduce a unified and optimized framework for diffusion-based generative models
by reinterpreting sampling and training processes.
Unlike DDPMs, which rely on stochastic Markovian
sampling, or DDIMs, which employ deterministic non-Markovian updates, EDMs aim to optimize both
training and sampling efficiency by introducing tailored sampling schemes and reparameterization strategies.
EDMs build upon the common framework of diffusion models by considering the data distribution q(x0)
perturbed by Gaussian noise, leading to a sequence of noisy variables {xt} over time.
While DDPMs use a stochastic reverse process, EDMs leverage an ordinary differential equation (ODE) for
deterministic sampling, as inspired by DDIMs. The probability flow ODE Song et al. (2020a) is expressed
as:
dx
dt = −σ(t)∇x log p(x; σ(t)),
(5)
where σ(t) is the noise schedule at time t. Unlike DDIMs, EDMs employ second-order solvers, such as Heun’s
method, to balance accuracy and computational efficiency.
To ensure stable training, EDMs redefine the loss function with noise-dependent preconditioning.
The
denoising target Dθ(x; σ) is expressed as:
Dθ(x; σ) = cskip(σ)x + cout(σ)Fθ(cin(σ)x; cnoise(σ)),
(6)
where Fθ is a neural network, and cskip, cout, and cin are scale-dependent coefficients. The overall training
objective becomes:
L = Eσ,x0,ϵ

λ(σ)∥Dθ(x + σϵ; σ) −x0∥2
,
(7)
where λ(σ) is a weighting function that emphasizes intermediate noise levels during training.
Compared to DDPMs, EDMs achieve faster sampling by reducing the number of neural function evaluations
through efficient solvers and noise schedules. Unlike DDIMs, which rely on deterministic updates, EDMs
integrate adaptive techniques to minimize truncation errors further, further improving fidelity and flexibil-
ity. This unified and modular design allows EDMs to achieve better results while significantly reducing
computational overhead(Karras et al., 2022).
8
2.2.4
Flow matching and rectified flow
Despite denoising diffusion being powerful for video generation tasks, another branch in the family of
differential-equation-based generative models began to arise recently, namely the flow matching genera-
tive models(Lipman et al., 2023; Liu et al., 2022c; Tong et al., 2023). While diffusion models learn the score
function of a specific SDE, flow matching aims to model the vector field implied by an arbitrary ODE directly.
A neural network is used for approximating the vector field, and the ODE can also be numerically solved
to obtain data samples. The design of such ODE and vector field often considers linearizing the sampling
trajectory and minimizing the transport cost(Tong et al., 2023). As a result, flow matching models have
simpler formulations and fewer constraints but better quality. VoiceBox(Le et al., 2023) shows the potential
of flow matching in fitting large-scale speech data, and LinDiff(Liu et al., 2023b) shares a similar concept
in the study of vocoders. More importantly, the rectified flow(Liu et al., 2022c) technique in flow matching
models further straightens the ODE trajectory in a concise way. By training a flow matching model again
but with its own generated samples, the sampling trajectory of rectified flow theoretically approaches a
straightforward line, which improves the efficiency of sampling. Rectified Flow(Liu et al., 2022c; 2023d) is a
generative model that efficiently transitions between two distributions π0 and π1 by solving ordinary differen-
tial equations (ODEs). Building on flow matching(Lipman et al., 2023) and stochastic interpolants(Albergo
et al., 2023), Rectified Flow introduces a rectification mechanism to optimize the ODE path, improving the
efficiency and stability of the sampling process. This approach establishes a strong theoretical baseline for
diffusion acceleration and provides a unified perspective on generative models.
The core dynamics of Rectified Flow are governed by the following ODE:
dZt = v(Zt, t)dt,
where Zt represents the data point at time t ∈[0, 1], which evolves continuously along the flow. The vector
field v(Zt, t) determines the direction and velocity of Zt’s movement. This vector field is parameterized using
a neural network, which is optimized to align the trajectories with the target distributions. The term dt
represents an infinitesimal time step, enabling a smooth transition between π0 and π1.
To learn the vector field v, Rectified Flow minimizes the following objective:
min
v
Z 1
0
E

∥(X1 −X0) −v(Xt, t)∥2
dt,
where X0 and X1 are sampled from the initial distribution π0 and target distribution π1, respectively.
The term Xt = tX1 + (1 −t)X0 represents a linear interpolation between X0 and X1. The loss function
measures the squared difference between the actual direction of interpolation (X1 −X0) and the vector field
v(Xt, t). This ensures that v effectively captures the necessary dynamics to transition samples between the
two distributions while preserving their structure.
A key theoretical advantage of Rectified Flow is that the ODE formulation guarantees that paths do not
cross. The vector field v(Zt, t) ensures unique and deterministic trajectories for all starting points, avoiding
ambiguities in sampling and maintaining stability during inference. With its focus on optimizing vector fields
and straightening trajectories, Rectified Flow offers a powerful and efficient alternative to traditional diffusion
models. By combining a strong theoretical foundation with practical efficiency, it sets a new benchmark for
generative modeling tasks.
2.2.5
Learning from feedback and reward models
The integration of human feedback into video diffusion models has evolved significantly, beginning with
foundational ideas in reinforcement learning and progressing to advanced mechanisms leveraging multimodal
and AI-driven annotations. Early work C2M(Ardino et al., 2021) user interaction by selecting objects in
the scene and specifying their final location through mouse clicks to generate video in complex scenes.
InstructVideo (Yuan et al., 2023) further emphasized aligning video generation with human preferences by
9
incorporating temporal coherence loss:
Lcoherence = Eθ


F −1
X
j=1
|(vj+1 −vj) −(oj+1 −oj)|2
2

,
(8)
where vj and oj denote predicted and ground truth frames. This method enhanced temporal consistency but
primarily focused on supervised fine-tuning with explicit human feedback signals. Subsequent research intro-
duced more scalable methods for integrating feedback. (Furuta et al., 2024b) demonstrated the use of binary
AI feedback from vision-language models (VLMs) to optimize dynamic object interactions in videos. Their
unified reinforcement learning framework leveraged AI annotations to align outputs with human-like quality
assessments, overcoming the limitations of manual data annotation. This concept was further extended in
T2V-Turbo(Li et al., 2024d) and its subsequent version T2V-Turbo-V2(Li et al., 2024e), which significantly
improved the quality and speed of text-to-video generation through the incorporation of AI feedback, enhanc-
ing dynamic object interaction.(Furuta et al., 2024a) In parallel, efforts to construct comprehensive preference
datasets began to take shape. (Wu et al., 2024g) presented VIDEOPREFER, a large-scale dataset created
using multimodal large language models (MLLMs) such as GPT-4 Vision. This dataset provided 135,000
preference annotations, allowing the training of VIDEORM, the first general-purpose reward model tailored
for video preferences. VIDEORM incorporated temporal dynamics, significantly enhancing the alignment
of video outputs with human expectations. (Furuta et al., 2024a) further proposed a robust framework for
improving dynamic interactions in video generation, showcasing AI feedback as a critical factor for realistic
object behavior in generated content. Other works, such as FreeScale(Qiu et al., 2024c)indirectly addressed
alignment by improving high-resolution visual generation in video diffusion models. Their approach, while
not explicitly focused on feedback, laid the groundwork for integrating multi-scale visual consistency with
human-preferred attributes in video content. Some advanced methods, like VINE(Lu et al., 2024b) also
highlighted the robustness of feedback-aligned generative priors for specific tasks like watermarking and
video editing. These innovations showed how feedback mechanisms could extend beyond video generation
to broader applications, ensuring fidelity and alignment with user intent.
2.2.6
One-shot and few-shot learning
One-shot and few-shot fine-tuning techniques enable generative models to adapt to new tasks or domains
with minimal training data. These approaches are particularly valuable in scenarios where acquiring large
datasets is infeasible, yet high-quality outputs are required. By leveraging either a single example (one shot)
or a small set of examples (few shots), these methods refine pretrained generative models to specialize in
specific tasks while maintaining generalization capabilities.
One-shot learning refers to a model’s ability to learn from just a single example. In generative modelling, this
is particularly useful when only one instance of the target data is available. For example, Tune-A-Video (Wu
et al., 2023b) demonstrates how a pre-trained text-to-image diffusion model can be fine-tuned on a single
text-video pair, enabling it to generate new videos that maintain the temporal consistency of the original
while adhering to text prompts. However, Tune-A-Video is based on the template video and edits it with
different content prompts which will restrict the freedom of generative video.
Few-shot fine-tuning extends this capability by adapting models to new domains or tasks using a small
number of training samples. Several techniques have emerged to enhance the generative capacity of models
under few-shot scenarios. LAMP (Learn A Motion Pattern) (Wu et al., 2023c) proposes a motion learning
model to capture the motion pattern from the training data and utilizes about 8˜16 videos to tune the
pretrained T2I model. However, it is constrained by their incomplete capture of image features. MAIM
(Huang et al., 2025) addresses this by integrating image features via the CLIP encoder, enabling videos with
intricate scenes and multiple objects.
2.2.7
Training-free methods
Training-free approaches enable direct video generation without additional training or fine-tuning, making
them valuable for black-box scenarios.
10
Text2Video-Zero (Khachatryan et al., 2023) introduced cross-frame attention based on pretrained text-to-
image diffusion model.
FateZero (Qi et al., 2023) enhanced this through intermediate attention maps,
and Free-Bloom (Huang et al., 2023a) combined GPT-3 with image diffusion models for improved semantic
coherence. PEEKABOO (Jain et al., 2024b) enabled spatio-temporal control through masked diffusion using
a latent diffusion model-based video generation model, while ControlVideo (Zhang et al., 2023e) extended
ControlNet with an interleaved-frame smoother to generate consistent controllable and structurally smooth
video. DreamVideo-2 (Wei et al., 2024b) proposed zero-shot subject-driven video customization with precise
motion control, enabling subject-preserving and motion-specific video editing. Following this, VideoElevator
(Zhang et al., 2024f) encapsulates T2V to enhance temporal consistency and harnesses T2I to provide more
faithful details. Recent innovations include GPT4Motion (Lv et al., 2024)’s combination of GPT-4, Blender,
and text-to-image diffusion models for physics-aware generation, AnyV2V (Ku et al., 2024)’s universal editing
framework built on an image editing model combined with an existing image-to-video generation model to
generate the edited video through temporal feature injection.
Video Custom Diffusion (Magic-Me) (Ma
et al., 2024e) built a training-free 3D Gaussian Noise Prior for video frame initialization, reconstructing
inter-frame correlation to achieve identity-preserved generation.
2.2.8
Token learning
Token learning, such as Textual Inversion Gal et al. (2022), is another paradigm within generative modeling.
Instead of fine-tuning the entire model, token learning focuses on training specific embeddings or tokens
that represent new concepts or ideas. By using textual inversion, the model learns a new token that can
be combined with existing tokens to guide generation without requiring large datasets or significant changes
to the model’s architecture. However, optimizing the single token embedding vector has limited expressive
capacity because of its limited optimized parameter size. In addition, using one word to describe concepts
with rich visual features and details is very hard and insufficient. Animate-A-Story(He et al., 2023a) ex-
tended traditional textual inversion to timestep-variable textual inversion (TimeInv) and adapted it to video
generation task to ensure flexible controls over structure and characters. DreamVideo(Wei et al., 2024c)
further incorporated textual inversion to capture the fine appearance of the subject from provided images to
generate videos with customized subjects.
2.3
Guidance
2.3.1
Classifier guidance
In generative modeling, diffusion models such as DDPM and DDIM can be guided using classifiers to improve
the quality of conditional generation. This approach, called classifier guidance (Dhariwal & Nichol, 2021),
integrates class information into the diffusion process by leveraging the gradients of a classifier trained on
noisy data. The classifier’s gradients help the model generate data conditioned on a given label, improving
the overall sample quality for specific categories.
Mathematically, the classifier-guided diffusion process modifies the reverse noising process by incorporating
the classifier’s log-probability gradients. This guidance is applied at each timestep of the diffusion process,
allowing the model to generate more accurate conditional samples.
The reverse noising process is represented as follows:
pθ,ϕ(xt|xt+1, y) = Z · pθ(xt|xt+1) · pϕ(y|xt)
(9)
where pθ is the reverse noising process from the original diffusion model, pϕ(y|xt) is the classifier’s conditional
probability, and Z is a normalizing constant.
We now describe how this approach can be applied to both DDPM and DDIM frameworks. In particular,
they(Dhariwal & Nichol, 2021) train a classifier pϕ(y|xt, t) on noisy images xt, and then use gradients
∇xt log pϕ(y|xt, t) to guide the diffusion sampling process towards an arbitrary class label y.
Algorithm for classifier-guided DDPM sampling. In the DDPM framework, classifier guidance can
be incorporated by modifying the reverse diffusion process. The update at each timestep involves sampling
11
from a Gaussian distribution with a mean adjusted by the classifier gradients. Detailed workflow can be
referred to Algorithm algorithm 1.
Algorithm 1 Classifier-guided DDPM sampling, given a diffusion model (µθ(xt), Σθ(xt)), classifier pϕ(y|xt),
and gradient scale s.(Dhariwal & Nichol, 2021)
1: Input: class label y, gradient scale s
2: xT ←sample from N(0, I)
3: for t = T to 1 do
4:
µ, Σ ←µθ(xt), Σθ(xt)
5:
xt−1 ←sample from N(µ + sΣ∇xt log pϕ(y|xt), Σ)
6: end for
7: return x0
Algorithm for classifier-guided DDIM sampling. For the DDIM framework, the deterministic nature
of the sampling process requires a slightly different approach. The classifier gradient is used to modify the
epsilon prediction, which guides the reverse sampling process in DDIM. Deatailed workflow can be referred
to Algorithm algorithm 2.
Algorithm 2 Classifier-guided DDIM sampling, given a diffusion model ϵθ(xt), classifier pϕ(y|xt), and
gradient scale s. (Dhariwal & Nichol, 2021)
1: Input: class label y, gradient scale s
2: xT ←sample from N(0, I)
3: for t = T to 1 do
4:
ϵ ←ϵθ(xt) −√1 −α¯t∇xt log pϕ(y|xt)
5:
xt−1 ←
√α
¯
t−1xt−√
1−α¯tϵ
√α¯t
+ p1 −α ¯
t−1ˆϵ
6: end for
7: return x0
In the DDIM case, it defines a new epsilon prediction ˆϵ(xt), which incorporates the classifier’s guidance into
the joint distribution of the diffusion model and the classifier:
ˆϵ(xt) := ϵθ(xt) −
p
1 −α¯t∇xt log pϕ(y|xt)
(10)
This modified epsilon prediction is then used to guide the reverse sampling process, replacing the standard
noise predictions from the original model.
The development of classifier guidance for diffusion models has sparked significant advances in video genera-
tion applications. (Shi et al., 2023a) explored compositionality in visual generation by using latent classifiers
to guide diffusion models in semantic space, demonstrating effective control over multiple attributes and
their relationships. (Chung et al., 2024) further reformulated classifier guidance to better respect manifold
constraints crucial for temporal consistency in videos. (Lian et al., 2024) then incorporated large language
models to generate dynamic scene layouts as intermediate representations, effectively guiding the diffusion
process for coherent motion and interactions. These developments demonstrate the field’s progression from
basic guidance mechanisms to more sophisticated approaches specifically tailored for video generation chal-
lenges.
2.3.2
Classifier-free guidance
While classifier-based guidance successfully allows for the conditional generation of data by leveraging the
gradients from a pre-trained classifier, it has several limitations. For instance, classifier guidance requires
the maintenance of a separate classifier, and its performance is closely tied to the quality of this classifier.
To address these concerns, classifier-free guidance(Ho & Salimans, 2022) offers an alternative that achieves
similar results without the need for a dedicated classifier.
12
Classifier-free guidance modifies the model’s predicted noise directly by using conditioning information,
allowing for conditional generation without relying on external classifier gradients. This approach avoids the
complexities of training and maintaining an additional classifier.
Rather than training a separate classifier model, they(Ho & Salimans, 2022) adopt an approach that jointly
trains an unconditional denoising diffusion model, pθ(x), and a conditional model, pθ(x|y). Both models
are parameterized through a shared score estimator: ϵθ(xt) for the unconditional case and ϵθ(xt, y) for the
conditional case. They use a single neural network to parameterize both models and then perform sampling
using a linear combination of conditional and unconditional scores, as shown in Equation 11.
ˆϵθ(xt, y) = (1 + w)ϵθ(xt, y) −wϵθ(xt)
(11)
Here, w is a guidance scale that controls the strength of the conditional information. This formulation allows
the model to perform conditional sampling without relying on classifier gradients.
Algorithm for Classifier-Free Guidance describes the sampling procedure for classifier-free guidance,
which adjusts the score estimates directly within the diffusion model without requiring any classifier gradi-
ents. Algorithms algorithm 3 and algorithm 4 describe training and sampling with classifier-free guidance
in detail.
Algorithm 3 Joint training a diffusion model with classifier-free guidance
Require: puncond: probability of unconditional training
1: repeat
2:
(x, y) ∼p(x, y)
▷Sample data with conditioning information from the dataset
3:
y ←∅with probability puncond
▷Randomly discard conditioning to train unconditionally
4:
λ ∼p(λ)
▷Sample log SNR value
5:
ϵ ∼N(0, I)
6:
xλ = αλx + σλϵ
▷Corrupt data to the sampled log SNR value
7:
Take gradient step on ∇θ∥ϵθ(xλ, y) −ϵ∥2
▷Optimize the denoising model
8: until converged
Algorithm 4 Conditional sampling with classifier-free guidance
Require: w: guidance strength
Require: c: conditioning information for conditional sampling
Require: λ1, . . . , λT : increasing log SNR sequence with λ1 = λmin, λT = λmax
1: x0 ∼N(0, I)
2: for t = 1, . . . , T do
3:
˜ϵt−1 ←(1 + w)ϵθ(xt−1) −wϵθ(xt−1)
▷Form the classifier-free guided score at log SNR λt−1
4:
xt ←
(xt−1−αλt−1 ˜ϵt−1)
αλt−1
▷Sampling step (could be replaced by another sampler, e.g., DDIM)
5:
xt ∼N(˜µλt−1,λt(xt−1, xt), ˜σ2
λt−1,λt)
6: end for
7: return xT
This algorithm leverages the classifier-free guidance to adjust the sampling procedure by computing the
guided score at each timestep. Instead of using gradients from a classifier, the model directly modifies its
own score estimates to account for the conditional information.
2.4
Diffusion model frameworks
This section explores several key frameworks within diffusion models, highlighting their distinctive approaches
and applications.
13
2.4.1
Pixel diffusion and latent diffusion
Remarkable progress has been made in developing large-scale pre-trained text-to-Video Diffusion Models
(VDMs), including proprietary models (e.g., Make-A-Video (Singer et al., 2023), Imagen Video (Ho et al.,
2022c), Video LDM (Blattmann et al., 2023b), Gen-2 (Esser et al., 2023)) and open-sourced ones (e.g.,
VideoCrafter (He et al., 2023b), ModelScopeT2V (Wang et al., 2023c). These VDMs can be categorized into
two primary architectures: (1) Pixel-based VDMs and (2) Latent-based VDMs. The former directly operates
on pixel values for denoising, as exemplified by Make-A-Video Singer et al. (2023), Imagen Video Ho et al.
(2022c), and PYoCo Ge et al. (2023). The latter manipulates the compressed latent space within a variational
autoencoder (VAE), as demonstrated by Video LDM Blattmann et al. (2023b) and MagicVideo Zhou et al.
(2022). However, both of them have pros and cons. Pixel-based VDMs can generate motion accurately
aligned with the textual prompt but typically demand expensive computational costs in terms of time and
GPU memory, especially when generating high-resolution videos. Latent-based VDMs are more resource-
efficient because they work in a reduced-dimension latent space. However, it is challenging for such a small
latent space (e.g., 8 × 5 for 64 × 40 videos) to cover rich yet necessary visual semantic details as described
by the textual prompt. Thus, if the generated videos are often not well-aligned with the textual prompts
or the generated videos are of relatively high resolution (e.g., 256 × 160 videos), the latent model will focus
more on spatial appearance but may also ignore the text-video alignment. Recently, Show-1(Zhang et al.,
2023a) integrates the strengths of both pixel and latent VDMs, resulting in a novel video generation model
that can produce high-resolution videos of precise text-video alignment at low computational cost (15G GPU
memory during inference). LATTE Ma et al. (2024c) further proposes a novel latent diffusion transformer
for video generation, which extracts spatio-temporal tokens from input videos and models video distribution
in the latent space using Transformer blocks, achieving state-of-the-art performance across multiple video
generation datasets by exploring optimal design choices such as video clip patch embedding, model variants,
timestep-class information injection, and temporal positional embedding.
2.4.2
Optical-flow-based diffusion models
Another distinct approach within diffusion models leverages optical flow to maintain temporal coherence
between frames. Optical flow is a common approach to represent motion by estimating the displacement
field between consecutive frames. Early approaches treated it as an optimization problem using handcrafted
features to maximize visual similarity(Horn & Schunck, 1981; Black & Anandan, 1993; Bruhn et al., 2005;
Sun et al., 2014). Incorporating deep learning-based methods revolutionized this field. FlowNet(Dosovitskiy
et al., 2015) pioneered end-to-end optical flow estimation, demonstrating the potential of deep learning in
this domain. Subsequent advancements on improved architectures and synthetic datasets further promoted
the progress of optical flow estimation(Ilg et al., 2017; Ranjan & Black, 2017; Sun et al., 2018; 2021; Hui
et al., 2018; 2020; Yang & Ramanan, 2019). RAFT(Teed & Deng, 2020) included iterative refinement with
correlation volumes, significantly boosting performance, while FlowFormer(Huang et al., 2022b; Shi et al.,
2023c) applied attention mechanisms within optical flow usage. VideoFlow(Shi et al., 2023b) extended optical
flow to video generation scenarios and utilized temporal information across multiple frames, achieving high
accuracy. (Hu et al., 2023)propose a Dynamic Multi-scale Voxel Flow Network (DMVFN) to explicitly model
the complex motion cues of diverse scales between adjacent video frames by dynamic optical flow estimation
to generate videos at lower computational costs. Also, Motion-I2V(Shi et al., 2024) further refined the motion
modelling by a diffusion-based motion field predictor and motion-augmented temporal attention to generate
more consistent videos even in the presence of large motion and viewpoint variation. By conditioning the
optical flow between a previous and a subsequent frame, this framework achieves realistic motion continuity,
which is critical for applications in video generation and motion synthesis. Additionally, recent advancements
explore upgrading pretrained image diffusion models to video generation by temporally warping input noise.
How I Warped Your Noise (Chang et al., 2024) proposed a novel noise representation called integral noise
(
R
-noise), preserving temporal correlations through an optical-flow-based noise warping algorithm. This
foundational approach mitigated issues like high-frequency flickering and texture-sticking artifacts, laying the
groundwork for temporally coherent video diffusion. Building upon this, Infinite-Resolution Integral Noise
Warping (Deng et al., 2025) method introduces a training-free approach where temporal correlations in noise
are preserved by warping input noise using integral representations, enabling consistent frame generation
while reducing computational overhead. These diffusion model frameworks demonstrate the versatility and
14
Figure 3: An example for optical flow usage in video scenario, Go-with-the-Flow (Burgert et al., 2025), a novel
framework that predicts 3D scene dynamics across sequential frames, outperforming conventional single-
frame approaches. By integrating multi-frame spatial-temporal relationships, it improves depth accuracy
and visual fidelity in scene reconstruction.
adaptability of diffusion processes across different domains, from high-resolution static images to complex,
temporally coherent videos. As shown in Figure 3, Go-with-the-Flow (Burgert et al., 2025) combined optical
flow extraction from training videos with real-time noise warping and fine-tuning video diffusion models on
paired warped noise and video data to enable robust and motion-controllable generation. Together, these
methods demonstrate the progression from temporal correlation preservation to advanced motion control,
showcasing the versatility and adaptability of diffusion models in video generation.
2.4.3
Noise scheduling
Diffusion models (Song et al., 2020b; Ho et al., 2020) perturb data with Gaussian noise through a diffusion
process for training, and the reverse process is learned to transform the Gaussian distribution back to the data
distribution. Perturbing data points with noise populates low data density regions to improve the accuracy
of estimated scores, resulting in stable training and image sampling. The forward process is controlled by
the handcrafted noise schedule. However, current noise scheduling strategies remain handcrafted for each
dataset, such as VP (Ho et al., 2020), VE (Song et al., 2020b), Cosine (Nichol & Dhariwal, 2021) and EDM
(Karras et al., 2022). These noise schedules perform well in low-resolution RGB spaces but yield poorer
results in higher resolutions (Dhariwal & Nichol, 2021; Hoogeboom et al., 2023). Recent studies (Chen,
2023; Hoogeboom et al., 2023) propose carefully designed noise schedules that outperform VP, VE, and
Cosine schedules in RGB space.
Besides, (Guo et al., 2023a) analyzed the power spectrum, introduced
a numerical quantification of noise levels, and proposed the weighted signal-to-noise ratio (WSNR) as a
unified metric for noise levels in both RGB and latent spaces. WSNR-equivalent training noise schedules
significantly enhance high-resolution model performance across both latent and RGB spaces.
2.4.4
Agent-based diffusion models
Agent-based approaches have revolutionized diffusion models for multimedia generation, where agents are im-
plemented as specialized neural modules with distinct functionality and decision-making capabilities. Drive-
GAN (Kim et al., 2021) pioneered this direction by utilizing multi-agent frameworks for video generation
through differentiable simulation. UniSim (Yang et al., 2024g) contributed by developing a neural closed-
loop sensor simulator using coordinated agents for scene reconstruction and sensor data generation. MORA
(Yuan et al., 2024c) established a comprehensive framework with five specialized agents for different genera-
tion stages, introducing self-modulated fine-tuning for dynamic agent contribution adjustment. VideoAgent
(Soni et al., 2024b) advanced the framework by integrating multimodal LLM feedback and real-world exe-
cution for iterative refinement. AutoGen (Wu et al., 2024d) and MetaGPT (Hong et al., 2024a) extended
these principles to broader generative tasks, implementing standardized communication protocols to enable
scalable multi-agent collaboration. This progression shows how agent-based diffusion models have evolved
from simple architectures to sophisticated multi-agent systems capable of handling complex generation tasks
while maintaining consistency through structured collaboration and feedback mechanisms.
15
Models
Backbone
VAE
Text Encoder
# Params
Training dataset and size
Resolution
Duration
# Frames
GPU size
GPU time
Academia open-source models
VideoGPT(Yan et al., 2021)
-
VQVAE
-
-
UCF-101, BAIR
64 × 64
-
-
-
-
CogVideo(Hong et al., 2023a)
DiT
2D VAE
GPT-3
15.5B
WebVid-5.4M
480 × 480
-
-
8 × RTX 6000
-
CogVideoX(Yang et al., 2024j)
STDIT
3D VAE
-
2-5B
LAION-5B, COYO-700M
768 × 1360
10s
160
-
-
MagicVideo(Zhou et al., 2022)
3D UNet
VideoVAE
CLIP
-
WebVid-10M, HD-VG-130M
1024 × 1024
-
-
1 × A100
-
Make-A-Video(Singer et al., 2022)
UNet
2D VAE
CLIP
9.7B
WebVid-10M, HD-VG-100M
768 × 768
-
76
-
-
LVDM(Rakhimov et al., 2023)
3D UNet
3D VAE
CLIP
1.6B
UCF-101, TaiChi
128 × 128
-
1024
8 × V100s
4.5 days
Video-LDM(Blattmann et al., 2023b)
3D UNet
-
-
-
WebVid-10M
512 × 1024
∼4.7s
113
2 × A100s
-
Latent-shift(Sun et al., 2023c)
ViT
2D VAE
CLIP ViT-L
1.5B
WebVid-10M
256 × 256
2-8s
-
1 × A100
-
MagViT(Yu et al., 2023a)
UNet
2D VAE
-
464M
UCF-101
128 × 128
2-8s
16
1 × V100
-
MagViT-V2(Yu et al., 2024b)
-
3D VAE
MLM
307M
UCF-101
640 × 360
-
-
-
-
VideoFusion(Zhou et al., 2023)
UNet
-
CLIP
2B
UCF101, Taichi-HD, SkyTimelapse
128 × 128
-
16
-
-
VideoComposer(Jiang et al., 2023a)
3D UNet
-
CLIP
-
WebVid-10M
256 × 256
-
16
-
-
InstructVideo(Yuan et al., 2023)
UNet
-
CLIP
-
WebVid-10M
-
-
-
4 × A100s
-
ModelScope(Wang et al., 2023c)
3D UNet
VQGAN
T5
∼1.7B
WebVid
256 × 256
-
-
A100
-
HiGen(Qing et al., 2024)
3D UNet
-
CLIP
-
WebVid-10M
448 × 256
-
32
8 × A100s
-
Dysen-VDM(Zhao et al., 2023a)
3D UNet
2D VAE
CLIP
-
UCF-101, MSRVTT
256 × 256
2s
16
16 × A100s
-
VideoGen(Chen & Sun, 2023)
UNet
-
CLIP
-
WebVid-10M
256 × 256
-
16
64 × A100s
-
Animate-A-Story(He et al., 2023a)
3D UNet
-
CLIP
-
WebVid-10M
256 × 256
-
16
-
-
AnimateDiff(Guo et al., 2023c)
UNet
2D VAE
-
-
Web Vid
1024 × 576
-
-
-, -
AnimateDiff-V2
UNet
-
CLIP
-
-
1024 × 1024
-
-
-, -
SimDA(Xing et al., 2024b)
UNet
-
T5-XXL
1.1B
WebVid-10M
256 × 256
-
-
1 × A100
-
AnimateLCM(Wang et al., 2024a)
-
-
CLIP
-
UCF-101
512 × 512
2s
16
1 × A100
-
Snap Video(Menapace et al., 2024)
DiT
3D VAE
T5-XXL
3.90B
UCF-101, MSRVTT
288 × 512
-
16
A100
-
VideoDirGPT(Lin et al., 2024a)
UNet
2D VAE
CLIP
-
UCF-101, MSR-VTT
-
-
-
16 × A6000
SiT(Ma et al., 2024a)
DiT
2D VAE
CLIP ViT-L
675M
LAION-5B
512 × 512
-
-
-
-
Open-Sora(Zheng et al., 2024c)
STDiT
3D VAE
T5-XXL
1.1B
WebVid-10M, Panda-70M
HD-VG-130M, MiraData
Vript, Inter4K
1280 × 720
2-16s
16
8 × H100s
3.5k hrs
Open-Sora-Plan(Lab & etc., 2024)
DiT
WF-VAE
mT5-XXL
-
Panda-70M
256 × 256
-
25-49
8× NPUs
-
I4VGEN(Guo et al., 2024a)
UNet
2D VAE
CLIP
-
-
512 × 512
2-8s
16
1 × V100
-
Zeroscope-v2
-
-
-
∼1.7B
-
1024 × 576
3-12s
-
-
-
CausVid (Yin et al., 2024)
DiT
-
-
-
-
640 × 352
10s
128
64 × H100s
2 days
RepVideo(Si et al., 2025)
-
-
-
-
-
-
-
-
32 × H100s
-
Industry commercial models
GEN-1
-
-
-
-
-
1280 × 720
∼8s
-
-
-
GEN-2
-
-
-
-
-
2048 × 1080
∼4s
-
-
-
GEN-3 Alpha
-
-
-
-
-
4096 × 2160
Variable
-
-
-
Imagen video(Ho et al., 2022a)
UNet
-
T5-XXL
16.2B
-
Up to 1280 × 768
Up to 5.3s
-
-
-
W.A.L.T(Gupta et al., 2023)
DiT
-
T5-XXL
3.00B
UCF-101, MSRVTT
512 × 896
2-5s
-
8× A6000s
-
Phenaki(Villegas et al., 2022)
C-ViViT
2D VAE
T5-XXL
0.8B
64 × 64
-
11-15
-
-
MiracleVision
-
-
-
-
1920 × 1080
60s
1,440
-
-
Lavie(Wang et al., 2023g)
UNet
2D VAE
CLIP
∼3B
Vimeo25M
1280 × 2048
∼10-20s
16-61
-
-
Seine (Chen et al., 2024d)
UNet
VQGAN
CLIP
-
WebVid-10M
320 × 512
-
16
-
-
VLogger (Zhuang et al., 2024)
UNet
VQVAE
CLIP
1.3B
WebVid-10M
320 × 512
∼2-3s
16
-
-
Optis (Chen et al., 2024d)
-
-
CLIP
1.3B
WebVid-10M
320 × 512
5min
320
A100
-
Vchitect-2.0
-
-
-
∼2B
-
720 × 480
∼10-20s
-
8 × A100s
-
Pika
-
-
-
-
-
1920 × 1080
3-7s
-
-
-
MovieGen
DiT
TAE
MetaCLIP+UL2+ByT5
30B
-
1920 × 1080
16s
-
6,144 H100s
1000k hrs
Show-1
UNet
-
T5-XXL
-
WebVid-10M
576 × 320
-
-
48 × A100s
-
Kling
DiT
3D VAE
-
-
-
1920 × 1080
up to 3min
-
-
Wanx 2.1
DiT
-
-
-
-
1920 × 1080
-
-
-
-
Nova Real
-
-
-
-
-
1280 × 720
∼6s
-
-
-
Veo-1
-
-
-
-
-
1920 × 1080
∼1min
-
-
-
Veo-2
-
-
-
-
-
4096 × 2160
∼2min
-
-
-
LumaAI Dream Machine
-
-
-
-
-
1360 × 752
∼5s
-
-
-
LumaAI Ray 2
-
-
-
-
-
1280 × 720
∼5-9s
-
-
-
VideoPoet
-
-
T5 XL
1.1 B
270M videos
896 × 512 8 fps
-
-
-
-
Lumiere
UNet
-
T5 XL
-
30M videos
- 16 fps
5s
-
-
-
Hailuo AI
-
-
-
-
-
1280 × 720
3- 5s
-
-
-
Mira
-
-
-
-
MiraData
-
10s-2min
-
-
-
VideoCrafter1(Chen et al., 2023a)
U-ViT
Video VAE
CLIP
-
Laion-coco-600M, Webvid-10M
1024 × 576
-
-
-
-
VideoCrafter2(Chen et al., 2024a)
U-ViT
Video VAE
-
-
WebVid-10M
512 × 320
-
-
-
Vidu
-
-
-
-
-
1920 × 1080
4-8s
-
-
-
EasyAnimate (Xu et al., 2024c)
DiT
3D VAE
T5-XXL
-
-
1024 × 576
-
up to 144
-
-
Mochi 1 (Team, 2024a)
DiT
3D VAE
T5-XXL
∼10B
-
1280 × 720
∼5.4s
-
4 × H100s
-
Jimeng (Lin et al., 2024b)
-
-
-
-
-
-
5s
-
-
-
Allegro
-
-
-
-
-
1280 × 720
-
-
-
-
LTX-Video (HaCohen et al., 2024)
DiT
Video-VAE
T5-XXL
-
-
768 × 512
5s
121
1 × H100
-
STIV (Lin et al., 2024b)
DiT
-
-
8.7B
-
512 × 512
-
-
-
-
Sora
DiT
3D VAE
Multiple LLMs
∼30B
-
1920 × 1080
5-20s
-
-
-
HunyuanVideo (Kong et al., 2025)
DiT
3D VAE
MLLM
∼13B
-
1280 × 720
∼5s
-
-
-
Step-Video-T2V (Ma et al., 2025)
DiT
Video VAE
CLIP, Step-LLM
∼30B
-
768 × 768
-
204
H100
-
SkyReels-V1 (SkyReels-AI, 2025)
DiT
3D VAE
MLLM
-
-
544 × 960
-
97
-
-
Magic-1-For-1 (Yi et al., 2025)
DiT
Video VAE
CLIP, LLM
-
-
-
-
-
-
-
Table 1: Comparison of modules and parameters in different diffusion generative models and their industry
applications. New models can be found in video generation arena leaderboard and vbench leaderboard.
(TAE refers to temporal autoencoder, and MLLM refers to multimodal large language model). Papers about
related component and dataset: VideoVAE(Zhou et al., 2022), mT5-XXL(Xue, 2020), WebVid(Bain et al.,
2021), UCF-101(Soomro et al., 2012), MSR-VTT(Xu et al., 2016), MiraData(Ju et al., 2024), Laion-coco
600M(LAION, 2023) Taichi-HD(Siarohin et al., 2019), SkyTimelapse(Xiong et al., 2018), Panda-70M(Chen
et al., 2024c), FaceForensics(Rössler et al., 2018), Inter4K(Stergiou & Poppe, 2023), Vimeo25M(Huang et al.,
2023b), MiraData(Zhao et al., 2023d), Vript(Yang et al., 2024a)
.
2.5
Architecture
As shown in Figure 4, most text-conditional visual generation models comprise three key modules:
a
variational autoencoder (VAE) compresses images and videos into a latent space. Second, a neural net-
work—typically incorporating a U-Net or Transformer-based backbone—performs the denoising within this
latent space.
The denoising process can be enhanced using optical flow or human feedback to improve
generation quality. A text encoder generates embeddings to guide the image or video generation process.
16
Figure 4: A pipeline for diffusion-based visual content generation leverages a pre-trained variational au-
toencoder (VAE), such as 2D VAE, 3D VAE, VQGAN, or TAE, to encode input images or videos into a
lower-dimensional latent representation. Within this latent space, diffusion models (e.g., DDPM, DDIM,
EDM) iteratively introduce noise and employ neural architectures, such as U-Net or Transformer-based
models, to learn a denoising process that reconstructs high-fidelity outputs. User-provided textual prompts
undergo refinement through large language models (e.g., T5, CLIP, GPT) before being mapped into an
embedding space via a trained text encoder. This embedding space serves as a conditioning mechanism,
guiding the diffusion process to ensure semantic coherence with the input prompt. Furthermore, the frame-
work integrates optical flow estimation methods (e.g., FlowNet, Raft) to enhance motion consistency in
generated video sequences and incorporates human feedback mechanisms (e.g., VIDEORM) to iteratively
improve generation quality.
2.5.1
UNet
The UNet(Ronneberger et al., 2015) architecture has become foundational in designing denoising models
for visual diffusion applications, originally developed for medical image segmentation and now adapted
widely for generative tasks across images, video, and audio. In image generation tasks, a UNet typically
encodes an input into progressively lower-resolution latent representations while increasing feature channels
through a series of encoding layers. This encoded latent is then upsampled back to the original resolution
by corresponding decoding layers.
In diffusion models, UNet can operate in either the pixel space or the latent space. For example, Latent
Diffusion Models (LDMs) (Rombach et al., 2022) utilize UNet within a lower-dimensional latent space, which
allows for more efficient, high-resolution generation. In pixel space, UNet implementations are used for direct
denoising, maintaining the input’s spatial resolution throughout the process.
Modern implementations of UNet in diffusion models build upon the original by replacing ResNet blocks with
Vision Transformer (ViT) layers(Dosovitskiy et al., 2020). ResNet blocks apply 2D-Convolutions, focusing
on extracting spatial features, while the ViT blocks introduce spatial self-attention and cross-attention
mechanisms. This configuration allows the model to condition generation based on text prompts or specific
17
time steps. Matching resolution layers in the encoder and decoder are connected by residual connections,
ensuring efficient information flow.
UNet has been further adapted to handle temporal information for video-based diffusion tasks. Extensions
like AnimateDiff incorporate temporal attention layers within the UNet to capture dependencies across
frames. (An et al., 2023) keep using 2D UNet and enable motion learning by shifting the feature channels
along the temporal dimension. Other modifications include transitioning from classic 2D-UNet to spatial-
temporal factorized 3D UNet architecture. VDM (Ho et al., 2022c) propose a spatial-temporal factorized 3D
UNet by adding temporal blocks for video generation, as a natural extension of the standard image diffusion
model. Similarly, (Ho et al., 2022a) builds on the similar video U-Net architecture to the cascaded image
diffusion model Imagen Saharia et al. (2022), while (He et al., 2022), MagicVideo(Zhou et al., 2022), and
ModelScope(Wang et al., 2023c)apply it to the latent space (Rombach et al., 2022), as shown in Table 1.
MoVideo(Liang et al., 2025) further generates the depth and optical flow of the whole video by utilizing
a 3D UNet architecture by adding extra-temporal modules, including temporal convolution and temporal
attention layers after spatial convolution and spacial attention layers, improving frame consistency and visual
quality for video generation.
2.5.2
Diffusion transformers
The Diffusion Transformer (DiT) replaces UNet’s traditional convolutional design with a Vision Transformer
(ViT) framework, offering improved spatial attention capabilities suited for generative tasks across images
and videos(Peebles & Xie, 2023). DiT achieves this through a 2D patchification process, where individ-
ual image frames are divided into patches that are then treated as tokens, allowing the model to capture
detailed spatial dependencies within each frame. GenTrone(Chen et al., 2024b) adapted DiT from class
to text conditioning and scaled GenTrone from approximately 900M to over 3B parameters demonstrating
Transformer-based diffusion models can be used in the visual generative domain. From Table 1, models
including CausVid(Yin et al., 2024), SiT(Ma et al., 2024a), and Sora also use DiT in video generation
settings. However, for video generation, 2D tokenization alone is insufficient to capture the sequential de-
pendencies critical for temporal coherence across frames. Approaches such as ViViT(Arnab et al., 2021)
and STDiT(Zheng et al., 2024c) address this limitation by extending DiT’s patchification from 2D to 3D,
introducing a 3D tokenization approach that simultaneously captures both spatial and temporal relation-
ships. One representative work is CogVideoX (Hong et al., 2023a), which uses STDiT to model spatial-
temporal evolution in video generation, capturing long-range dependencies for improved motion consistency
and semantic fidelity. In this structure, consecutive frames are processed as spatiotemporal patches, en-
abling the model to understand dynamic changes across frames. Building on these approaches, frameworks
like VDT(Lu et al., 2024a) incorporate both causal attention, which restricts attention to previous frames,
and sparse causal attention, which further limits the focus to a subset of recent frames. VersVideo(Xiang
et al., 2023) incorporated multi-excitation paths for spatial-temporal convolutions with dimension pooling
across different axes and multi-expert spatial-temporal attention blocks to further boost the model’s spatial-
temporal performance instead of simply extending 2D operations with temporal operations to significantly
escalate training and inference costs.Text2Performer(Jiang et al., 2023b)proposes ae continuous VQ-diffuser
to directly output the continuous pose embeddings for better motion modeling. These mechanisms enhance
model’s effectiveness for video-based diffusion tasks.
2.5.3
VAE for latent space compression
Diffusion and denoising in RGB pixel space, as demonstrated in (Ho et al., 2020; Dhariwal & Nichol, 2021;
Saharia et al., 2022; Ramesh et al., 2022; Ho et al., 2022b), require high computational resources, significantly
increasing both training cost and inference latency. To mitigate its resource consumption, Latent Diffusion
Models (LDM)(Rombach et al., 2022) leverage variational autoencoders (VAEs) to compress images from
pixel space into a more compact latent space enabling the diffusion process to occur in this optimized
space. This approach enhances both training and inference efficiency by reducing the complexity of the data
representation.
Classical VAEs for image and video compression include standard VAEs(Kingma, 2013), quantized versions
like VQVAE(Esser et al., 2021), VQGAN(Van Den Oord et al., 2017), and their GAN-enhanced variants,
18
which improve reconstruction quality for higher compression. For example, VideoGPT(Yan et al., 2021) use
a 3D-VQVAE in the latent space for video generation. Seine(Chen et al., 2024d) and MAGVIT(Yu et al.,
2023a) integrate 3D-VQGAN with discriminators for latent space encoding to achieve better visual quality.
The VAE framework not only compresses effectively but also enable the training of multiple generative tasks
across downstream tasks.
By integrating VAE in model building, some image generation models(Ho et al., 2022a; Li et al., 2024b; Podell
et al., 2023; Bao et al., 2023b; Peebles & Xie, 2023; Lu et al., 2024c; Ma et al., 2024a; Chen et al., 2023c;
Li et al., 2024j; Team, 2024b; Esser et al., 2024) also leverage latent space encoding/decoding, freezing VAE
parameters during diffusion training and inference. Some diffusion models, including Make-A-Video(Singer
et al., 2022), Imagen(Ho et al., 2022a), Show-1(Zhang et al., 2023a), directly learn pixel distributions gener-
ating videos, as shown in Table 1. However, video generation involves both spatial and temporal complexity,
leading to higher computational costs.
Besides, some representative diffusion video generation models,
including Sora(Liu et al., 2024e), Phenaki(Villegas et al., 2022), VideoCrafter(Chen et al., 2023a), Animate-
Diff(Guo et al., 2023c), and VideoPoet(Kondratyuk et al., 2023), start using VAE compression in video
settings before training and inference in latent space. Many video generation models, including Latte(Ma
et al., 2024c), Lavie(Huang et al., 2023b), MagicVideo(Zhou et al., 2022), Align-your-latent(Blattmann et al.,
2023b), and AnimateDiff(Guo et al., 2023c), are devrived from Stable Diffusion’s image 2D VAEs, as training
a full 3D latent space from scratch is challenging. However, temporal compression in 2D VAE-based video
models relies on uniform frame sampling, which ignores motion information, leading to low FPS and less
smooth video generation.
Some approaches also start to utilize hybrid 2D-3D or fully 3D VAEs. For example, MagViT(Yu et al., 2023a),
EasyAnimate(Xu et al., 2024c), Open-Sora(Zheng et al., 2024c; Lab & etc., 2024), and CogVideo-X(Yang
et al., 2024j) employ hybrid 2D-3D VAEs, leveraging volumetric encoding for video generation. MAGViT(Yu
et al., 2023a) adopting 3D VQGAN structures integrating both 3D and 2D downsampling, and MAGViT-
V2(Yu et al., 2024b) utilizing a fully 3D VAE with 3D convolutional encoder and overlapping downsampling
for enhanced spatial-temporal fidelity. HunyuanVideo(Kong et al., 2025), Kling, Mochi 1 (Team, 2024a),
and SkyReels-V1(SkyReels-AI, 2025) continue leveraging 3D VAEs for compressed training in latent space
in industry scenarios. Step-Video-T2V(Ma et al., 2025) and Magic-1-For-1(Yi et al., 2025) introduce novel
Video VAE structures for efficient generation. However, training a video VAE independently without ensuring
compatibility often leads to a latent space gap, preventing accurate projection into pixel space (Zhao et al.,
2024b). CV-VAE (Zhao et al., 2024b) introduces a novel approach using latent space regularization to train
a video VAE that extracts continuous latent for generative video models while maintaining compatibility
with existing pretrained image and video models. Also, to trade off lower memory and computational cost
with slightly lower reconstruction quality, the latest video generation model, Moive Gen(teamMeta, 2024)
further uses interleaved 2D-1D convolutional encoders in its VAE. This approach balances efficiency and
quality by reducing the redundancy of temporal information while maintaining spatial coherence.
2.5.4
Text encoder
In text-conditional visual generation models, the text encoder plays a crucial role in capturing semantic
information from input text prompts, directly affecting the generated content. Early text-to-image models
used text encoders trained on paired text-image datasets, either trained from scratch(Nichol et al., 2021;
Ramesh et al., 2021) or fine-tuned from pre-trained models such as CLIP(Radford et al., 2021).
CLIP,
which employs contrastive learning to align text and image embedding spaces, enables the text encoder
to effectively represent both visual and linguistic semantics after being trained on large-scale multimodal
datasets. Once an input text prompt is tokenized and embedded, it serves as a conditioning mechanism
for the diffusion model’s generative backbone. CLIP-based text encoders are widely used in text-to-image
diffusion models, as shown in Table 1, including DALLE-2(Ramesh et al., 2022), Stable Diffusion(Rombach
et al., 2022), DiT(Bao et al., 2023a; Peebles & Xie, 2023), Fit(Lu et al., 2024c), SiT(Ma et al., 2024a),
HunyuanDiT(Li et al., 2024j), and Scaling Diffusion (Esser et al., 2024; Labs, 2024). In these models, the
text encoder’s parameters are often frozen to reduce computational and memory overhead during training,
ensuring that most training resources are allocated to the diffusion process itself.
19
However, CLIP often struggle with understanding detailed text descriptions. To address this, large language
models (LLMs) trained on extensive text corpora provide stronger text comprehension and generation capa-
bilities. Imagen(Ho et al., 2022a) compared CLIP with pre-trained LLMs such as BERT(Devlin et al., 2019)
and T5(Raffel et al., 2020) as text encoders. Their findings showed that scaling the size of the text encoder
improves the quality of text-to-image generation. Notably, (Ho et al., 2022a) found that the T5-XXL encoder
significantly enhances image-text fidelity, leading to its adoption in several models such as Latte(Ma et al.,
2024c), Open-Sora(Zheng et al., 2024c), and SimDA(Xing et al., 2024b). Some approaches further combine
CLIP and T5 to enhance text comprehension. Step-Video-T2V(Ma et al., 2025) integrates CLIP, Step-LLM,
and Video-VAE to refine alignment between textual descriptions and generated video content. To improve
text encoding, ByT5(Xue et al., 2022b) introduced a byte-level tokenization-free approach, effectively han-
dling diverse text inputs. This has been leveraged in Movie-Gen, where ByT5 combined with MetaCLIP and
UL2 jointly enhances text understanding for improved video generation. Multilingual contexts also benefit
from LLM-based encoders. Open-Sora-Plan (Lab & etc., 2024) incorporates multilingual T5 (mT5-XXL)
alongside WF-VAE, optimizing text representations for video synthesis across diverse linguistic structures.
Recent image(Tan et al., 2024c; Team, 2024b) and video(Yang et al., 2024j; Yi et al., 2025), generation mod-
els employ large language models such as Baichuan(Yang et al., 2023a), Llama(Touvron et al., 2023a;b), and
ChatGLM(Du et al., 2021) to enhance the semantic understanding of complex text. SkyReels-V1(SkyReels-
AI, 2025) and HUnyuanVideo(Kong et al., 2025) further integrate pre-trained Multimodal Large Language
Model (MLLM) as their text encoders to enhance visual-text alignment and improve instruction adherence
in diffusion models.
3
Implementation
In this part, we present a detailed analysis of the datasets, training engineering techniques, and evaluations
of video generation.
3.1
Datasets
In this part, we review the most popular datasets used for video generation, as summarized in table 2.
Specifically, we split the datasets into academic and commercial datasets.
Academic benchmark datasets.
As a long-standing problem, there exist numerous traditional text-
to-video benchmark datasets, such as MSR-VTT (Xu et al., 2016), DiDeMo (Hendricks et al., 2017),
LSMDC (Rohrbach et al., 2015), and VATEX (Wang et al., 2019), which consist of videos and corresponding
captions collected from the real world or annotators. However, those datasets are always small, low quality,
and cover only a small number of different areas, such as sports, cooking, and movies.
Building upon those traditional benchmark datasets, researchers have recently introduced datasets (Sanabria
et al., 2018; Zellers et al., 2021; Lee et al., 2021; Chen et al., 2024c; Ju et al., 2024) with a large number of
videos. For example, YT-Temporal-180M (Zellers et al., 2021) contains 180M videos collected from YouTube.
InternVid (Wang et al., 2023h) takes over 7 million videos lasting nearly 760K hours, yielding 234M video
clips accompanied by detailed descriptions of a total of 4.1B words. Also, researchers noticed that video
quality, such as resolution, is also important. Inter4k-1k (Stergiou & Poppe, 2023), as a pioneer, introduces
1k videos with a resolution of 4k, enabling the understanding and generation of high-resolution videos.
On the other side, as manually collecting datasets is time-consuming and expensive, researchers started to
generate datasets using off-the-shelf models. VidProM (Wang & Yang, 2024) contains 1.67 million unique
text-to-video prompts generated by Pika, VideoCraft2 (Chen et al., 2024a), and Text2Video-Zero (Khacha-
tryan et al., 2023).
On the other side, other researchers focus on providing comprehensive video generation benchmark
datasets (Huang et al., 2024a; Wang & Yang, 2024) from different perspectives, e.g., temporal consistency,
style consistency, semantic consistency, and video quality. VBench takes 100 prompts to evaluate video
generation models from different perspectives. FETV (Liu et al., 2023e) reuses the texts from the MSR-
VTT test set and WebVid for open-domain text-video pairs, guaranteeing the diversity of prompts. It also
20
Datasets
Modalities
Vision
Text
Duration
Resolution
Domains
# of samples
Sources
License
Academia datasets
UCF-101 (Soomro et al., 2012)
V, A
Real
-
AVG. 7 s
320x240
-
V (13,320)
YouTube
CC-BY
Kinetics-400 (Carreira & Zisserman, 2017)
V, A
Real
-
AVG. 10 s
-
-
V (306,245)
YouTube
-
BSCV (Liu et al., 2023c)
V
Real
-
-
480P
-
V (28 k)
-
-
VFHQ (Xie et al., 2022)
V
Real
-
-
700x700
-
V (16,827)
FFHQ, VoxCeleb1
-
DMLab-40k (Yan et al., 2023b)
V
Synthetic
-
-
-
3D Rendered
V (40 k)
-
-
Habitat (Yan et al., 2023b)
V
Synthetic
-
-
-
3D Rendered
V (200 k)
-
-
Minecraft (Yan et al., 2023b)
V
Synthetic
-
-
-
3D Rendered
V (200 k)
-
-
DEVIL (Szeto & Corso, 2022)
V
Real
-
-
-
Camera Infos
V (1,250)
Flickr
MIT
Inter4K-1k (Stergiou & Poppe, 2023)
V
Real
-
AVG. 5 s
4K
-
V (1 k)
-
-
MSR-VTT (Xu et al., 2016)
V, T, A
Real
Real
AVG. 15 s
320x240
-
T/V (10 k)
Youtube
-
DiDeMo (Hendricks et al., 2017)
V, T, A
Real
Real
AVG. 7 s
-
-
T/V (27 k)
Flickr
BSD 2-Clause
LSMDC (Rohrbach et al., 2015)
V, T, A
Real
Real
AVG. 5 s
1080p
-
T/V (118 k)
-
-
VATEX (Wang et al., 2019)
V, T, A
Real
Real
AVG. 10 s
-
-
T/V (41 k)
Youtube
CC-BY-4.0
YouCook2 (Zhou et al., 2018a)
V, T, A
Real
Real
AVG. 5 mins
-
-
T/V (14 k)
Youtube
-
How2 (Sanabria et al., 2018)
V, T, A
Real
Real
AVG. 90 s
-
-
T/V (79 k)
Youtube
Creative Commons BY-SA 4.0
ActivityNet Caption (Heilbron et al., 2015)
V, T, A
Real
Real
AVG. 120 s
-
-
T/V (100 k)
Youtube
-
VideoCC3M (Nagrani et al., 2022)
V, T, A
Real
Real
AVG. 10 s
-
-
T/V (10.3 M)
-
-
WebVid10M (Bain et al., 2021)
V, T
Real
Real
AVG. 18 s
360p
-
T/V (10.7 M)
-
AGPL-3.0
WTS70M (Stroud et al., 2021)
V, T
Real
Real
AVG. 10 s
-
-
T/V (70 M)
-
-
HowTo100M (Miech et al., 2019)
V, T, A
Real
Auto Captioning
AVG. 4 s
240p
-
T/V (136 M)
Youtube
Apache License 2.0
HD-VILA-100M (Xue et al., 2022a)
V, T
Real
Auto Captioning
AVG. 13 s
720p
-
T/V (100 M)
-
See license
YT-Temporal-180M (Zellers et al., 2021)
V, T, A
Real
Real
-
-
-
T/V (180 M)
Youtube
-
ACAV100M (Lee et al., 2021)
V, T, A
Real
Real
AVG. 10 s
-
-
T/V (100 M)
-
MIT License
Vript-400k (Yang et al., 2024a)
V, T, A
Real
Real
AVG. 11 s
720p
-
T/V (420 k)
-
-
VidProdM (Wang & Yang, 2024)
V, T
Synthetic
Real
AVG. 2 s
-
-
T (1.67 M), V (6.69 M)
-
CC-BY-NC 4.0
FETV (Liu et al., 2023e)
V, T
Real
Real
-
-
-
T (619), V (541)
MSR-VTT and WebVid
CC-BY-NC 4.0
InternVid (Wang et al., 2023h)
V, T
Real + Synthetic
Auto Captioning
AVG. 12 s
720p
-
T/V (234 M)
-
CC BY-NC-SA 4.0
AIGCBench (Fan et al., 2024)
I, V, T
Real
Real
-
-
-
T/I (2,928), T/V (1,000)
-
Apache License 2.0
AVE (Argaw et al., 2022)
V, T
Real
Real
AVG. 4 s
-
-
V (196k )
-
-
Panda-70M (Chen et al., 2024c)
V, T
Real
Auto Captioning
AVG. 9 s
720p
-
T/V (70 M)
-
See license
HD-VG-130M (Wang et al., 2024f)
V, T
Real
Auto Captioning
-
720p
-
T/V (130 M)
-
See license
MiraData-77k (Ju et al., 2024)
V, T
Real
Auto Captioning
AVG. 72 s
720p
Camera Infos
V (330 k)
-
GPL-v3
LAION-AESTHETICS 6.5+
I, T
Synthetic
Real
-
-
-
T/I (625 k)
-
-
Animate bench
I, T
Synthetic
Real
-
-
-
T/I (105)
-
Apache License 2.0
DrawBench (Saharia et al., 2022)
I, T
Real
Real
-
-
-
I/T (200)
-
-
PartiPrompts (Yu et al., 2022b)
I, T
Real
Real
-
-
-
T (1.6 k)
-
Apache License 2.0
Commercial datasets
Midjourney-v5-1.7M
I, T
Synthetic
Real
-
-
-
T/I (1.7 M)
-
Apache License 2.0
Midjourney-Kaggle-Clean
I, T
Synthetic
Real
-
-
-
T/I (250 k)
-
cc0-1.0
Unsplash-lite
I, T
Synthetic
Real
-
-
-
T/I (250 k)
-
See license
Mixkit
V, T
Real
Real
AVG. 18 s
720p
-
T/V (1,234)
-
Commercial and non-commercial
Pixabay
I, V, T
Real
Real
AVG. 25 s
720p
-
T/V (31,616)
-
Commercial and non-commercial
Pexels-400k
V, T
Real
Real
-
720p
-
T/V (400,476)
-
MIT
Table 2: The overview of most popular datasets used in training video generation models. We also include
image datasets as they are usually used in training. “I”, “V”, “T”, and “A” represent image, video, text,
and audio. Other commercial datasets include those released by Pond5, Adobe Stock, Shutterstock, Getty,
Coverr, Videvo, Depositphotos, Storyblocks, Dissolve, Freepik, Vimeo, and Envato.
contains manually written prompts describing scenarios that are unusual in the real world, and cannot be
found in existing text-video datasets.
Except for the benchmark dataset for video generation, there are also some datasets targeting face video
generation, video inpainting, and video restoration. For example, VFHQ (Xie et al., 2022) is proposed for
the face super-resolution, which contains over 16, 000 high-fidelity clips of diverse interview scenarios.
Commercial benchmark datasets.
There are also some companies providing high-quality image/video-
text paired data such as Pond5, Adobe Stock, Shutterstock, Getty, Coverr, Videvo, Depositphotos, Sto-
ryblocks, Dissolve, Freepik, Vimeo, and Envato, which are widely used in training video generation meth-
ods (Zheng et al., 2024c).
3.2
Training engineering
A comprehensive pipeline for data curation, preprocessing, and training is needed to train a large foundation
model for video generation. In this part, we present an overview of data preprocessing, training techniques,
and acceleration methods.
Data preprocessing. Instead of directly using the original data from the dataset, more and more meth-
ods (Zheng et al., 2024c; Li et al., 2024j; Yang et al., 2024j; Blattmann et al., 2023a) have started to select,
filter, and enhance video-text paired data before training the model as shown in fig. 5. OpenSora (Zheng
et al., 2024c), as a representative open-sourced method, first splits videos into shorter clips using scene
detection (Ravi et al., 2025). Later, clips are evaluated using the aesthetic and optical flow scores, and the
clips with low scores are removed from training. After that, the qualified clips are captioned using machine
captioners (Hong et al., 2024b; Li et al., 2025; Xu et al., 2024d) and a matching score will be employed
to evaluate the alignment between clips and their captions. Finally, samples with adequate scores (high
aesthetic quality, large video motion, and strong semantic consistency) are used for training the model. The
21
Figure 5: The data preprocessing pipeline of Open-Sora Zheng et al. (2024c).
overall pipeline is presented in fig. 5. Similarly, SVD (Blattmann et al., 2023a) filters out samples with low
text-video alignment evaluated by CoCa (Yu et al., 2022a), static videos (evaluated by optical flow), and
high text overlays.
Training techniques. Video generation models (Blattmann et al., 2023a; Zheng et al., 2024c) are usually
initialized with image models and pretrained with image data. Then, the models are pretrained with video
data for learning efficient video representations. After that, models are trained using high-quality data to
form the final video generation models. However, due to the difficulty of the video generation problem,
different training techniques (Blattmann et al., 2023a; Xing et al., 2023; Hong et al., 2023a; Yang et al.,
2024j) are proposed. For example, CogVideoX (Yang et al., 2024j) employs a multi-resolution frame pack
strategy to enable the generation of videos with varied lengths.
And a progressive training strategy is
employed to generate videos of different resolutions. On the other side, to scale up the training of video
generation models, multi-node parallelization and VRAM-efficient scheduling are utilized Yang et al. (2024j);
Zheng et al. (2024c).
Video generation acceleration techniques. Due to the high computational requirements of training
and inferencing video generation methods, researchers have developed several acceleration techniques. For
example, Open-Sora (Zheng et al., 2024c) employs flash attention (Dao et al., 2022), ZeRO (Rajbhandari
et al., 2020), and gradient checkpoint for kernel optimization, hybrid parallelism, and larger batch size
training. Moreover, Open-Sora employs efficient STDiT as the vision encoder while preprocessing text and
video data for acceleration. On the other side, parameter-efficient fine-tuning (PEFT) (Geng et al., 2024a; Jia
et al., 2022), which is widely applied in training large foundation (language and vision-language) models (Li
et al., 2025; Touvron et al., 2023a;b), could be used in video generation for training acceleration (Xing
et al., 2024b; Pan et al., 2022). We also include a detailed discussion on long video generation acceleration
techniques in section 4.5.
22
3.3
Evaluation metrics and benchmarking findings
Metrics
Video-Condition Consistency
Style Consistency
Temporal Consistency
Semantic Consistency
Video Quality
Temporal Quality
Static Quality
Human Evaluation
Fine-grained Alignment
(Semantic Consistency)
Overall Alignment
Temporal Quality / Consistency
Static Quality
Existing evaluation metrics include human evaluation and auto evaluation. Human evaluation (Liu et al.,
2023e; Huang et al., 2024a) always includes several aspects such as static quality, temporal quality (consis-
tency), overall alignment, and fine-grained alignment (semantic consistency). And human evaluation is al-
ways employed to test the alignment between auto-evaluation and human evaluators, such as VBench (Huang
et al., 2024a), which uses 16 human evaluations on 16 different dimensions mainly focusing on temporal and
semantic consistency. A human-verified metric is STREAM (Kim et al., 2023b), testified under the same
dimensions using auto-evaluation and human evaluation.
On the other side, as auto-evaluation (Huang et al., 2024a;b; Fan et al., 2024) is cheaper and faster, it is widely
applied to evaluate video quality and conditioned quality (semantic consistency and temporal consistency).
Video quality usually only contains video as the input of metrics while conditioned quality also uses other
modal inputs such as image, text, or audio to evaluate the generated video.
Video quality consists of static quality and temporal quality. 1) For static quality, AIGCBench (Fan et al.,
2024) employs a pre-trained optical flow estimation model, i.e., RAFT (Teed & Deng, 2020), and calculates
the mean square of the average flow score between adjacent frames from the generated video for estimating
the motion effects as Flow-Square-Mean. Dover (Wu et al., 2023a) is also employed for evaluating the static
quality from aesthetic and technical perspectives. Moreover, the number of video frames is used for
accessing the quality. On the other side, FETV (Liu et al., 2023e) employs FID (Heusel et al., 2017) and
FVD (Unterthiner et al., 2019) metrics. 2) For temporal quality, AIGCBench assesses it by calculating
the average similarity between adjacent frames using CLIP (Radford et al., 2021) noted as GenVideo Clip
(Adjacent frames). VBench (Huang et al., 2024a) evaluates it by subject, background, temporal flickering,
motion, dynamic degree, aesthetic quality, and imaging quality using multiple pre-trained models including
DINO, CLIP, Amt, RAFT, and LAION. Bonneel et al. (2015) proposes a warping error for improving the
23
Figure 6: The VBench leader board at Feb. 3rd, 2025. SORA still scores the highest on some of the metrics,
e.g., quality score and selected score.
consistency between frames. Except for those methods, STREAM (Kim et al., 2023b) is proposed to avoid
the problem of FVD, which has a stronger emphasis on the spatial aspect than the temporal naturalness of
video and demonstrates considerable instability and diverges from human evaluations. It evaluates temporal
and semantic quality at the same time.
The metrics for evaluating conditioned quality can be categorized into semantic consistency, temporal con-
sistency, and style consistency. 1) In semantic consistency, for image/video-video metrics, AIGCBench (Fan
et al., 2024) employs Mean Squared Error (MSE First) and Structural Similarity Index Measure (SSIM
First) for evaluating the first generated frames. To further understand how the semantics are preserved
across frames, AIGCBench calculates the cosine similarity between the input image and each frame of gen-
erated video using CLIP (Image-GenVideo CLIP). Moreover, GenVideo-RefVideo SSIM is also employed to
measure the spatial structural similarity of the generated videos to the reference videos by calculating the
SSIM (Structural Similarity Index Measure) between the corresponding frames of the generated and reference
videos as the referenced video is at hand. For Text-to-Video metrics, AIGCBench employs GenVideo-Text
Clip and GenVideo-RefVideo CLIP (Keyframes), calculating the similarity between the input text / refer-
enced video and keyframes from generated videos using CLIP. Similar to AIGCBench, FETV (Liu et al.,
2023e) propose to use stronger models, such as CLIP finetuned on MSR-VTT, BLIP, and UMP, and VQA
models for evaluating the semantic consistency. Moreover, FETV also introduces a novel QA-based metric
(Otter-VQA). This metric generates yes-no questions on key elements from the text prompt via the Vicuna
model and calculates the average number of questions that receive a positive answer by feeding the gen-
erated (or ground-truth) videos and questions into the Video LLM as the score. VBench (Huang et al.,
2024a) evaluates the semantic consistency by objects, humans, color, relationships, scene, appearance, and
temporal style using ViCLIP, CLIP, Tag2Text, UMT, and GRiT. 2) For temporal consistency, researchers
usually directly employ the temporal quality metrics. Recently, TC-bench Feng et al. (2024c) proposed to
focus on the transition of objects, relations, and background for better temporal consistency. 3) For style
consistency, VBench employs CLIP and ViCLIP to assess the similarity between generated videos and the
style description.
24
Except for those metrics, VBench (fig. 6) and Video Generation Arena also set up online evaluation platform,
providing an accessible leaderboard for researchers.
3.4
Industry solutions
As shown Backbone column in Table 1, video generation industry has evolved significantly with the adoption
of diffusion models that leverage advanced architectures like DiT, UNet, and U-ViT. DiT (Diffusion Trans-
former) is widely used in models like W.A.L.T(Gupta et al., 2023), MovieGen, Kling, and Mochi 1(Team,
2024a), excelling in capturing long-range dependencies for sharp, coherent visuals in video generation tasks.
UNet, utilized in models like Imagen video, Lavie(Wang et al., 2023g), Seine(Chen et al., 2024d), and VLog-
ger(Zhuang et al., 2024), is known for its ability to handle both local and global features, making it ideal for
high-resolution video generation. U-ViT (U-Transformer for video) is another backbone seen in models like
VideoCrafter1(Chen et al., 2023a) and VideoCrafter2(Chen et al., 2024a), effective at processing both spatial
and temporal dependencies for more accurate video generation. These architectures continue to shape the
industry’s approach to advanced video generation.
Variational Autoencoders (VAE) are widely used in industry to learn latent video representations. As shown
VAE column in Table 1, models such as HunyuanVideo(Kong et al., 2025), Mochi 1(Team, 2024a), and
Kling utilize 3D VAE for temporal consistency. On the other hand, models like Lavie(Wang et al., 2023g)
and Phenaki(Villegas et al., 2022) utilize 2D VAEs, which offer greater computational efficiency but are
less effective at capturing the temporal dynamics of video content. Meanwhile, VQ-VAE, used in models
like VLogger(Zhuang et al., 2024) and Vchitect-2.0, introduces vector quantization to improve efficiency by
discretizing the latent space. Additionally, Video VAE architectures, such as in VideoCrafter(Chen et al.,
2023a) and LTX-Video(HaCohen et al., 2024), are specifically designed for video generation, optimizing the
ability to handle both spatial and temporal complexities.
The choice of VAE architecture thus directly
influences the quality, efficiency, and temporal coherence of video generation.
Text encoders, such as T5-XXL, are essential for aligning video with textual descriptions. Based on the
column text encoder in the Table 1, models like Imagen video, W.A.L.T.(Gupta et al., 2023), and MovieGen
use T5-XXL for precise video-text alignment, enhancing generative capabilities. Meanwhile, models like
Lavie(Wang et al., 2023g) and Seine(Chen et al., 2024d) leverage CLIP-based encoders, which offer flexibility
for generating both short clips and full-length videos. T5-XXL excels in large-scale generation, while CLIP
provides versatility for diverse visual styles and content.
The scale of models varies, as shown in the parameter column of Table 1, with larger models like Sora
(with ∼30B parameters) and MovieGen (with ∼30B parameters) offering scalability for large-scale produc-
tion. Smaller models including Vchitect-2.0 and VideoPoet maintain strong performance while using fewer
parameters (1B–2B) to balance performance and computational cost.
Training datasets such as WebVid-10M(Bain et al., 2021) and Vimeo25M(Huang et al., 2023b) are commonly
used for large-scale models like Seine(Chen et al., 2024d), Lavie(Wang et al., 2023g) and Show-1 from Table 1,
ensuring diverse outputs. Models including Lumiereand VideoPoet choose to gather video and image data
directly from public websites, rather than relying on pre-existing datasets.
Resolution plays a key role in video output quality. As shown in the resolution coulmn in the Table 1, GEN-3
Alpha and Sora support high resolutions like 4090 × 2160 and 1920 × 1080, ideal for cinematic production.
In contrast, models like Seine(Chen et al., 2024d) and Phenaki(Villegas et al., 2022) offer lower resolutions
like 320 × 512 or 64 × 64, balancing quality and computational efficiency.
Video duration is also a key consideration. Based on the duration column in the Table 1, W.A.L.T(Gupta
et al., 2023) and VLogger(Zhuang et al., 2024) excel in generating short-duration videos for platforms such
as social media, while models such as Kling and Mira are capable of creating longer videos, suited for film
production or extended content.
GPU requirements for training and inference vary. As shown in the GPU size and GPU hours column in the
Table 1, Large models such as Show-1 and MovieGen need substantial GPU resources, with Show-1 requiring
around 48 A100 GPUs. Smaller models like LTX-Video(HaCohen et al., 2024) can achieve faster processing,
offering practical real-time video generation for commercial use.
25
High-level (semantic) condition
Low-level (fine grained) condition
Control-Net condition
ID-Adapter
Cross-frame attention
Input addition
CLIP 
encoder
Cross 
attentions
CLIP feature
Cross 
attentions
CLIP feature conditioning
Diffusion U-Net
Image 
encoder
Diffusion 
U-Net
Input concatenation
Image 
tokenizer
Diffusion U-Net
or DiT
U-Net
Figure 7: Methods for controlling video generation using image-based conditions.
We categorize these
methods along a spectrum ranging from high-level semantic conditions to low-level conditions. Additionally,
we illustrate four classic approaches commonly used to condition input images.
The video generation industry continues to grow with models such as MovieGen and Sora catering to high-
end production, while consumer-focused models like VLogger(Zhuang et al., 2024) and EasyAnimate (Xu
et al., 2024c) offer fast solutions for short-form video content. The challenge remains to optimize video
quality while managing computational resources for broader accessibility.
4
Applications
4.1
Conditions
In this section, we examine a variety of “conditions” that researchers use to guide the video generation
process. Unlike purely unconditional or text-based generation, which can lack specific control or produce
unsatisfactory temporal consistency, leveraging additional forms of conditioning provides more precise and
robust ways to steer the synthesis. These conditions can stem from many sources—images, spatial con-
straints, camera parameters, audio signals, high-level editing instructions, or even physical simulations—and
can serve different roles, from specifying global traits like style and identity to prescribing detailed frame-
by-frame motion.
We organize these conditions into subtopics based on the type of guidance they offer. First, we discuss
image-based methods (Image condition 4.1.1), where a single reference image or a sequence of images conveys
both coarse semantic cues and fine-grained details for video generation. We then explore spatial conditions
(Spatial condition 4.1.2), such as trajectories, bounding boxes, or scene layouts, which grant users exact
control over object movement and scene composition. Next, we address camera parameter inputs (Camera
parameter condition 4.1.3) for 3D-consistent view synthesis and cinematic manipulation. We also cover audio-
based approaches (Audio condition 4.1.4) for synchronizing speech, music, or other sound cues with visual
content. Finally, we examine methods that focus on high-level editing (High-level video condition 4.1.5) and
specialized application constraints (Other conditions 4.1.6), including incorporating physics-based realism,
working with physiological signals, and utilizing timestamp information as conditions. Through these diverse
forms of conditioning, researchers aim to create video generation systems that balance creative freedom with
strong user control, opening the door to a wide range of potential applications.
4.1.1
Image condition
Controlling video generation with images is a crucial capability, as images offer visually intuitive and fine-
grained control over content that text alone cannot achieve. This functionality serves as a foundational
building block for various video applications, such as identity-preserving video generation and image anima-
tion.
26
Figure 8: Make Pixel Dance Zeng et al. (2024a) shows that by conditioning both on the first and last frame,
the model implicitly learns to generate large-scale motion.
Depending on how the conditioning image is integrated into the diffusion model, the methods can be broadly
categorized into two types: semantic-level image conditioning and fine-grained image conditioning.
Semantic-level conditioning. Similar to text-conditioned video diffusion, semantic-level image condition-
ing provides high-level coarse guidance for video generation. This is typically achieved by encoding the input
image into a low-dimensional feature that retains rich semantic information, such as identity. These features
are then fed into the video model through cross-attention layers in a manner similar to how text embeddings
are incorporated into the network. For instance, DreamVideo (Wei et al., 2024c) employs an identity adapter
to encode the identity of the conditioning image, while ID-Animator (He et al., 2024d) uses a face adapter
to capture the facial identity of a human face. Additionally, CLIP features are commonly utilized to extract
global information from the image, which is then used to condition video generation (Zhang et al., 2023d;
Xing et al., 2023; Wang et al., 2023e; Xu et al., 2024b; Jiang et al., 2024b; Zhao et al., 2024a).
Fine-grained-level conditioning.
High-level control is not always desirable. In some cases, we aim
to fully control every detail of the generated video. At the extreme, fine-grained conditioning means the
conditioning image directly corresponds to one of the frames in the generated video. This is achieved by
exposing the network to the fine-grained features of the conditioning image.
The most straightforward approach is to concatenate the image (or its latent representation) with the
noise input. This concatenation typically occurs along the channel axis, with the conditioning image often
serving as the first frame of the training video. This approach is widely adopted in many video generation
models, including SVD (Blattmann et al., 2023a), VDT (Lu et al., 2024a), Motion I2V (Shi et al., 2024),
AtomoVideo (Gong et al., 2024), and CogVideoX (Yang et al., 2024j). Since the model predicts multiple
frames simultaneously and the conditioning image is usually a single frame, the image can either be repeated
across all frames or temporally padded with black pixels, accompanied by an additional mask channel to
indicate the validity of the condition for each frame. Notably, this concatenation-based approach is also
widely used in image generation Brooks et al. (2023); Ke et al. (2024); Zeng et al. (2024c); He et al. (2024c).
Concatenation can also be done along the temporal axis (Shi et al., 2024; Ma et al., 2024b; Zhang et al.,
2024i; Gu et al., 2024a; Dai et al., 2023), where the conditioning image is explicitly set as the first frame of
the predictions. This method avoids the need for modifications to the model architecture.
While concatenation is often used for conditioning on the first frame, Make Pixel Dance (Zeng et al., 2024a)
demonstrates that conditioning on both the first and last frames enables the model to implicitly learn large-
scale motion generation, as shown in Fig. 8
27
MagDiff (Zhao et al., 2024a) designs a pyramid structure to encode conditioning images, effectively capturing
contextual information at multiple scales and enhancing the robustness of the input.
PIA (Zhang et al., 2023f) introduces a plug-and-play image conditioning approach that enables seamless
conditioning across various personalized text-to-image models without the need for fine-tuning.
This is
achieved by non-invasively injecting conditioning features into the first convolutional layer of the U-Net.
Image conditioning can also be implemented using a ControlNet-like approach (Zhang et al., 2023b). For
instance, DreamVideo (Wang et al., 2023a) incorporates image conditioning by forwarding the conditioning
image through the downsampling blocks of the diffusion U-Net and injecting the resulting intermediate
feature maps into the video model. Similarly, SparseCtrl (Guo et al., 2024b) employs a ControlNet-like
structure but extends it to support temporally sparse conditions, enabling the use of one or more frames as
conditioning inputs.
I2V-Adapter (Guo et al., 2023b) highlights a potential challenge of concatenating: when fine-tuning a video
model initialized from a text-to-video model, directly concatenating the input image can disrupt the fun-
damental weights. To address this, they propose cross-frame attention mechanisms to enhance the image
condition. Specifically, additional cross-frame attention layers are introduced after each self-attention layer,
where keys and values are derived from the first frame, and learned queries target each output frame. This
architecture is later adopted in EMO (Tian et al., 2024). VideoCrafter (Chen et al., 2023a) also use cross
attentions for image conditioning, but uses an additional projection network to align the image embeddings
with text embeddings.
Moreover, with the incorporation of optical flow, image conditions can be utilized more explicitly. By warping
the image features using optical flow and decoding the warped image, the generated frames effectively “copy”
information from specific locations in the conditioning image. This approach is employed in several flow-
based video generation methods (Ni et al., 2023; Li et al., 2024i).
Multi-level condition. Semantic-level and fine-grained conditions are often combined to achieve more
robust and precise control.
For instance, both concatenation and CLIP feature cross attention can be
integrated into the U-Net together to improve conditioning consistency (Xu et al., 2024b; Xing et al., 2023;
Zhang et al., 2023d), VideoBooth (Jiang et al., 2024b) combines CLIP feature cross-attention with image
latent cross-frame attention to provide a balance of coarse guidance and fine detail control.
VideoComposer (Wang et al., 2023e) controls the details of generated videos by employing a Spatio-Temporal
Condition Encoder, which encodes various modalities, such as images, sketches, depth maps, and edge maps.
Simultaneously, it leverages the CLIP embedding of an image to control the style of the generated video.
ConsistI2V (Ren et al., 2024) enhances image conditioning by integrating multiple branches that support both
coarse and fine control. These include noise concatenation, image cross-attention, and temporal attention.
Additionally, it introduces layout control by using the low-frequency components of the input image as noise
initialization for the video generation process.
4.1.2
Spatial condition
Spatial Condition
User-Driven Spatial Interactions
Peekaboo, DragNUWA, DragAnything
Multi-Modal Controls
MVideo, DreamVideo-2, MotionPrompting
Scene-Level Conditions
Streetscapes, SparseCtrl, CeneMaster
Object-Level Conditions
Tora, Motion-I2V, FreeTraj,ObjCtrl-2.5D,
Boximator, SG-I2V, MotionBooth
28
Spatial-conditioned video generation leverages explicit spatial information and constraints to guide the syn-
thesis process, granting users greater control over the layout, movement, and dynamics of video elements.
These spatial conditions may appear at multiple levels of granularity—from direct object trajectories and
manipulations to structural guidance via depth maps or scene layouts—and can be combined with other
modalities to ensure coherence and consistency. Ultimately, spatial conditions form the backbone for con-
trolling motion, be it object-level movement or global camera transitions, resulting in videos that more
faithfully align with user intent.
Object-level conditions. This category emphasizes controlling the motion and placement of individual
objects, often through explicit trajectory specification. Such approaches give users fine-grained command
over how objects appear and move throughout the generated scene.
Early examples include Tora (Zhang et al., 2024g), which enables users to input detailed trajectories for
multiple objects, resulting in high-fidelity, multi-object videos that precisely follow the prescribed paths.
Similarly, Motion-I2V (Shi et al., 2024) accepts sparse trajectory annotations and motion brushes directly
on a reference image, ensuring temporal consistency and accurate object-level motion control.
Building upon these ideas, FreeTraj (Qiu et al., 2024a) supports free-form, user-defined trajectories to guide
object motion, allowing flexible and intuitive spatial control. By integrating trajectory information into latent
space features and employing temporal consistency modules, FreeTraj ensures seamless motion across frames
while maintaining coherent scene appearances. Taking this concept further, ObjCtrl-2.5D (Wang et al.,
2024i) introduces object-centric trajectory control in a 2.5D latent space. Users can define paths—including
rotations, translations, and interactions—across multiple objects. With techniques like keyframe-based in-
terpolation and motion-aware attention, ObjCtrl-2.5D guarantees spatial fidelity and smooth transitions.
Beyond trajectory drawing, object localization methods also facilitate spatial conditioning. Boximator (Wang
et al., 2024d) allows users to control object positions through bounding boxes, incorporating a self-tracking
mechanism to maintain motion coherence without retraining. Similarly, SG-I2V (Namekata et al., 2024)
introduces a training-free approach that leverages bounding boxes and user-defined trajectories in an image-
to-video diffusion pipeline. By aligning self-attention feature maps across frames and applying frequency-
based post-processing, SG-I2V enforces temporal consistency, preserves high-frequency details, and enables
precise zero-shot motion control without additional fine-tuning. Such bounding-box-based approaches com-
plement trajectory-driven methods, offering additional avenues for achieving granular and visually consistent
object movements. Additionally, MotionBooth (Wu et al., 2024c) fine-tunes text-to-video (T2V) models for
subject-level control, introducing tailored losses (e.g., subject region and subject token cross-attention losses)
that preserve object identity while enabling flexible subject movement. Its training-free “motion injection”
allows users to define unique object trajectories on the fly with minimal overhead, maintaining fidelity to
the customized subject’s appearance even under significant motion changes.
Scene-level conditions. While object-level conditions dictate the movement of individual elements, scene-
level constraints shape the overall layout, geometry, and camera perspective. Methods like Streetscapes (Deng
et al., 2024) rely on street maps, height maps, and camera trajectories to ensure spatial coherence and
temporal stability in city-scale videos. SparseCtrl (Guo et al., 2024b) uses sparse sketches or depth maps at
keyframes to propagate structural guidance through the sequence, reducing the need for dense inputs while
maintaining spatial fidelity. CineMaster (Wang et al., 2025) further emphasizes 3D-aware scene manipulation
by integrating precise 3D bounding box placement, flexible object and camera motion, and interactive layout
control into a two-stage workflow.
Multi-modal controls. A natural extension of spatial conditioning involves integrating other modalities,
such as text, images, or optical flow, to provide more holistic guidance. MVideo (Zhou et al., 2024a) fuses
semantic (textual) and spatial (image and trajectory) inputs, employing motion-aware attention mechanisms
to ensure that these heterogeneous signals inform both appearance and temporal dynamics, thus allowing
for richer and more contextually grounded video generation.
By combining spatial and trajectory conditions, methods can align object movements and appearances
across time. DreamVideo-2 (Wei et al., 2024c) exemplifies this synergy by using a dual-stream diffusion
architecture. Its Appearance Stream preserves spatial features while a Motion Stream enforces trajectory-
29
Figure 9: An example of integrating camera poses and object motion control for video generation (Fu et al.,
2024).
based temporal dynamics.
Optical flow maps and frame-wise image conditioning further ensure spatial
consistency. Similarly, MotionPrompting (Geng et al., 2024b) maps explicit motion prompts—representing
object or camera trajectories—into latent space and refines them through a two-stage generation process,
achieving both stable global motion patterns and precise spatial fidelity.
User-driven spatial interactions. Beyond automated integration of spatial cues, several works empower
users to interactively manipulate scene content. Peekaboo (Wu et al., 2020) enables direct control over object
placement and movement via spatial and temporal masks, offering flexible adjustments without retraining the
model. DragNUWA (Yin et al., 2023a) allows users to guide arbitrary object movements and camera shifts
by combining trajectories with textual and image inputs, unlocking complex video editing through multi-
modal manipulation.
Extending this idea further, DragAnything (Wu et al., 2024f) leverages draggable
points on objects so users can intuitively reposition and pose them across frames, enhancing interactivity
and customization within video diffusion frameworks.
In essence, spatial conditions and motion controls are tightly interconnected. Specifying spatial guidance
through trajectories, bounding boxes, scene layouts, or depth maps sets the stage for orchestrating how
objects and viewpoints evolve over time. By blending object-level trajectories, scene-scale structures, and
interactive user cues, these methods collectively push toward more controllable and user-aligned video gen-
eration.
4.1.3
Camera parameter condition
Camera parameter conditioning, which precisely manipulates camera poses and trajectories, is crucial for
achieving realistic 3D consistency in video generation.
Equally important is local object motion con-
trol—defining how specific entities move within the scene—so that the overall dynamic is both coherent
and visually compelling. Recent works interweave these two aspects, providing new ways to blend cinemato-
graphic flexibility with targeted motion editing.
Methods like CameraCtrl (He et al., 2024a) rely on camera pose and trajectory inputs to guide video
generation with geometric representations and temporal attention, allowing creators to simulate dynamic
viewpoints for virtual reality and film production. Extending these ideas, CVD (Kuang et al., 2024) enforces
epipolar constraints to maintain geometric and motion consistency across multiple perspectives. Further
exploring 3D constraints, CamCo (Xu et al., 2024b) adopts Plücker coordinates to ensure coherent epipolar
geometry, while Cavia (Xu et al., 2024a) employs cross-view and cross-frame attention layers to align spatial
and temporal cues in multi-view videos. VD3D (Bahmani et al., 2024d) uses scene representations like depth
maps or NeRF, merging appearance and geometry streams for more realistic 3D effects, and AC3D (Bahmani
et al., 2024b) integrates Plücker coordinates with hierarchical training and cross-frame attention to manage
complex motions across multiple viewpoints. Augmenting these efforts, MotionBooth (Wu et al., 2024c)
30
Figure 10:
In audio-conditioned video generation, in conjunction with traditional conditions (e.g., im-
age, text), audio information is usually adopted to augment synchronization with talking motion (facial
expressions, head movements, lip and teeth dynamics) to create a more realistic and emotionally expressive
animation (Tan et al., 2024a).
introduces a lightweight latent shift module for training-free camera motion control, synchronizing smooth
camera movements with subject fidelity via concise loss functions and attention alignment. For more in-depth
discussion on camera conditioning, please refer to Sec. 4.6.3.
On the object motion side, various approaches link bounding-box–level or trajectory-based editing to camera
control for deeper storytelling. Direct-A-Video (Yang et al., 2024i) allows users to specify camera movements
such as panning or zooming alongside simple bounding-box manipulation for objects, removing the need
for explicit motion annotations.
MotionCtrl (Wang et al., 2024j) goes further by letting users provide
detailed trajectory paths for both objects and the camera, integrating these into a diffusion-based model
to achieve precise movements and strong frame-to-frame consistency. Likewise, 3DTrajMaster (Fu et al.,
2024) incorporates 6DoF pose sequences for multi-entity 3D motion control, using a 3D-motion–grounded
injector architecture that fuses text and pose embeddings and a gated attention mechanism within a diffusion
pipeline. CineMaster (Wang et al., 2025) further couples bounding-box–based object manipulation with
camera parameter specification in a two-stage pipeline. A Blender-based interface collects 3D bounding boxes
and camera trajectories, which are automatically interpolated before guiding a specialized diffusion model
that disentangles camera from object motion for enhanced 3D alignment. By merging camera parameters
with object trajectories, these methods deliver more comprehensive control over how scenes unfold, enabling
cohesive, lifelike video generation.
4.1.4
Audio condition
Audio conditions enable synchronization between sound and visuals, crucial for applications like talking head
generation and co-speech gesture animation. By leveraging audio inputs, models generate videos that reflect
the nuances of speech, emotion, and rhythm inherent in the audio signal.
Several works have focused on generating expressive talking head videos synchronized with speech.
EMO (Tian et al., 2024) conditions video generation on audio input and a single reference image, using
facial bounding box masks and head movement speed as weak conditions. By integrating audio features
from pretrained models like wav2vec into a diffusion-based framework, EMO synchronizes facial expressions
and lip movements with the input audio, capturing emotional tones.
31
Similarly, VASA-1 (Xu et al., 2024e) and DreamTalk (Ma et al., 2023) utilize audio features extracted with
wav2vec models to generate talking face videos. VASA-1 incorporates audio features from previous frames
to maintain temporal consistency, employing classifier-free guidance for motion generation. DreamTalk uses
a transformer-based audio encoder to process audio windows, producing synchronized lip movements and
expressions with the help of a lip synchronization expert and a style predictor. Speech2Lip (Wu et al., 2023e)
builds on these ideas by adopting a decomposition-synthesis-composition strategy, where a speech-driven
implicit model focuses on lip movement in a canonical space while a geometry-aware mapping (GAMEM)
handles pose variability for speech-insensitive elements like head movements. A contrastive sync loss further
boosts synchronization quality, even with limited training data, yielding sharper visuals and more accurate lip
alignment than previous methods. Expanding beyond facial animations, ANGIE (Liu et al., 2022b) addresses
co-speech gesture generation by learning motion representations through unsupervised learning. It constructs
codebooks of reusable gesture patterns and predicts future motions based on quantized motion codes and
audio inputs, resulting in natural gesture animations synchronized with speech. Similarly, OmniHuman (Lin
et al., 2025) extends audio-driven generation for more comprehensive human animation tasks such as full-
body movements and human-object interactions, leveraging a multi-condition training strategy (text, audio,
and pose) in a progressive manner, and delivers realistic motion synthesis and robust lip-sync accuracy.
For emotional expressiveness, EMOPortraits (Drobyshev et al., 2024) integrates a speech-driven mode into
one-shot head avatar synthesis, using a multi-view dataset of extreme facial expressions (FEED) to generate
highly expressive facial animations driven by audio inputs. FlowVQTalker (Tan et al., 2024a) combines
normalizing flow and vector quantization to model and generate emotional talking face videos, predicting
facial dynamics using audio inputs, a source image, and an emotion label.
To capture subtle expressions and head movements, AniTalker (Liu et al., 2024d) and AniPortrait (Wei
et al., 2024a) utilize motion representations learned through self-supervised learning. AniTalker introduces a
variance adapter to synthesize speech with controlled attributes, serving as input to a diffusion-based motion
generator. AniPortrait employs a two-stage method, obtaining 3D representations from audio inputs and
converting them into high-quality portrait animations using a diffusion model.
Additionally, CCVS (Le Moing et al., 2021) (Context-aware Controllable Video Synthesis) incorporates audio
tracks as ancillary information to influence the video’s rhythm and mood, maintaining temporal coherence
by utilizing context frames and object trajectories.
32
4.1.5
High-level video condition
High-level
Video Condition
Efficiency-Oriented Methods
Efficiency & Adaptation
SimDA, VidToMe,
STEM Inversion,
Tune-A-Video
Specialized Control
Instruction-based
Editing
InstructVid2Vid, InsV2V
Human-centric
Editing
Follow-Your-Pose, MagicPose,
AnimateAnyone, DisCo, DreamPose,
AniTalker, DPE
Interactive Editing
DragVideo, Drag-A-Video, ReVideo,
VideoSwap
Consistent Editing
Canonical Representation
LNA, VidEdit, DiffusionAtlas,
CoDeF, StableVideo, Lee et al.
Latent Space
Manipulation
Text2Video-Zero, Rerender A Video,
Pix2Video, I2VEdit, RAVE, ControlVideo,
Diffusion Video Autoencoder, ColorDiffusor
Structural & Motion
Guidance
Gen-1, MagicEdit, VideoComposer,
MoCA, FlowVid,
VideoControlNet(Chu et al.),
MotionClone, MotionDirector, SAVE,
VideoControlNet(Hu et al.)
Attention-based
Editing
Tune-A-Video, Vid2Vid-Zero,
Edit-A-Video, EI2, SimDA, UniEdit,
AnyV2V, VidToMe, TokenFlow
In recent years, diffusion-based video editing has become a powerful paradigm for synthesizing and manipu-
lating videos while maintaining consistency in appearance, motion, and structure. Research in this domain
has flourished across different dimensions, including strategies for preserving spatio-temporal coherence,
specialized user controls, and approaches that boost efficiency. Below, we group these methods under three
broad categories and detail the core ideas within each.
Consistent editing. A large body of work addresses how to preserve uniformity across frames, focusing
on refined attention mechanisms, structural and motion guidance, latent space manipulation, and canon-
ical representations. In terms of attention-based approaches, some of the earliest breakthroughs emerged
when text-to-image diffusion models were extended to video. Tune-A-Video(Wu et al., 2023b) introduced
sparse-causal spatio-temporal attention to adapt Stable Diffusion for one-shot video editing, while Vid2Vid-
Zero(Wang et al., 2023d) developed a dense spatial-temporal attention module and cross-frame attention to
capture longer-range temporal dependencies. Edit-A-Video(Shin et al., 2024) likewise addressed temporal
flicker by injecting attention maps from previous frames and applying a novel temporal-consistent blending
33
method. Further enhancements came from methods like EI2(Zhang et al., 2024j) and SimDA(Xing et al.,
2024b), which minimized feature shifts via Shift-restricted Temporal Attention Modules, instance centering,
spectral normalization, and Latent-Shift Attention to achieve smoother updates with fewer parameters. Some
techniques also explicitly combine spatial and temporal features: UniEdit(Bai et al., 2024) and AnyV2V(Ku
et al., 2024) fuse self-attention features across frames or integrate DDIM inversions to preserve both content
and motion. Token-level operations further enhance frame alignment, with VidToMe(Li et al., 2024g) merg-
ing redundant tokens locally and globally, and TokenFlow(Geyer et al., 2023) identifying nearest-neighbor
tokens to propagate consistent structure.
Beyond direct attention, some methods leverage explicit structural or motion signals to guide the diffusion
process. Gen-1(Esser et al., 2023) demonstrated that providing depth maps alongside noisy latents ensures
coherent scene geometry, while MagicEdit(Liew et al., 2023) and VideoComposer(Jiang et al., 2023a) used
ControlNet or spatio-temporal encoders for depth, pose, or sketch guidance. Optical flow is also popular
for ensuring consistent movement: MoCA(Yan et al., 2023a) projects edits from the initial frame onto sub-
sequent frames through flow warping, FlowVid(Liang et al., 2024a) conditions the diffusion on flow-aligned
features, and VideoControlNet(Chu et al., 2023) uses similar flow-based constraints to reduce flicker. Motion-
Clone(Ling et al., 2024c) instead relies on internal temporal attention signals to extract dominant motion
cues, making it training-free. Splitting appearance and motion into separate pathways further enhances
flexibility, as in MotionDirector(Zhao et al., 2023c) (with dual-path LoRA) and SAVE(Song et al., 2023)
(with a motion word and motion-aware cross-attention loss). A specialized example is VideoControlNet (Hu
et al.)(Hu & Xu, 2023), which segments the video into I-, P-, and B-frames for independent generation,
motion-guided updates, and interpolation, respectively.
Latent space manipulation is another powerful direction for preserving consistency.
Text2Video-
Zero(Khachatryan et al., 2023) and Rerender A Video(Yang et al., 2023b) warp the latent codes of initial
or subsequent frames to guide motion while keeping appearance stable. Pix2Video(Ceylan et al., 2023) and
I2VEdit(Ouyang et al., 2024b) inject self-attention features from prior frames to ensure visual continuity.
Other methods, such as RAVE(Kara et al., 2024) (noise shuffling and the grid trick) or ControlVideo(Zhang
et al., 2023e) (an interleaved-frame smoothing pipeline), unify frame latents during denoising. Diffusion
Video Autoencoder(Kim et al., 2023a) explicitly disentangles identity, motion, and background within la-
tent representations to simplify partial edits, and ColorDiffusor(Liu et al., 2023a) addresses color drift with
dedicated Color Propagation Attention and an Alternated Sampling Strategy.
Finally, canonical representations use a consistent 2D atlas or parameterization to propagate edits across
frames.
Methods like Layered Neural Atlases(Kasten et al., 2021), VidEdit(Couairon et al., 2023), and
DiffusionAtlas(Chang et al., 2023b) map videos into an atlas space, apply edits there, and then map them
back to the original frames. CoDeF(Ouyang et al., 2024a) extends this concept using a hash-based design
for canonical and deformation fields, while StableVideo(Chai et al., 2023) and Lee et al.(Lee et al., 2023)
propagate keyframe edits via atlas-based correspondences, with the latter estimating dense semantic links
to handle shape changes.
Specialized control. Several methods emphasize more direct or domain-specific control, whether through
user interaction, human-centric capabilities, or language-based instructions. Interactive editing systems like
DragVideo(Deng et al., 2023) and Drag-A-Video(Teng et al., 2023) adapt point-based image manipulation to
the video domain by tracking user-selected points across time and optimizing latent codes for smooth motion.
ReVideo(Mou et al., 2024) lets users edit the first frame and then specify new motion trajectories, supported
by a three-stage training strategy that decouples content and motion. VideoSwap(Gu et al., 2024b) targets
subject swapping via semantic point correspondences and layered atlases, maintaining consistent appearances
in dynamic scenes.
Human-centric editing often centers on preserving identity, pose, and expressions across multiple frames.
Approaches like Follow-Your-Pose(Ma et al., 2024d) and MagicPose(Chang et al., 2023a) typically train for
appearance control first and then refine temporal or pose-specific features using modules like ControlNet.
Specialized appearance encoders, as in AnimateAnyone(Hu, 2024) or MotionEditor(Tu et al., 2024), capture
high-fidelity details that are integrated into the denoising process for better motion generation. Broader
subject or background manipulations appear in DisCo(Wang et al., 2024e), DreamPose(Karras et al., 2023),
34
and AniTalker(Liu et al., 2024d), leveraging ControlNet-based architectures to separate or recombine different
video elements. DPE(Pang et al., 2023) focuses specifically on decoupling pose and expression through a
bidirectional cyclic training scheme, skipping 3D Morphable Models for simpler pipelines.
Instruction-based editing harnesses natural language to guide transformations in video, making them ac-
cessible to non-experts. InstructVid2Vid(Qin et al., 2024) and InsV2V(Cheng et al., 2023) adapt Prompt-
to-Prompt editing for multi-frame scenarios by generating training data from captioning systems. They
preserve coherence via specialized objectives like an Inter-Frames Consistency Loss and strategies such as
Long Video Sampling Correction.
Efficiency-oriented methods. Since diffusion-based pipelines can be computationally intensive, numer-
ous works focus on streamlining editing without degrading consistency. SimDA(Xing et al., 2024b) em-
ploys Latent-Shift Attention and lightweight adapters, while VidToMe(Li et al., 2024g) uses Token Merging
(ToMe) to compress attention tokens both locally and globally, saving compute cycles across frames. STEM
Inversion(Li et al., 2024f) applies an EM algorithm to find compact bases for each frame, reducing the
dimensionality of video representations and speeding up inference. On a similar note, Tune-A-Video(Wu
et al., 2023b) demonstrates that carefully designed sparse spatio-temporal attention modules can maintain
temporal stability while significantly cutting resource usage.
4.1.6
Other conditions
Recent advancements in video generation have incorporated various special conditions to enhance control,
realism, and applicability. These conditions include audio inputs, physical simulations, trajectories, scene
layouts, and physiological signals. Below, we survey these developments, categorizing the works based on
the special conditions they employ.
Physics-based video generation. Incorporating physics-based conditions ensures that generated motions
adhere to real-world dynamics. PhysGen (Liu et al., 2024c) conditions video generation on external forces
and physical parameters like gravity and friction to simulate realistic object motion. Starting from a single
image, it extracts physical properties, simulates dynamics using a physics engine, and renders the motion
using diffusion models, allowing for physically grounded and controllable video sequences.
Similarly, MotionCraft (Aira et al., 2024) uses physics-based optical flow derived from simulations to guide
video generation. By warping the latent space of a pretrained diffusion model according to the optical flow, it
animates the starting frame with the desired motion, enabling the synthesis of complex movements without
retraining the model.
Extending physics-based conditioning to interactive scenarios, VideoAgent (Soni et al., 2024a) introduces
multi-agent interactions guided by instructional text prompts and environmental contexts. By simulating
agent behaviors that adhere to physical laws, it generates purposeful videos involving multiple interacting
agents.
Physiological signal conditioning. Incorporating physiological signals opens avenues for health-related
applications Synthetic Generation of Face Videos with Plethysmograph Physiology (Wang et al., 2022c) uses
a given PPG signal to guide video generation, reflecting physiological signals like heart rate. Starting from
a single image, it introduces variations to enhance the training of remote photoplethysmography models.
Timestamp conditioning. Timestamp conditions pinpoint precisely when events occur, ensuring proper
sequencing and duration. MinT (Wu et al., 2024h) exemplifies this by binding each event to a designated
time range, using Rescaled Rotary Positional Encoding for coherence, and leveraging scene cut conditioning
for seamless transitions. With an LLM-based prompt enhancer to expand event captions, MinT maintains
smooth multi-event progression, improves text alignment, and preserves subject integrity, highlighting the
importance of time-based control for narrative-rich video generation.
4.2
Enhancement
The enhancement of image and video qualities has been a long-lasting and well-studied topic in computer
vision. Early literature focused on low-level (i.e., pixel-level) video enhancement, where we process videos
35
based on low-level photometric or geometric features to resolve specific issues such as video noise, motion
blur, etc. However, in recent years, the success of generative models including GANs and diffusion models
has offered new options to tackle these traditional low-level tasks. In this chapter, we review and discuss the
potential of these new generative models on traditional video enhancement tasks including video denoising,
video inpainting, video super-resolution, video interpolation and prediction.
4.2.1
Video denoising and deblurring
Video denoising aims to enhance video quality by reducing noise and restoring degradation issues like out-
of-focus blur, motion blur, and compression artifacts such as moiré patterns (Rota et al., 2023). The noise
or degradation defined in these tasks is low-level meaning that it generally does not require much high-level
(semantic/object-level) understanding of the video.
Interestingly, the latest diffusion models are inspired and structured with a denoising framework, so they seem
to be a direct fit for video denoising. However, they have completely different focuses. The latest diffusion
models are mostly designed for highly creative generation tasks, where we use the model to generate from
an image of complete noise. The whole process is more about “imagining” what might be hidden beneath
the noise, rather than simply “removing” the noise. With large datasets, diffusion models learn the overall
distribution of images rather than focusing on refining individual local patches based on appearance. In
contrast, video denoising assumes that the input videos, although noisy, should start with enough quality,
where at least the main content should be be recognizable. The goal is to reduce noise while keeping the
main content unchanged in the video, leaving little room for creativity compared to generative diffusion
models.
More broadly, there have been few successful generative models, not just diffusion models, specifically de-
veloped for video denoising tasks. The reasons are three-fold.
• Different focus: Video denoising requires the model to strictly maintain the original content of the
video and avoid introducing extra new information. The task itself limits creativity, so generative
modesls generally do not fit very well in this area.
• Existence of good traditional methods. Since the task primarily only involves minor local
refinement, video denoising is generally not a very challenging task. Also, due to its long history
in research, many effective traditional methods already exist and can address the problem reliably.
Using generative models for this relatively simple task may seem overkilling.
• Efficiency. Compared to traditional methods, generative models are much more complex, making
them harder to train and less efficient in practical applications.
4.2.2
Video inpainting
Video inpainting is a process used to fill in the missing or corrupted parts of a video Quan et al. (2024). A
masked area is usually specified as a part of the input, and the task is aimed at filling in that area using
spatial and temporal cues to make the video appear seamless and coherent. Note that the masked area can
be specified in various ways depending on the application scenario. Here are some examples.
• In applications where the goal is to restore a corrupted area in the video, the masked area is defined
as the region of corruption, and the task mainly focuses on filling in the area based on the local
continuity of the frame.
• In applications where the goal is to remove unwanted objects, the mask area is defined as the object
masks, and the task focuses more on finding temporal correspondences to fill in the background
behind the object.
While traditional methods typically adhere to a predefined problem setting with a specified masked area,
recent diffusion methods aim to be more creative by incorporating language instructions and addressing
36
“A yellow maple leaf.” (2.7 s)
“A MINI Cooper driving down a road.” (5.3 s)
“A train traveling over a bridge in the mountains.” (8.0 s)
Figure 11: Example of video inpainting with text instructions for video generation. The contents in the
masked regions are inpainted under text instrubtions. Original figure from AVID (Zhang et al., 2024h)
more challenging cases through complex generation. One such example is shown in Figure 11, where masks
and instructions are provided to enable video editing applications as a video inpainting task (Zhang et al.,
2024h).
GAN (Generative Adversarial Network) models were first adopted to solve the inpainting problem from
a generative perspective. Starting from image inpainting, Pathak et al. (2016) uses inpainting as a self-
supervised task to learn video representations, where an adversarial loss is introduced to produce sharper
results. Yu et al. (2018) also introduced local and global WGANs for enhanced results. Recently, Chang
et al. (2019) extended this idea to free-form video inpainting, where the masks can have arbitrary shapes
such as text captions, and proposed a temporal PatchGAN loss to enhance temporal consistency.
Diffusion moodels have also been explored for inpainting tasks, and such explorations mainly start from
image inpainting. RePaint (Lugmayr et al., 2022) applied a pretrained unconditional DDPM and modified
the standard denoising strategy by sampling the masked regions from the diffusion model and sampling
the unmasked areas from the given image. SmartBrush (Xie et al., 2023) explored image inpaiting with
both text and shape guidance with precision control. They try to generate a new object to be filled in the
shape specified by the masks. To preserve the background and improve consistency, they add an extra mask
prediction module in the diffusion process, which is used to guide the sampling process in the inference
stage.
For video inpainting, Green et al. (2024) reframe the task as a conditional generative modeling
problem tackled with conditional video diffusion models. They use sampling schemes specific to inpainting
problems to capture long-range dependencies and show that their model can inpaint diverse novel objects
that are semantically consistent with the context.
To better utilize the multi-modal capability of diffusion models, some research work has focused on adding
inputs of other modalities, such as text instructions, into video inpainting, paving the way for real applications
like text-guided video editing. AVID (Zhang et al., 2024h) builds on existing text-guided image-inpainting
models and improves temporal consistency by integrating motion modules. They also proposed a structure
37
Figure 12: Examples of video extrapolation/prediction (Row 1&2) and interpolation (Row 3). For diffusion
models, it is possible to add text instructions for a better control. Original figure from Fu et al. (2023)
guidance module tailored for different types of masks, and a middle-frame attention guidance method for
longer videos. Subsequently, CoCoCo (Zi et al., 2024) proposes a more efficient motion capture module and
an instance-aware region selection method for better “consistency, controllability, and compatibility” (and
hence the name "CoCoCo").
Furthermore, UniPaint (Wan et al., 2024) presents an interesting argument that video inpainting and in-
terpolation can be unified and mutually enhanced as both tasks involve filling missing information in the
spatial and temproal domains, respectively, for which they propose a spatial-temporal masking strategy dur-
ing training. Wu et al. (2024b) introduces a pure language-driven video inpainting task, where the specified
inpainting masks are no longer needed. Instead, they use natural language instructions, such as instructions
to remove objects, to guide and condition the inpainting process.
4.2.3
Video interpolation and extrapolation/prediction
Video interpolation aims to generate intermediate frames between two consecutive frames to increase the
video’s frame rate.
In contrast, video extrapolation or prediction focuses on forecasting the next frame
following the input frames. Both tasks involve generating new frames along the temporal dimension and
thus depend heavily on understanding the motion between frames.
Traditionally, video interpolation and extrapolation/prediction are distinct tasks approached with different
methodologies. Video interpolation focuses on finding or synthesizing intermediate frames to reflect the
motion between existing frames, while video extrapolation requires more creativity and may rely on external
domain knowledge or common sense to help predict future frames. Moreover, video interpolation has clear
and direct real-life applications and generally produces more reliable results because it uses temporal cues
from both preceding and succeeding frames. As a result, video interpolation has traditionally been of greater
interest compared to video extrapolation/prediction.
However, from the perspective of diffusion models, video interpolation and extrapolation can be tackled in a
more unified framework. One example is illustrated in Figure 12. Therefore, we discuss both tasks together
in this chapter.
38
With the rise of large pretrained diffusion models, video interpolation and extrapolation are increasingly
viewed as conditional generation tasks, rather than relying heavily on motion analysis and image warping.
In both cases, generative methods follow the same approach—generating new frames based on the surround-
ing context frames. This allows a unified diffusion model to be trained to handle both interpolation and
extrapolation.
One early example is MCVD (Masked Conditional Video Diffusion) (Voleti et al., 2022), where a diffusion
model is trained to generate current frames conditioned on past and future frames. By randomly masking
these frames, the model adapts to different tasks. If both past and future frames are masked, the model
performs unconditional video frame generation. If only either past or future frames are masked, it handles
past frame or future frame prediction, respectively. When neither are masked, the model performs video
interpolation. This random masking enables MCVD to learn video interpolation, prediction, and generation
in a unified framework. Similar ideas also appeared in RaMViD (Random-Mask Video Diffusion) (Höppe
et al., 2022), but instead of masking in the temporal dimension, RaMViD apply masks in the spatial domain
and use a unified framework to tackle video interpolation, infilling, and upsampling.
Moreover, some later methods also specifically explored the power of diffusion models on video interpola-
tion. VIDIM uses cascaded diffusion models, where low-resolution target frames are first generated before
generating high-resolution versions conditioned on these generated low-resolution frames(Jain et al., 2024a).
LDMVFI explored latent diffusion models to achieve better perceptual generation quality for video interpo-
lation (Danier et al., 2024).
Some other recent work has also adapted video interpolation/extrapolation to specific application scenarios.
Fu et al. (2023) proposes MMVG (Multi-modal Masked Video Generation) to tackle a new adapted task
called text-guided video completion (TVC) motivated by the uncertainty of frame prediction. Specifically, a
video can have multiple potential future outcomes, so it is better to add text guidance to describe a specific
future that we want to generate. Apart from that, some work explored interpolation for specific styles such
as cartoon (Xing et al., 2024a).
Overall, we have seen great differences between traditional and generative methods.
• Creativity: Traditional methods adhere to the original content, motion, and physics in the video,
while generative methods can generate in a more creative way.
• Dependence on motion: Traditional methods mostly rely on strong motion assumptions such as
motion smoothness and linearity, so they may fail if the underlying motion is complex, nonlinear,
or ambiguous (Jain et al., 2024a). Also, these smooth motion assumptions often result in overly
smoothed predictions by traditional methods. However, diffusion models are less vulnerable to these
problems due to their low dependence on motion understanding.
• Difficulty: Traditional methods are typically designed for simple settings, where only one interme-
diate or future frame is predicted from context frames, with short time intervals between them to
maintain smooth motion assumptions. In contrast, generative methods can handle more complex
tasks, such as interpolating or predicting 5-10 frames at a time, and the context frames can be far
apart in time. An extreme example would be having only the first and last frames of a 5-second
video. In this case, generative models are well-suited to generate the entire video since most of the
information is completely made up. Traditional methods, however, would struggle to handle this
scenario due to lack of context. We will discuss more about long video generation in Section 4.5.
4.2.4
Video super-resolution
Video super-resolution is another well-established task in video restoration, focusing on enhancing low-
resolution videos by upsampling the video frames to produce clearer and sharper high-resolution outputs.
Early research focused more on image super-resolution algorithms, followed by video super-resolution meth-
ods, which introduced temporal consistency as an additional optimization objective. Some visual examples
are shown in Figure 13.
39
(a) Zoomed LR
(b) BSRGAN
(c) Real-ESRGAN+
(e) LDM
(f) Ours
(d) FeMaSR
Figure 13: Video super-resolution examples of latest methods. Original figure from StableSR (Wang et al.,
2024c)
In recent years, diffusion models have also been applied to super-resolution tasks. StableSR (Wang et al.,
2024c) explored leveraging the prior knowledge from pre-trained diffusion models to improve super-resolution
quality. They employed a frozen Stable Diffusion model to generate high-resolution images conditioned on the
low-resolution features computed from a pretrained super-resolution model. A trainable time-aware encoder
module was added to combine the super-resolution features with the diffusion features. Similarly, Yuan et al.
(2024b) proposed to utilize a frozen pre-trained text-to-image diffusion model as the base to accomplish
text-to-video super-resolution. They first use the image diffusion model to generate high-resolution images
one-by-one and then refine the results through a temporal adapter.
Some latest methods have also investigated different ways to combine super-resolution networks (mostaly a
VAE) and diffusion networks (U-Net). SATeCo (Chen et al., 2024e) combined the super-resolution (VAE)
and diffusion model (U-Net) by inserting the U-Net structure in between the VAE encoder and decoder,
together with some adapation or alignment modules. The latest VEnhancer (He et al., 2024b) also shares
a similar structure with additional space-time data augmentation and video-aware conditioning, which are
trained to enhance AI-generated video on both space and time resolutions. In comparison, Upscale-A-Video
(Zhou et al., 2024b) is more designed for long videos. They first utilize the U-Net to generate each video
segment given optical flow inputs and perform a global recurrent latent propagation to ensure consistency
across a longer time range.
The VAE decoder is then applied to decode high-resolution images.
Both
structures have shown pros and cons on flexibility, fidelity and quality.
In addition to using diffusion models to enhance video resolutions, some research work has also studies
how to enhance/adapt low-resolution image/video generation models so that it can also work well on high-
resolution tasks.
Guo et al. (2025) propose a tuning-free method through a pivot replacement strategy
that leverages reliable semantic guidance from the low-resolution model, as well as a tuning method that
integrate learnable multi-scale upsampler modules to learn structural details at the new higher resolution.
To follow up, FreeScale (Qiu et al., 2024c) introduces a tuning-free inference paradigm that fuses features
from different perceptive scales and extracts the desired frequency for each part of the frame.
40
4.2.5
Combining multiple video enhancement tasks
As we have discussed in the previous chapters, many diffusion methods have unified and tackled multiple
enhancement tasks at the same time. To recall a few, UniPaint Wan et al. (2024) combines video inpainting
and video interpolation, and MCVD (Voleti et al., 2022) unifies video interpolation and video prediction.
Notably, VEnhancer (He et al., 2024b) is a plug-and-play method used to enhance a low-quality video
generated by other generation models by removing artifacts and increasing both of its spatial and temporal
resolutions.
This trend mainly stems from the flexibility of diffusion models which can be trained to generate outputs in
a task-agnostic manner. As a result, the differences among enhancement models are more based on different
training data and strategy instead of model structures.
4.2.6
Summary
In summary, recent diffusion-based methods have significantly advanced traditional video enhancement tech-
niques, improving tasks such as video denoising, inpainting, interpolation, extrapolation/prediction, and
super-resolution. From this review, we can see a clear trend as follows.
• Diffusion-based methods introduce greater creativity into traditional low-level video enhancement
tasks. Leveraging the strength of pretrained diffusion models, these approaches can tackle more
complex and challenging scenarios where traditional methods often fall short.
• Diffusion models have also enabled the use of multi-modal inputs, allowing enhancement tasks to
be performed with additional inputs like text instructions. This advancement is revolutionary, as
text-based input provides an effective means of making the enhancement process more controllable
and customizable, opening new possibilities for video editing.
• From the perspective of diffusion models, the distinction between image and video is becoming less
significant. Traditional methods often develop image and video enhancement algorithms separately,
as video enhancement requires careful attention to temporal correspondences and alignment (the
“physics” of motion). In contrast, diffusion models, which are less dependent on explicit motion
understanding, allow for a more seamless and scalable adaptation between image and video models.
• Nevertheless, diffusion models can introduce more fidelity issues due to the inherent randomness of
the diffusion process. In video enhancement, the output is expected to preserve the original content
or subject matter, but diffusion models may introduce new elements that were not originally present.
Finding ways to balance fidelity and quality can be an interesting topic for future research.
4.3
Personalization
Personalization in video generation is essential for creating content that accurately represents specific sub-
jects, capturing their unique identities, expressions, and motions.
Two primary approaches enable this
personalization: subject-specific fine-tuning and subject-specific-driven inference.
While fine-tuning ap-
proaches carefully adapt model parameters to a particular subject, inference-based methods rely on minimal
references—often a single image—to guide identity preservation and motion synthesis without retraining.
Subject-specific fine-tuning.
Subject-specific fine-tuning involves adapting a pre-trained model to a
particular subject by retraining or modifying its parameters using data specific to that subject. This approach
allows the model to embed detailed subject characteristics directly into its learned representation.
DreamVideo (Wei et al., 2024c) fine-tunes an identity adapter on a few static images of the subject, capturing
subject-specific identity traits, while separately training a motion adapter on target motion patterns. This
decoupling allows the system to integrate both identity and motion seamlessly into the final output.
Similarly, PersonalVideo (Li et al., 2024c) adopts a fine-tuning strategy to preserve high identity fidelity
without degrading the model’s motion and semantic capacities. By integrating an isolated identity adapter
into the spatial self-attention layers of a pre-trained text-to-video diffusion model, PersonalVideo fine-tunes
41
DreamVideo 
DreamVideo-2
Training
Inference
…
…
General data training
Subject-specific data finetuning
Subject-specific data
General  data
Subject-specific data
Figure 14:
A high-level comparison of two personalization paradigms—subject-specific finetuning and
subject-specific-driven inference—illustrated by DreamVideo (Wei et al., 2024c) and DreamVideo-2 (Wei
et al., 2024b). In DreamVideo, each new subject (its images and desired motion) is used to finetune the
model. In contrast, DreamVideo-2 leverages a pretrained diffusion model trained on general videos (decom-
posed into subjects and bounding boxes); it then generates videos for a new subject based on a single image
and bounding boxes—without requiring additional finetuning.
identity features without disrupting motion dynamics. This method enhances robustness through simulated
prompt augmentation, ensuring that even with limited reference images, the model achieves reliable identity
preservation alongside customizable style and motion.
By extending these techniques to scenarios without specialized video data, Still-Moving (Chefer et al., 2024)
leverages text-to-image (T2I) customization and introduces lightweight spatial and temporal adapters for
text-to-video pipelines. By training on duplicated still images (or “frozen videos”), it retains motion priors
while aligning T2I and T2V features, enabling diverse personalization with only a handful of static images. In
parallel, Dynamic Concepts (Abdal et al., 2025) addresses more inherently dynamic concepts—subjects whose
characteristic motion is as integral to their identity as appearance—by fine-tuning a Diffusion Transformer
in two stages. First, it learns an appearance “basis” from an unordered set of frames, and then refines
temporal dynamics by freezing the learned parameters and focusing on motion consistency. Finally, Video
Alchemist (Chen et al., 2025) expands personalization to multi-subject, open-set scenarios by fine-tuning a
Diffusion Transformer, and personalizes both foreground objects and backgrounds while mitigating overfitting
through targeted data augmentation.
Subject-specific-driven inference. Subject-specific-driven inference personalizes video outputs at run-
time without retraining the model for each new subject. These methods typically rely on a single reference
image (and sometimes auxiliary inputs like audio or text prompts) to guide identity preservation and ex-
pression synthesis.
Many audio-driven talking head frameworks fall into this category. EMO (Tian et al., 2024), VASA-1 (Xu
et al., 2024e), AniTalker (Liu et al., 2024d), and AniPortrait (Wei et al., 2024a) generate expressive talking
head videos from a single reference image and an audio input. They rely on diffusion-based models and
carefully designed motion encoding strategies to preserve subject identity while animating expressions and
head movements. Approaches like DreamTalk (Ma et al., 2023), EMOPortraits (Drobyshev et al., 2024),
and FlowVQTalker (Tan et al., 2024a) extend this concept by incorporating emotional expressiveness, pre-
42
dicting personalized emotion intensity from audio without additional style references, and ensuring identity
consistency even under intense expressive variations. Beyond facial animation, ANGIE (Liu et al., 2022b)
expands this paradigm to co-speech gesture generation, personalizing gesture patterns from minimal inputs
while retaining the speaker’s characteristic style.
Recent works further refine and expand subject-specific-driven inference into broader video customization
scenarios. For instance, DreamVideo-2 (Wei et al., 2024b) introduces zero-shot subject-driven video gen-
eration with precise motion control. Using only a single reference image and bounding box sequences to
define the subject’s trajectory, it employs reference attention mechanisms and mask-guided motion modules
to integrate subject appearance with flexible, fine-grained motion paths—achieving personalization without
additional fine-tuning steps.
Building on this idea of inference-time personalization, ConsisID (Yuan et al., 2024a) demonstrates how
decomposing identity features into low-frequency (global facial contours) and high-frequency (fine details)
components can ensure identity preservation in text-driven synthesis. By integrating these components into
a pre-trained Diffusion Transformer, ConsisID consistently captures the subject’s essential features across
varying poses and expressions, all without retraining for each subject.
Lastly, MCNet (Hong & Xu, 2023) addresses motion transfer scenarios, animating a still image to follow the
dynamics of a driving video while preserving identity through an implicit identity representation conditioned
memory module. By extracting and reusing structural and appearance details from a single reference image,
MCNet maintains subject fidelity throughout complex motions—again, without retraining on the target
subject. Together, these approaches illustrate the growing versatility of inference-based frameworks that
balance flexible control, identity preservation, and visual fidelity without the burden of subject-specific fine-
tuning.
4.4
Consistency
Consistent Appearance & Motion
Simple Conditioning
Spatial / Temporal Conditioning,
Layout Guidance
Ours
Prior
Methods
"a dog wearing a
Superhero outfit with
red cape flying through
the sky."
Inconsistent Appearance & Abrupt Motion
SEINE
Ours
Teddy bear walk-
ing
down
5th
Avenue.
A fast-paced mo-
torcycle race on
a winding track.
Videos: click to play in Adobe Acrobat
Figure 15: Comparison of image-to-video generation results obtained from SEINE and ConsistI2V. SEINE
shows degenerated appearance and motion as the video progresses, while ConsistI2V maintains visual con-
sistency utilizing spatial and temporal conditions.
Improving the consistency of generated videos has been a hot topic recently. Researchers have been working
on improving consistency in two different perspectives, i.e., semantic consistency, and motion consistency.
Early attempts (Yu et al., 2021; Skorokhodov et al., 2022; Shen et al., 2023; Zhang et al., 2023c) for GAN-
based video generation methods mainly add additional discriminators or learned representations for control-
ling consistency. For example, DI-GAN (Yu et al., 2021) introduces an implicit neural representations-based
video generator that improves motion dynamics by manipulating space and time coordinates along with a
motion discriminator for temporal consistency. StyleGAN-V (Skorokhodov et al., 2022) employs continuous
motion representations as positional embeddings.
43
Diffusion models. Semantic consistency (Ren et al., 2024; Zhang et al., 2024i; Jiang et al., 2024b; Li
et al., 2024g; Zhou et al., 2024d; Sun et al., 2023b) ensures the semantic meaning including objects and
relationships between objects are smooth between different frames. The biggest problem is semantic drifting,
which refers to the gradual shift or change in the meaning or context of visual content as a video progresses.
StableVideo (Chai et al., 2023), as a pioneer work, proposes to propagate the representation to the next
frame to ensure consistency. Similarly, TokenFlow (Geyer et al., 2023) preserves consistency by explicitly
propagating diffusion features based on inter-frame correspondences using sampled tokens for video editing.
GLOBER (Sun et al., 2023a) employs a novel adversarial loss along with conditioning the generation on the
global features extracted by a video encoder for global consistency. Specifically, for face video generation,
EMO (Tian et al., 2024) preserves visual/identity consistency by encoding a reference image/video/audio
and cross-attention for video generation.
Motion consistency consists of multiple aspects, such as video smoothness, reasonable object motion, and
stable camera movement, while two different frameworks, including residual-based framework (Deng et al.,
2024; Zhou et al., 2024d) and optical flow-based framework, are proposed to ensure the motion consistency
of generated videos. For the residual-based methods, they usually add additional motion representation
extracted from previous frames or text prompts. As a representative work, Cinemo (Ma et al., 2024b) learns
to generate residual images between a previous frame and a later frame for motion control. Moreover, it
also incorporates a new noise refinement with discrete cosine transformation for controlling the input noise.
VideoComposer (Wang et al., 2023f) uses motion vectors, depth sequences, mask sequences, and sketch
sequences to assist in the generation of video frames. On the other side, in addition to the image-based
residual, StoryDiffusion (Zhou et al., 2024d) proposes to split the long text prompts into multiple short text
prompts to improve consistency.
For the optical flow-based models, they usually learn to generate optical flow between a previous frame and a
later frame to maintain consistency. LFDM (Ni et al., 2023) introduces a flow predictor during training and
injects flows into the denoising procedure for motion consistency. Similarly, Motion-I2V (Shi et al., 2024)
consists of two stages. It first generates optical flows based on the input image and then videos are generated
conditioned on the concatenation of these optical flows. For video editing, CAMEL (Zhang et al., 2024b)
improves consistency through a frequency-based perspective, where the video is decomposed using low- and
high-pass filters. Moreover, it learns additional motion prompts for further stabilize the video.
Apart from these methods, another popular paradigm for consistency is controlling the noise used in diffusion
models. PYoCo (Ge et al., 2023) is first proposed to employ a novel correlated noise model, which consists
of a noise shared among all frames and an individual noise conditioned on the previous frames. Following
this, a
R
-noise (Chang et al., 2024) is proposed to warp noise following the flow and preserve the spatial
Gaussian property, which is able to preserve the motion and semantic consistency at the same time.
One application of consistency is long video generation, which poses high requirements for motion and
semantic consistency. long video generation (Ouyang et al., 2024c) is always achieved by ensuring a smooth
transition and semantic consistency among different frames. StoryDiffusion (Zhou et al., 2024d) proposes
to use a novel consistent self-attention and a semantic space temporal prediction for object consistency.
Similarly, StreamingT2V (Henschel et al., 2024) employs a long-term memory block, a short-term memory
block, and also a new blending approach for infinitely long video generation. The short-term block focuses
on consistency between adjacent frames, while the long-term block forces the model to depend on the first
several frames to avoid forgetting. Later, SEINE (Chen et al., 2024d) proposes to employ a random-mask
model for generating transitions based on textual descriptions. Multiple randomly masked images can be
seen as images from different scenes and they can provide an overall comprehensive angle to consistency and
long video generation. On the other side, 3D scene mesh (Fridman et al., 2023) is also used for generating
videos, which can be seen as another application of consistency. Specifically, the video frames and meshes
are updated through several pre-trained models and the consistency between different frames is preserved
through the iterated refinement of the mesh.
44
Multimodal Scripts
CLIP
Compressor
SDXL
Compressed
Tokens
Decoder
Encoder
Image Renderer Training
Multimodal Modeling with VLM
Image
Render
Auto-regressive Prediction
KeyFrame 3224
KeyFrame 3225
KeyFrame 3226
Image-to-video Gen
Image-to-video Gen
Image
Render
Image
Render
Characters:
- A man in a black suit with 
slicked-back hair. <Face ID>
- A woman with a teal headdress 
and matching necklace.<Face ID>
Scene Elements:
- Crowded event
- Elegant Lighting
Plot:
- A man … a woman  … are dancing 
closely at a crowded event.
There is a subtle but palpable
sense of connection or      
confrontation between the man
and the woman, suggested by
their direct gaze and proximity
to each other amidst a crowded
setting
Face Embedding
Description Embedding
Inference
…
…
…
…
…
Multimodal Scripts
Multimodal Scripts
Figure 16: The framework of MovieDreamer. The autoregressive model takes multimodal scripts as input
and predicts the tokens for keyframes. These tokens are then rendered into images, forming anchor frames
for extended video generation. MovieDreamer ensures long-term coherence and short term fidelity in visual
storytelling with the character’s identity well preserved.
4.5
Long video
Long video understanding, including retrieval, classification, and generation, has been a challenging problem
for a long time. Videos are always of different lengths and might be infinitely long, such that models find
it hard to keep content consistent among different frames. In this part, we will introduce representative
methods in long video generation.
Almost all the traditional long video generation methods are divide-and-conquer-based methods. They
usually first generate keyframes and then generate other frames conditioned on these generated keyframes.
TATS (Ge et al., 2022) uses multiple models (interpolation and autoregressive transformers) to ensure the
transition between frames is consistent. Later, to further improve the flexibility of long video generation,
Brooks et al. (2022) generates low-resolution sequences, which are then enhanced to high resolution. Besides,
CogVideo (Hong et al., 2023a) further interpolates non-key frames with the help of keyframes and text
conditions with a two-stage-based framework. Another work by Zhang et al. (2023c) is built upon StyleGAN-
v (Skorokhodov et al., 2022), which suffers from undesired content jittering for long video generation. They
further proposed a novel B-Spline-based motion representation to ensure temporal smoothness.
Diffusion models. Different from traditional models, the solutions in diffusion models are more diverse and
can be split into two main categories, i.e., divide and conquer (sliding windows and keyframes), autoregressive
models (including sliding windows), and consistency. For divide-and-conquer-based methods (EMO), the
generation of long videos is split into several stages and for each stage, the video is generated based on the
previous one or the input condition for refinement. NUWA-XL (Yin et al., 2023b) employs a hierarchical
method for generating long videos. Specifically, a global diffusion model is first applied to generate the
keyframes across the entire time range, and then local diffusion models recursively fill in the content between
nearby frames. An earlier work (Harvey et al., 2022) proposes a RAG-style generation pipeline for generating
45
Training on 3D dataset
Camera conditioning
3D-aware architecture
Figure 17: For a video diffusion model to be 3D aware, the general paradigm includes training on 3D dataset,
adding camera control to the model, and introducing 3D-aware model architectures.
adapted long video frames continued on any other videos along with a novel generation scheme (attention
map) for not forgetting the early frames. Gen-L-Video (Wang et al., 2023b) also employs a hierarchical
strategy to construct long videos through a set of short videos. FreeNoise (Qiu et al., 2024b), as an example,
employs a sequence of noises for long-range correlation and performs temporal attention over them by sliding
windows. Moreover, it also uses a motion injection method to condition the generation of long videos on
multiple text prompts. Besides, instead of considering directly generating long videos, MAVIN (Zhang et al.,
2024a) chooses to first generate several small clips and then generate the transition between them to maintain
consistency with a boundary frame guidance module and a noise sampling strategy.
On the other side, autoregressive methods are drawing more and more research attention. For example, a
sliding window, which can be seen as an application of autoregressive methods, is used to enable a short-video
generator to generate long videos while the hidden representation of the video is interpolated when condition-
ing on different prompts. Moreover, many methods (Gao et al., 2024; Zhao et al., 2025; Xie et al., 2024a) are
proposed with multimodal autoregressive models. MovieDreamer uses a multimodal autoregressive model
with diffusion models to generate long videos conditioned on multimodal scripts. The multi-modal autore-
gressive model is designed to capture the key semantics in the multimodal conditions. (Xie et al., 2024a)
further shows that existing models can be easily extended to autoregressive video diffusion models by varying
the noise level instead of using a stable noise level.
Apart from these methods, more models are proposed to address the high computational requirements
and the efficiency using diverse strategies and solutions (Diffusion Forcing). Specifically, to speed up the
generation procedure, Video-Infinity (Tan et al., 2024b) is proposed to distribute the generation for one long
video to multiple GPUs with the proposed clip parallelism and dual-scope attention modules. It is able to
generate videos up to 2,300 frames in approximately 5 minutes with 8 × Nvidia 6000 Ada GPUs (48G),
which is 100 times faster than the current methods.
4.6
3D-aware video diffusion
The world exists in three spatial dimensions plus time, while a video represents a 2D projection of this 3D
world. Consequently, video diffusion models should adhere to certain 3D constraints. These models gener-
ate multiple frames simultaneously and capture inter-frame relationships through attention layers spanning
different frames.
Existing research, such as SORA (Li et al., 2024h), demonstrates that by training on
large-scale datasets, networks can implicitly learn 3D priors embedded within the data.
46
Figure 18: A visualization of the 3D datasets used by Cavia (Xu et al., 2024a).
While this approach highlights a promising direction for learning 3D priors, there remains significant value in
explicitly incorporating 3D awareness into video diffusion models. Doing so could improve 3D controllability
and facilitate easier and more effective learning for the model.
4.6.1
Training on 3D dataset
Video diffusion models are typically trained on in-the-wild videos that feature arbitrary camera motions and
dynamic scenes, making it challenging for these models to learn robust 3D priors. To enhance a model’s
ability for 3D generalization and multi-view consistency, one straightforward approach is to fine-tune the
video diffusion model using datasets that explicitly capture 3D characteristics.
SV3D (Blattmann et al., 2023a) fine-tunes a pre-trained stable video diffusion model on orbiting videos of
3D synthetic objects from the GSO (Downs et al., 2022) and OmniObject3D (Wu et al., 2023d) datasets.
These orbiting videos include both static orbits, where the camera circles an object at regularly spaced
azimuths with a fixed elevation, and dynamic orbits, where the camera trajectory is irregular. Similarly,
V3D (Chen et al., 2024f) fine-tunes on the Objaverse dataset, leveraging 360-degree orbiting videos, along
with proprietary datasets featuring similar setups (Junlin Han, 2024; Melas-Kyriazi et al., 2024).
Generative Camera Dolly (Van Hoorick et al., 2024) introduces two synthetic datasets, Kubric-4D and
ParallelDomain-4D, which provide multi-view videos of dynamic scenes.
By fine-tuning a Stable Video
Diffusion (SVD) model solely on these synthetic datasets, they achieve zero-shot generalization to real-world
novel view synthesis tasks.
CamCo (Xu et al., 2024b) fine-tunes a video diffusion model on real-world camera fly-through data from the
WebVid10M dataset (Bain et al., 2021), using estimated camera poses. This enables the model to generate
scene-level videos with precise camera controls. Similarly, CVD (Kuang et al., 2024) combines RealEstate10K
(Zhou et al., 2018b) and WebVid10M (Bain et al., 2021) to train a camera-controllable video diffusion model.
Diffusion4D (LIANG et al., 2024) extends video diffusion into the realm of 4D generation by fine-tuning on
orbiting videos where both the camera and object are in motion, enabling models to capture spatiotemporal
dynamics more effectively.
Cavia (Xu et al., 2024a) focuses on camera-conditioned image-to-multi-view-video generation. They train
their model on a large-scale dataset of approximately 0.4 million videos annotated with camera poses. These
videos are sourced from a diverse collection of datasets, including Wild-RGBD (Xia et al., 2024), MVImgNet
(Yu et al., 2023c), RealEstate10K (Zhou et al., 2018b), CO3D (Reizenstein et al., 2021), DL3DV-10K (Ling
et al., 2024b), Objaverse (Deitke et al., 2022; 2023), Diffusion4D (LIANG et al., 2024), OpenVid (Nan et al.,
2024), and InternVid (Chen et al., 2023b). A visualization of the dataset collections used in Cavia can be
seen in Fig. 18.
HumanVid (Wang et al., 2024h) introduces a human-centric dataset enriched with camera annotations,
enabling precise modeling of human-centric video generation.
47
KFC-W (Chou et al., 2024) introduces an auxiliary task of multi-view inpainting for training video models.
This approach enables the model to learn 3D priors from large-scale, unposed internet photos.
CineMaster (Wang et al., 2025) proposes a customized data labeling pipeline that leverages existing depth
prediction (Yang et al., 2024e;f), instance segmentation (Ravi et al., 2024) and scene reconstruction (Zhang
et al., 2024d) models to generate labeled data.
This enables a 3D-aware model to train on large-scale,
in-the-wild video data.
4.6.2
Architecture for 3D diffusion models
In addition to data-driven approaches for learning 3D priors, another promising strategy involves designing
3D-specific model architectures that incorporate inductive biases for generating view consistent images.
Traditional U-Net based video diffusion models typically capture inter-frame relationships using temporal
attention layers, which are implemented through per-pixel attention along the temporal dimension. While
this enables information flow between frames at the same spatial positions, it is suboptimal for achieving 3D
awareness.
Recently, multi-view diffusion models (Shi et al., 2023d; Tang et al., 2023; Yang et al., 2024b; Liu et al.,
2024b;f; Hu et al., 2024; Kant et al., 2024; Yang et al., 2024d; Oh et al., 2023; Tang et al., 2024) have gained
significant attention for static 3D scene generation.
By employing cross-frame attention, these models
effectively enhance the spatial relationships necessary for 3D consistency. For static 3D scene generation,
epipolar-constrained attention (Xu et al., 2024b; Huang et al., 2023c; Zheng et al., 2024a) further enforces
strict adherence to 3D constraints, ensuring greater structural accuracy.
This concept has also been extended to multi-view video diffusion models (Li et al., 2024a; Xie et al., 2024b;
Xu et al., 2024a).
By integrating both cross-frame and cross-view attention mechanisms, video models
are fine-tuned to support 4D generation, enabling them to produce spatiotemporally consistent videos that
adhere to 3D principles.
In DiT-like structures (Gao∗et al., 2024; Chou et al., 2024; Liang et al., 2024b; Gu et al., 2025; Wang et al.,
2025), cross-frame attention is inherently achieved as tokens for different temporal and spatial locations are
jointly mixed through transformer, allowing the model to learn 3D priors directly from data.
4.6.3
Camera conditioning
Cameras serve as a bridge between the 3D world and 2D images. Given a 3D scene, intrinsic and extrinsic
camera parameters determine how the 3D scene is projected into a 2D image. Adapting a video diffusion
model to be 3D-aware involves conditioning the model with camera parameters, allowing users to control
the generated views.
Typically, fine-tuning is required after introducing this additional conditioning, as
discussed in Sec. 4.6.1.
Several commonly used approaches exist for conditioning video diffusion models with camera parameters:
• MLP-based embedding. A straightforward method is to use a fully connected multi-layer percep-
tron (MLP) to embed camera parameters. These parameters are often represented as elevation and
azimuth angles, as in SV3D (Voleti et al., 2024; Yang et al., 2024c), panning and zooming movement
(Yang et al., 2024h), or as a relative camera extrinsic matrix (Van Hoorick et al., 2024; Wang et al.,
2024k; 2025). This embedding serves as a global condition, typically introduced into the diffusion
model via cross-attention layers.
• Ray map. A camera defines a mapping from each pixel location in image space to a ray in 3D
space. This mapping can be represented as a "ray map," which stores the ray origin and direction for
each pixel location instead of RGB color. The ray map forms a spatially aligned representation with
six channels, matching the resolution of an image. This method is particularly suited for scene-level
video diffusion models requiring pixel-aligned conditioning (Gao∗et al., 2024; Watson et al., 2024;
Wu et al., 2024e).
48
• Plücker coordinates. Plücker coordinates are an alternative to a ray map, parameterizing a ray
as (o × d, o), where o is the ray origin, d is the normalized ray direction, and × denotes the cross
product. This representation provides a uniform way to encode rays, making it a suitable positional
embedding for video diffusion models (Xu et al., 2024b;a; Bahmani et al., 2024d;b; He et al., 2024a;
Kuang et al., 2024; Yao et al., 2024; Wang et al., 2024h; Zheng et al., 2024a; Liang et al., 2024b).
• Point-based. Camera conditioning can also be achieved implicitly by motions of 2D or 3D points.
For instance, I2VControl-Camera (Feng et al., 2024b) employs point trajectories in the camera
coordinate system as control signals, enabling precise control over the generated views. Similarly,
Motion Prompting (Geng et al., 2024a) converts camera poses into image-space point trajectories.
This is achieved by projecting a 3D point cloud, estimated using a monocular depth estimator, onto
2D points in image space. Diffusion as Shader (Gu et al., 2025) leverages a tracking video of dense
3D points as control signals to train a video DiT model, enabling detailed control over both camera
and motion controls.
4.6.4
Inference-time tricks
Interestingly, there are a few efforts being made to explore whether it is possible to directly use a pre-trained
video diffusion model for 3D-aware video generation without any fine-tuning. ViVid-1-to-3 (Kwak et al.,
2024) combines a video diffusion model with Zero-1-to-3 XL to jointly denoise multi-view videos, effectively
leveraging the 3D priors from Zero-1-to-3 XL alongside the temporal consistency inherent in video diffusion
models. NVS-Solver (You et al., 2024) guides the sampling process of a pre-trained Stable Video Diffusion
(SVD) model by iteratively modulating the score function with the given scene priors represented derived
from warped input views, effectively achieving novel view synthesis from sparse input views. CamTrol (Hou
et al., 2024) directs video generation by employing DDIM inversion noise derived from point cloud renderings,
enabling 3D-aware video synthesis without the need for model fine-tuning.
5
Benefits to other domains
5.1
Video representation learning
Video representation learning (Ravanbakhsh et al., 2024), as a foundational task in video understanding,
it provides downstream tasks with powerful backbones and useful representations.
Early methods (Seo
et al., 2020; Wu et al., 2022a) primarily relied on 2D Convolutional Neural Networks (CNNs) (He et al.,
2016; Krizhevsky et al., 2012) to extract spatial features from individual frames. These methods treated
video data as sequences of static images, which limited their ability to capture temporal dynamics.
To
address this, researchers began incorporating Recurrent Neural Networks (RNNs) (Hopfield, 1982) and Long
Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) networks to model temporal dependencies
across frames. However, these approaches (Liu et al., 2016; 2018) often struggled with capturing complex,
long-range temporal relationships and integrating spatial-temporal information efficiently.
The development of 3D CNNs marked a significant advancement, enabling simultaneous modeling of spatial
and temporal features by applying convolutions across both space and time dimensions. This shift allowed for
more holistic video representation (Tran et al., 2015), leading to better performance in tasks such as action
recognition (Yang et al., 2019; Kong et al., 2017) and video captioning (Yan et al., 2022). More recently,
attention mechanisms and Transformers (Vaswani et al., 2017) have been introduced to further enhance video
representation learning (Zhao et al., 2022; Gabeur et al., 2020; Cao et al., 2022; Botach et al., 2022; Wu
et al., 2022b). These models, especially those utilizing self-attention, can capture long-range dependencies
and complex interactions between frames, leading to more robust and generalizable video representations.
The evolution of video representation learning has been characterized by a gradual shift from static image
processing to dynamic, sequence-based modeling, reflecting the unique challenges of video data.
On the other side, video-language learning (Radford et al., 2021; Li et al., 2022) has been boosted by
the integration of advanced models (Vaswani et al., 2017; He et al., 2016) and the increasing complexity of
tasks that involve understanding both visual and textual information. Early attempts at video representation
49
learning (Lei et al., 2021) relied heavily on separate processing of video and language inputs, where modality-
dependent models were used to extract features from video frames and textual data independently. However,
this approach often fails to capture the intricate relationships between the two modalities. The introduction
of Transformer-based models (Ye et al., 2023) marked a significant advancement, enabling more effective
modeling of cross-modal interactions through self-attention mechanisms. These models can jointly process
video and language inputs (Zhao et al., 2023b), leading to improved performance on tasks, e.g., video
captioning (Li et al., 2023), video retrieval (Wang et al., 2024b), and video question answering (Pan et al.,
2023).
As shown by previous works in image generation (Chi et al., 2024), a pretrained video generative model
might be able to serve as prior, loss, or regularize in learning video representations. Recently, a pioneer
work (Yeh et al., 2024) employs image and video generative models for hateful visual content recognition. It
queries well-trained SOTA image and video generative models to automatically scale up training data pairs
for hateful visual content recognition, achieving superior performance.
5.2
Video retrieval
Video retrieval, mainly video-text retrieval (VTR), which involves cross-modal alignment and abstract un-
derstanding of temporal images (videos), has been a popular and fundamental task of language-grounding
problems (Wang et al., 2020a;b; 2021; Yu et al., 2023b). Most existing conventional video-text retrieval
frameworks (Yu et al., 2017; Dong et al., 2019; Zhu & Yang, 2020; Miech et al., 2020; Gabeur et al., 2020;
Dzabraev et al., 2021; Croitoru et al., 2021) focus on learning powerful representations for video and text and
extracting modality-dependent representations with modality-dependent encoders. Inspired by the success
of self-supervised pretraining methods (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and
vision-language pretraining (Li et al., 2020; Gan et al., 2020; Singh et al., 2022) on large-scale unlabeled
cross-modal data, recent works (Lei et al., 2021; Cheng et al., 2021; Gao et al., 2021; Ma et al., 2022; Park
et al., 2022; Wang et al., 2022a;b; Zhao et al., 2022; Gorti et al., 2022; Luo et al., 2022; Liu et al., 2022d;a;
Cao et al., 2022) have attempted to pretrain or fine-tune video-text retrieval models (Gorti et al., 2022; Lei
et al., 2021).
With the rapid progress in video generation, leveraging the advancement in video generation (Chen et al.,
2024b; Muaz et al., 2023) to advance video retrieval by reranking (Zhou et al., 2024e; Dutta et al., 2021) or
other methods becomes more and more promising. On the other side, retrieval augmented generation could
also help faithful video generation (Arefeen et al., 2024; Rosa, 2024; Tevissen et al., 2024; He et al., 2023a).
5.3
Video QA and captioning
Video question answering (VideoQA) (Zhong et al., 2022; Wu et al., 2024a) is a task to predict the correct
answer given a question and a video. Early work (Zeng et al., 2017; Xu et al., 2017; Zhao et al., 2017;
Jang et al., 2017) focused on simpler factoid questions with the introduction of datasets like MSVD-QA and
TGIF-QA. Techniques during this period primarily relied on attention mechanisms and memory networks
to handle the spatial and temporal complexities of video data. Later, more challenging datasets emerged,
such as NExT-QA, which emphasized inference and reasoning over merely recognizing visual facts. This
shift necessitated the development of more advanced models, including Graph Neural Networks (GNNs) for
relational reasoning and Transformers for their powerful sequence modeling capabilities, particularly when
enhanced with cross-modal pre-training strategies.
Recently, the focus has increasingly shifted towards models that can handle multi-modal inputs and perform
more complex reasoning tasks. Knowledge-based VideoQA datasets and techniques have also attracted more
and more attention, pushing the boundaries of what VideoQA systems can achieve. Modular networks and
neural-symbolic approaches are being explored to improve flexibility and interpretability, addressing the
need for models that can reason about causal and temporal relationships in video content. On the other
side, understanding long videos could be a big challenge to current models as long videos require a more
comprehensive understanding among different frames and a long term memory.
50
Video diffusion model 
as 3D prior
Multi-view video 
diffusion model
3D/4D scene representations
(NeRF, Gaussian Splatting, Mesh, …)
Video diffusion model 
as motion prior
•
Direct supervision
•
SDS loss family
•
Feed forward 
reconstruction
Figure 19: A common paradigm for 3D and 4D generation is by distilling 3D or motion knowledge from
video generation models to 3D and 4D scene representation.
In the future, pre-trained models originally designed for video generation can be used to enhance VideoQA
by predicting future frames or generating hypothetical scenarios to answer complex questions. Also, text-
to-video generation methods can be used for generating diverse VideoQA benchmarks.
5.4
3D and 4D generation
One emerging area of research focuses on generating 3D and 4D scene representations, such as textured
meshes, Neural Radiance Fields (NeRF) (Mildenhall et al., 2021), and Gaussian Splatting (GS) (Kerbl
et al., 2023). These representations hold significant potential for applications in 3D content creation and
immersive virtual reality. However, due to the heterogeneous nature of 3D and 4D representations, and the
scarcity of large-scale 3D and 4D datasets, it is difficult to design feed-forward neural networks capable of
directly generating such representations.
To address these challenges, a common approach involves distilling knowledge from video generative models
to reconstruct multiple viewpoints of a 3D scene. This strategy leverages advancements in video generation
to synthesize complex 3D structures effectively. Recent studies have explored the potential of video diffusion
models in facilitating 3D and 4D generation, offering promising avenues for bridging the gap between video
and high-dimensional scene representation.
5.4.1
Video diffusion for 3D generation
When generating 3D scenes from 2D models using multiple views, a major challenge is to maintain view
consistency, which requires capturing inter-frame correlations across the generated views. Video diffusion
models provide a general solution to inter-frame correlations. By treating the temporal dimension as different
viewpoints, pre-trained video diffusion models can be fine-tuned into multi-view diffusion models. Several
methods have been proposed to use a 2D video generation model for 3D scene generation:
V3D (Chen et al., 2024f) leverages the predictions of fine-tuned video diffusion models as direct supervision
to generate 3D Gaussian splatting or meshes. SV3D (Voleti et al., 2024) employs the outputs of a fine-tuned
Stable Video Diffusion model to supervise a Neural Radiance Field (NeRF) in the coarse stage, then uses
SDS loss to refine high-frequency details of DMTet (Shen et al., 2021) initialized from the coarse stage.
IM-3D (Melas-Kyriazi et al., 2024) adopts an instructive NeRF-to-NeRF (Haque et al., 2023) pipeline for
distillation. This involves iteratively adding noise to rendered results and using a diffusion model to denoise
the images, which then serve as targets for updating the 3D representation. VFusion3D (Junlin Han, 2024)
demonstrates the potential of fine-tuned video diffusion models as data generators for creating large-scale
3D datasets. These datasets can be used to train feed-forward 3D generation models. Hi3D (Yang et al.,
2024c) follows a coarse-to-fine 3D generation pipeline. It first generates low-resolution orbiting videos using
a text-to-video model, then refines these into high-resolution orbiting videos with a video-to-video model.
Both models are fine-tuned from pre-trained video diffusion models with camera condition extensions. The
high-resolution orbiting videos are subsequently used to reconstruct 3D meshes. ReconX (Liu et al., 2024a)
fine-tunes video diffusion models for sparse-view novel view synthesis, enabling efficient reconstruction of
51
Figure 20: By training a feed-forward reconstruction model from a video generative model, Wonderland
Liang et al. (2024b) achieves remarkable 3D scene generation results.
3D scenes from limited input views.
With recent advancements in large reconstruction models (LRMs)
(Hong et al., 2023b; Zhang et al., 2024e), Wonderland (Liang et al., 2024b) demonstrates that by training a
feed-forward 3D reconstruction model from the latent space of a video generative model, remarkable scene
generation results can be achieved. Some examples are shown in Fig. 20.
5.4.2
Video diffusion for 4D generation
The incorporation of the additional time dimension in 4D generation significantly increases the complexity
of developing and training feed-forward networks. Video diffusion models, functioning as motion generators,
offer a promising solution by enabling the extraction of scene dynamics and motion information for 4D
generation.
Several approaches leverage video diffusion models to train dynamic Neural Radiance Fields (NeRFs) or
other 4D representations: MAV3D (Singer et al., 2023), 4D-fy (Bahmani et al., 2024c), Dream-in-4D (Zheng
et al., 2024b), Align-Your-Gaussians (Ling et al., 2024a), and Animate124 (Zhao et al., 2023e) apply Score
Distillation Sampling (SDS) loss along the temporal axis to train dynamic NeRFs using video diffusion
models. These methods enable the models to learn motion dynamics directly from video diffusion. Vidu4D
(Wang et al., 2024g) and STAG4D (Zeng et al., 2024b) propose robust reconstruction pipelines to reconstruct
dynamic Gaussian Surfels or Gaussian splatting with monocular videos generated by video diffusion models.
PLA4D (Miao et al., 2024) animates static 3D Gaussian splatting generated by 3D diffusion models, using
video diffusion predictions combined with a motion alignment model. TC4D (Bahmani et al., 2024a) uses
SDS loss to supervise dynamic NeRFs, incorporating a trajectory-conditioned video model that disentangles
global and local motion components, facilitating the generation of large-scale motions.
Dynamic motion knowledge can also be combined with spatial 3D knowledge from multi-view diffusion
models: EG4D (Sun et al., 2024) use a combination of SVD and SV3D models to directly generate multi-
view videos, which are then used to supervise a 4D Gaussian splatting. 4Real (Yu et al., 2024a) generates
4D videos by using video diffusion model to supervise fixed-viewpoint video and freeze-time video with SDS
loss. The use of deformable 3D GS ensures the temporal and view consistency. DreamGaussian4D (Ren
et al., 2023) employs a training-free video-to-video pipeline that uses pre-trained video diffusion models to
refine the dynamic textures of generated mesh sequences.
Several approaches aim to fine-tune video diffusion models into 4D-aware diffusion models, enabling the
unified learning of spatial and temporal information. Such a direct 4D generator could be further distilled
into a 4D scene representation for better temporal and view consistency or real-time rendering.
52
Figure 21: CAT4D (Wu et al., 2024e) trains a multi-view video diffusion model conditioned on a monocular
video, which enables 4D view synthesis from casually captured monocular videos.
Diffusion4D (LIANG et al., 2024) fine-tunes a video diffusion model conditioned on camera pose and times-
tamps to generate orbiting views of dynamic scenes, which are then used to optimize dynamic Gaussian
splatting.
4Diffusion (Zhang et al., 2024c) and Animate3D (Jiang et al., 2024a) train multi-view video
diffusion models capable of simultaneously generating videos from different viewpoints, and then train dy-
namic NeRFs with SDS loss. VideoMV (Zuo et al., 2024) trains a multi-view video diffusion model and a
feed-forward Gaussian splatting reconstruction model that directly predicts GS from sparse views generated
by the multi-view video diffusion model. Vivid-ZOO (Li et al., 2024a) reuses pre-trained 2D video and
multi-view diffusion models to bridge the gap toward 4D generation.
SV4D (Xie et al., 2024b) and CAT4D (Wu et al., 2024e) train multi-view video diffusion models conditioned
on monocular reference videos, enabling video-to-4D tasks, as shown in Fig. 21.
6
Conclusion and outlook
This survey presented a comprehensive analysis of diffusion-based video generation, tracing its evolution
from traditional GAN-based approaches to current state-of-the-art methodologies. Through our system-
atic examination, we have highlighted how diffusion models have addressed many historical challenges in
video generation while introducing new opportunities and considerations for future research. The field of
diffusion-based video generation has demonstrated remarkable progress in generating temporally consistent
and visually compelling videos.
Our analysis reveals several key insights: First, the transition from GAN-based methods to diffusion models
has significantly improved training stability and output quality, though computational efficiency remains
a crucial challenge. Second, the integration of diffusion models with various architectural innovations has
enhanced temporal coherence and motion consistency, particularly in long video generation.
Third, the
application of video generation methods has broadened considerably, from low-level vision tasks to complex
video understanding problems.
Despite these advances, several critical challenges remain:
• Computational Efficiency: Future research should focus on reducing the computational demands of
both training and inference, particularly for real-time applications.
• Long-Video Generation: While progress has been made in handling long video generation, main-
taining consistency and coherence in long videos remains challenging.
53
• Adhering Physical Rules: Ensuring generated videos adhere to physical laws and maintain realistic
object dynamics requires continued attention.
• Ethical Considerations: As these technologies become more powerful, developing robust frameworks
for addressing bias and preventing misuse becomes increasingly important.
Corresponding to these challenges, we anticipate several promising research directions: 1) the development
of more efficient architectures and training strategies to reduce computational requirements while maintain-
ing generation quality. 2) integration of physical priors and domain knowledge to improve the realism of
generated videos. 3) exploration of hybrid approaches that combine the strengths of diffusion models with
other generative frameworks. 4) investigation of scalable solutions for real-world applications, particularly
in domains such as virtual reality and autonomous systems.
As the field continues to evolve, the relations between diffusion-based video generation and related domains,
such as video understanding and 3D generation, will likely drive further innovations.
The community’s
focus on addressing both technical and ethical challenges will be crucial in realizing the full potential of this
technology while ensuring its responsible development and deployment.
References
Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov,
Daniel Cohen-Or, and Kfir Aberman. Dynamic concepts personalization from single videos, 2025. URL
https://arxiv.org/abs/2502.14844. 4.3
Luca Savant Aira, Antonio Montanaro, Emanuele Aiello, Diego Valsesia, and Enrico Magli. Motioncraft:
Physics-based zero-shot video generation. arXiv preprint arXiv:2405.13557, 2024. 4.1.6
Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying frame-
work for flows and diffusions. arXiv: 2303.08797, 2023. 2.2.4
Nuha Aldausari, Arcot Sowmya, Nadine Marcus, and Gelareh Mohammadi. Video generative adversarial
networks: a review. ACM Comput. Surv., 55(2):1–25, 2022. 1, 2.1.1
Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift:
Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477,
2023. 2.5.1
Pierfrancesco Ardino, Marco De Nadai, Bruno Lepri, Elisa Ricci, and Stephane Lathuiliere. Click to move:
Controlling video generation with sparse motion. In ICCV, pp. 14729–14738, 2021. 2.2.5
Md. Adnan Arefeen, Biplob Debnath, Md. Yusuf Sarwar Uddin, and Srimat Chakradhar. irag: An incre-
mental retrieval augmented generation system for videos. CoRR, abs/2404.12309, 2024. 5.2
Dawit Mureja Argaw, Fabian Caba Heilbron, Joon-Young Lee, Markus Woodson, and In So Kweon. The
anatomy of video editing: A dataset and benchmark suite for ai-assisted video editing. In Shai Avidan,
Gabriel Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision –
ECCV 2022, volume 13668, pp. 201–218, 2022. 3.1
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. Vivit: A
video vision transformer. In ICCV, pp. 6836–6846, 2021. 2.5.2
Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon
Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, and David B. Lindell. Tc4d: Trajectory-
conditioned text-to-4d generation. In ECCV, pp. 53–72, 2024a. 5.4.2
Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea
Tagliasacchi, David B Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera con-
trol in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024b. 4.1.3, 4.6.3
54
Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka,
Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B. Lindell. 4d-fy: Text-to-4d genera-
tion using hybrid score distillation sampling. In CVPR, pp. 7996–8006, 2024c. 5.4.2
Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael
Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.
Vd3d: Taming
large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024d. 4.1.3,
4.6.3
Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. Uniedit: A
unified tuning-free framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185,
2024. 4.1.5
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image
encoder for end-to-end retrieval. In ICCV, 2021. 1, 3.1, 3.4, 4.6.1
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A
vit backbone for diffusion models. In CVPR, pp. 22669–22679, 2023a. 2.5.4
Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and
Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In ICML, pp. 1692–1717,
2023b. 2.5.3
Michael J Black and Padmanabhan Anandan. A framework for the robust estimation of optical flow. In
ICCV, pp. 231–236, 1993. 2.4.2
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,
Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video
diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a. 3.2, 4.1.1, 4.6.1
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and
Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR,
pp. 22563–22575, 2023b. 2.1.3, 2.4.1, 2.4.4, 2.5.3
Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, and Hanspeter Pfister.
Blind video temporal consistency. TOG, 34(6), 2015. 3.3
Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation
with multimodal transformers. In CVPR, pp. 4975–4985, 2022. 5.1
Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu,
Alexei A. Efros, and Tero Karras. Generating long videos of dynamic scenes, 2022. 4.5
Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing
instructions. In CVPR, pp. 18392–18402, 2023. 4.1.1
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
models are few-shot learners. In NeurIPS, 2020. 5.2
Andrés Bruhn, Joachim Weickert, and Christoph Schnörr. Lucas/kanade meets horn/schunck: Combining
local and global optic flow methods. Int. J. Comput. Vis., 61(3):211–231, 2005. 2.4.2
Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong
Deng, Lingxiao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motion-controllable video diffusion models
using real-time warped noise. In CVPR, 2025. 3, 2.4.2
55
Shuqiang Cao, Bairui Wang, Wei Zhang, and Lin Ma. Visual consensus modeling for video-text retrieval. In
AAAI, pp. 167–175, 2022. 5.1, 5.2
Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. A comprehensive
survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt, 2023. (document), 1
Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.
In CVPR, 2017. 3.1
Duygu Ceylan, Chun-Hao P. Huang, and Niloy J. Mitra. Pix2video: Video editing using image diffusion. In
ICCV, pp. 23149–23160, 2023. 4.1.5
Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion
video editing. In ICCV, pp. 23040–23050, 2023. 1, 4.1.5, 4.4
Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao
Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting
with identity-aware diffusion. In ICML, 2023a. 4.1.5
Pascal Chang, Jingwei Tang, Markus Gross, and Vinicius C Azevedo. How i warped your noise: a temporally-
correlated noise prior for diffusion models. In ICLR, 2024. 2.4.2, 4.4
Shao-Yu Chang, Hwann-Tzong Chen, and Tyng-Luh Liu. Diffusionatlas: High-fidelity consistent diffusion
video editing. arXiv preprint arXiv:2312.03772, 2023b. 4.1.5
Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston Hsu. Free-form video inpainting with 3d gated
convolution and temporal patchgan. In ICCV, pp. 9066–9075, 2019. 4.2.2
Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel,
Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video
data. ACM Transactions on Graphics (TOG), 43(6):1–11, 2024. 4.3
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang
Liu, Qifeng Chen, Xintao Wang, et al.
Videocrafter1: Open diffusion models for high-quality video
generation. arXiv preprint arXiv:2310.19512, 2023a. 2.4.4, 2.5.3, 3.4, 4.1.1
Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.
Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pp. 7310–
7320, 2024a. 2.4.4, 3.1, 3.4
Jiahui Chen et al.
Internvid: Learning text-to-video generation from web-scale video-text data.
arXiv
preprint arXiv:2303.11123, 2023b. 4.6.1
Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping
Luo, Huchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training of diffusion transformer for photorealistic
text-to-image synthesis. In ICLR, 2023c. 2.5.3
Long Chen and Jing Sun. Videogen: Generalized text-to-video generation. arXiv preprint arXiv:2306.05012,
2023. 2.4.4
Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao
Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In
CVPR, pp. 6441–6451, 2024b. 2.5.2, 5.2
Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972,
2023. 2.4.3
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-Wei Chao, Byung Eun
Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Cap-
tioning 70m videos with multiple cross-modality teachers. In CVPR, pp. 13320–13331, 2024c. 1, 3.1
56
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir
Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization
in video generation. arXiv preprint arXiv:2501.06187, 2025. 4.3
Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua
Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and
prediction. In ICLR, 2024d. 2.4.4, 2.5.3, 3.4, 4.4
Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, and Tao Mei. Learning
spatial adaptation and temporal coherence in diffusion models for video super-resolution. In CVPR, pp.
9232–9241, 2024e. 4.2.4
Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion models are
effective 3d generators, 2024f. 4.6.1, 5.4.1
Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent video-to-video transfer using synthetic dataset. arXiv
preprint arXiv:2311.00213, 2023. 4.1.5
Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen.
Improving video-text retrieval by
multi-stream corpus alignment and dual softmax loss. CoRR, abs/2109.04290, 2021. 5.2
Xiaowei Chi, Rongyu Zhang, Zhengkai Jiang, Yijiang Liu, Yatian Wang, Xingqun Qi, Wenhan Luo, Peng
Gao, Shanghang Zhang, Qifeng Liu, and Yike Guo. M2chat: Empowering vlm for multimodal llm inter-
leaved text-image generation, 2024. 5.1
Gene Chou, Kai Zhang, Sai Bi, Hao Tan, Zexiang Xu, Fujun Luan, Bharath Hariharan, and Noah Snavely.
Generating 3d-consistent videos from unposed internet photos, 2024. 4.6.1, 4.6.2
Ernie Chu, Shuo-Yen Lin, and Jun-Cheng Chen. Video controlnet: Towards temporally consistent synthetic-
to-real video translation using conditional image diffusion models. arXiv preprint arXiv:2305.19193, 2023.
4.1.5
Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifold-
constrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. 2.3.1
Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv
preprint arXiv:1907.06571, 2019. 1, 2.1.1
Paul Couairon, Clément Rambour, Jean-Emmanuel Haugeard, and Nicolas Thome. Videdit: Zero-shot and
spatially aware text-driven video editing. TMLR, 2023. 4.1.5
Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie,
and Yang Liu.
Teachtext: Crossmodal generalized distillation for text-video retrieval.
In ICCV, pp.
11563–11573, 2021. 5.2
Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Fine-grained
open domain image animation with motion guidance, 2023. 4.1.1
Duolikun Danier, Fan Zhang, and David Bull.
Ldmvfi: Video frame interpolation with latent diffusion
models. In AAAI, pp. 1472–1480, 2024. 4.2.3
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. In NeurIPS, 2022. 3.2
Aram Davtyan and Paolo Favaro. Controllable video generation through global and local motion dynamics.
In ECCV, pp. 68–84, 2022. 1
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,
Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects.
arXiv preprint arXiv:2212.08051, 2022. 4.6.1
57
Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan,
Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Von-
drick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: A universe of
10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 4.6.1
Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas Guibas, Noah Snavely, and Gordon Wetzstein.
Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion. In SIG-
GRAPH, pp. 1–11, 2024. 4.1.2, 4.4
Yitong Deng, Winnie Lin, Lingxiao Li, Dmitriy Smirnov, Ryan D Burgert, Ning Yu, Vincent Dedun, and
Mohammad H. Taghavi. Infinite-resolution integral noise warping for diffusion models. In ICLR, 2025.
2.4.2
Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-
style video editing. arXiv preprint arXiv:2312.02216, 2023. 4.1.5
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In NAACL, pp. 4171–4186, 2019. 2.5.4, 5.2
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, pp.
8780–8794, 2021. 2.3.1, 2.3.1, 1, 2, 2.4.3, 2.5.3
Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. Dual encoding
for zero-example video retrieval. In CVPR, pp. 9346–9355, 2019. 5.2
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick
Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional
networks. In ICCV, pp. 2758–2766, 2015. 2.4.2
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. In ICLR, 2020. 2.5.1
Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,
Thomas B. McHugh, and Vincent Vanhoucke.
Google scanned objects: A high-quality dataset of 3d
scanned household items, 2022. 4.6.1
Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis,
and Maja Pantic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8498–8507, 2024. 4.1.4, 4.3
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General
language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.
2.5.4
Titir Dutta, Anurag Singh, and Soma Biswas. Styleguide: Zero-shot sketch-based image retrieval using
style-guided image generation. IEEE Trans. Multim., 23:2833–2842, 2021. 5.2
Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. Mdmmt: Multidomain
multimodal transformer for video retrieval. In CVPR Workshops, pp. 3354–3363, 2021. 5.2
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.
In CVPR, pp. 12873–12883, 2021. 2.5.3
Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.
Structure and content-guided video synthesis with diffusion models. In ICCV, pp. 7346–7356, 2023. 2.4.1,
4.1.5
58
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,
Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution
image synthesis. In ICML, 2024. 2.5.3, 2.5.4
Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan. Aigcbench: Comprehensive evaluation of image-
to-video content generated by ai, 2024. URL https://arxiv.org/abs/2401.01651. 3.1, 3.3
Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and
Baining Guo. Ccedit: Creative and controllable video editing via diffusion models, 2024a. 2.1.3
Wanquan Feng, Jiawei Liu, Pengqi Tu, Tianhao Qi, Mingzhen Sun, Tianxiang Ma, Songtao Zhao, Siyu Zhou,
and Qian He. I2vcontrol-camera: Precise video camera control with adjustable motion strength, 2024b.
4.6.3
Weixi Feng, Jiachen Li, Michael Saxon, Tsu-jui Fu, Wenhu Chen, and William Yang Wang.
Tc-bench:
Benchmarking temporal compositionality in text-to-video and image-to-video generation, June 2024c. 3.3
Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene
generation, 2023. 4.4
Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, and Sean Bell.
Tell me what happened: Unifying text-guided video completion via multimodal masked video generation.
In CVPR, pp. 10681–10692, 2023. 12, 4.2.3
Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang,
and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. arXiv
preprint arXiv:2412.07759, 2024. 9, 4.1.3
Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry
Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint
arXiv:2412.02617, 2024a. 2.2.5
Hiroki Furuta et al. Learning dynamic object interactions in video generation through ai feedback. arXiv
preprint, 2024b. 2.2.5
Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video
retrieval. In ECCV, pp. 214–229, 2020. 5.1, 5.2
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv
preprint arXiv:2208.01618, 2022. 2.2.8
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training
for vision-and-language representation learning. In NeurIPS, 2020. 5.2
Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao. Vid-gpt: Introducing gpt-style
autoregressive generation in video diffusion models, 2024. 4.5
Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. Clip2tv: An empirical
study on transformer-based methods for video-text retrieval. CoRR, abs/2111.05610, 2021. 5.2
Ruiqi Gao∗, Aleksander Holynski∗, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P.
Srinivasan, Jonathan T. Barron, and Ben Poole∗. Cat3d: Create anything in 3d with multi-view diffusion
models. arXiv, 2024. 4.6.2, 4.6.3
Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi
Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In ECCV, pp.
102–118, 2022. 2.1.2, 4.5
59
Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin
Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion
models. In ICCV, pp. 22930–22941, 2023. 2.4.1, 4.4
Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-
Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, and
Deqing Sun. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint
arXiv:2412.02700, 2024a. 3.2, 4.6.3
Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-
Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video
generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024b. 4.1.2
Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.
Tokenflow: Consistent diffusion features for
consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 4.1.5, 4.4
Litong Gong, Yiran Zhu, Weijie Li, Xiaoyang Kang, Biao Wang, Tiezheng Ge, and Bo Zheng. Atomovideo:
High fidelity image-to-video generation, 2024. 4.1.1
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 1
Satya Krishna Gorti, Noël Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and
Guangwei Yu.
X-pool: Cross-modal language-video attention for text-video retrieval.
In CVPR, pp.
4996–5005, 2022. 5.2
Dylan Green, William Harvey, Saeid Naderiparizi, Matthew Niedoba, Yunpeng Liu, Xiaoxuan Liang,
Jonathan Lavington, Ke Zhang, Vasileios Lioutas, Setareh Dabiri, et al. Semantically consistent video
inpainting with conditional diffusion models. arXiv preprint arXiv:2405.00251, 2024. 4.2.2
Xianfan Gu, Chuan Wen, Weirui Ye, Jiaming Song, and Yang Gao.
Seer: Language instructed video
prediction with latent diffusion models. In ICLR, 2024a. 4.1.1
Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Jun-
hao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with
interactive semantic point correspondence. In CVPR, pp. 7621–7630, 2024b. 4.1.5
Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin,
Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3d-aware video diffusion for versatile video
generation control, 2025. 4.6.2, 4.6.3
Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong
Zhang, Xintao Wang, Qifeng Chen, et al.
Make a cheap scaling: A self-cascade diffusion model for
higher-resolution adaptation. In ECCV, pp. 39–55, 2025. 4.2.4
Qiushan Guo, Sifei Liu, Yizhou Yu, and Ping Luo. Rethinking the noise schedule of diffusion-based generative
models. N/A, 2023a. 2.4.3
Xiefan Guo, Jinlin Liu, Miaomiao Cui, and Di Huang. I4vgen: Image as stepping stone for text-to-video
generation. arXiv preprint arXiv:2406.02230, 2024a. 2.4.4
Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming
Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma. I2v-adapter: A general image-to-video adapter
for diffusion models, 2023b. 4.1.1
Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua
Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725, 2023c. 2.4.4, 2.5.3
60
Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse
controls to text-to-video diffusion models. In ECCV, pp. 330–348, 2024b. 4.1.1, 4.1.2
Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José
Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023.
2.4.4, 3.4
Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran
Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitter-
man, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion, 2024. 2.4.4, 3.4
Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-
nerf2nerf: Editing 3d scenes with instructions. In ICCV, pp. 19683–19693, 2023. 5.4.1
William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Dietrich Weilbach, and Frank Wood. Flexible
diffusion modeling of long videos. In NeurIPS, 2022. 4.5
Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl:
Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024a. 4.1.3, 4.6.3
Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang,
and Ziwei Liu.
Venhancer: Generative space-time enhancement for video generation.
arXiv preprint
arXiv:2407.07667, 2024b. 4.2.4, 4.2.5
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, pp. 770–778, 2016. 5.1
Mingming He, Pascal Clausen, Ahmet Levent Taşel, Li Ma, Oliver Pilarski, Wenqi Xian, Laszlo Rikker,
Xueming Yu, Ryan Burgert, Ning Yu, and Paul Debevec. Diffrelight: Diffusion-based facial performance
relighting. In SIGGRAPH Asia, pp. 1–12, 2024c. 4.1.1
Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator:
Zero-shot identity-preserving human video generation, 2024d. 4.1.1
Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for
high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. 2.5.1
Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao
Wang, Chao Weng, Ying Shan, and Qifeng Chen. Animate-a-story: Storytelling with retrieval-augmented
video generation. CoRR, abs/2307.06940, 2023a. 2.2.8, 2.4.4, 5.2
Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran
He, Qifeng Chen, and Ying Shan.
Scalecrafter: Tuning-free higher-resolution visual generation with
diffusion models. In ICLR, 2023b. 2.4.1
Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-
scale video benchmark for human activity understanding. In CVPR, pp. 961–970, 2015. 3.1
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing
moments in video with natural language. In ICCV, pp. 5804–5813, 2017. 3.1
Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan,
Zhangyang Wang, Shant Navasardyan, and Humphrey Shi.
Streamingt2v: Consistent, dynamic, and
extendable long video generation from text, 2024. 4.4
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, pp. 6626–6637,
2017. 3.3
61
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.
2.3.2
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, pp.
6840–6851, 2020. 1, 2.2.1, 2, 2.4.3, 2.5.3
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al.
Imagen video: High definition video
generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. 2.1.3, 2.4.4, 2.5.1, 2.5.3, 2.5.4
Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.
Cascaded diffusion models for high fidelity image generation. JMLR, 23(47):1–33, 2022b. 2.1.3, 2.5.3
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.
Video diffusion models. arXiv:2204.03458, 2022c. 1, 2.1.3, 2.4.1, 2.5.1
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997.
5.1
Fa-Ting Hong and Dan Xu. Implicit identity representation conditioned memory compensation network for
talking head video generation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 23062–23072, 2023. 4.3
Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu.
Depth-aware generative adversarial network for
talking head video generation. In CVPR, pp. 3397–3406, 2022. 1
Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili
Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and
Jürgen Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative framework, 2024a.
2.4.4
Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for
text-to-video generation via transformers. In ICLR, 2023a. 2.1.2, 2.4.4, 2.5.2, 3.2, 4.5
Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang,
Junhui Ji, Zhao Xue, Lei Zhao, Zhuoyi Yang, Xiaotao Gu, Xiaohan Zhang, Guanyu Feng, Da Yin, Zihan
Wang, Ji Qi, Xixuan Song, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Yuxiao Dong, and Jie Tang.
Cogvlm2: Visual language models for image and video understanding. arxiv preprint arXiv:2408.16500,
2024b. 3.2
Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung
Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d, 2023b. 5.4.1
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.
simple diffusion: End-to-end diffusion for high
resolution images. In ICML, pp. 13213–13232, 2023. 2.4.3
J J Hopfield. Neural networks and physical systems with emergent collective computational abilities. PNAS,
79(8):2554–2558, 1982. 5.1
Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video
prediction and infilling. arXiv preprint arXiv:2206.07696, 2022. 4.2.3
Berthold KP Horn and Brian G Schunck. Determining optical flow. Artif. Intell., 17(1-3):185–203, 1981.
2.4.2
Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation,
2024. 4.6.4
Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, and Shubham Tulsiani. Mvd-fusion: Single-view 3d via depth-
consistent multi-view generation. In CVPR, 2024. 4.6.2
62
Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In
CVPR, pp. 8153–8163, 2024. 4.1.5
Miao Hu, Xianzhuo Luo, Jiawen Chen, Young Choon Lee, Yipeng Zhou, and Di Wu. Virtual reality: A
survey of enabling technologies and its applications in iot. CoRR, abs/2103.06472, 2021. 1
Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, and Shuchang Zhou. A dynamic multi-scale voxel flow
network for video prediction. In CVPR, pp. 6121–6131, 2023. 2.4.2
Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided video-to-video translation framework by using
diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. 4.1.5
Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang. Free-bloom: Zero-shot text-
to-video generator with llm director and ldm animator. In NeurIPS, 2023a. 2.2.7
Jiahui Huang, Yew Ken Chia, Samson Yu, Kevin Yee, Dennis Küster, Eva G Krumhuber, Dorien Herremans,
and Gemma Roig. Single image video prediction with auto-regressive gans. Sensors, 22(9):3533, 2022a.
2.1.2
Xinyue Huang et al. Lavie: Latent video encoding for diffusion models. arXiv preprint arXiv:2309.04512,
2023b. 1, 2.5.3, 3.4
Yonglong Huang, Cheng Yu, Nannan Li, Fuqin Deng, Ruiquan Ge, and Changmiao Wang. Make an image
move: Few-shot based video generation guided by clip. In ICPR, pp. 146–161, 2025. 2.2.6
Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding
Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained
diffusion. arXiv preprint arXiv:2312.06725, 2023c. 4.6.2
Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and
Hongsheng Li. Flowformer: A transformer architecture for optical flow. arXiv preprint arXiv:2203.16194,
2022b. 2.4.2
Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu,
Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and
Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024a. 3.1,
3.3
Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit,
Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin,
Yu Qiao, and Ziwei Liu. Vbench++: Comprehensive and versatile benchmark suite for video generative
models, 2024b. 3.3
Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Liteflownet: A lightweight convolutional neural network
for optical flow estimation. In CVPR, pp. 8981–8989, 2018. 2.4.2
Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. A lightweight optical flow cnn—revisiting data fidelity
and regularization. IEEE Trans. Pattern Anal. Mach. Intell., 43(8):2555–2569, 2020. 2.4.2
Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet
2.0: Evolution of optical flow estimation with deep networks. In CVPR, pp. 2462–2470, 2017. 2.4.2
Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. Video interpolation with
diffusion models. In CVPR, pp. 7341–7351, 2024a. 4.2.3
Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via
masked-diffusion. In CVPR, pp. 8079–8088, 2024b. 2.2.7
Y. Jang, Y. Song, Y. Yu, Y. Kim, and G. Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question
answering. In CVPR, pp. 1359–1367, 2017. 5.3
63
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam
Lim. Visual prompt tuning. In ECCV, 2022. 3.2
Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any
3d model with multi-view video diffusion. In NeurIPS, 2024a. 5.4.2
Yifan Jiang, Kevin Yang, et al. Videocomposer: Compositional text-to-video generation with latent diffusion
models. arXiv preprint arXiv:2302.01329, 2023a. 2.4.4, 4.1.5
Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2performer:
Text-driven human video generation. In ICCV, pp. 22690–22700, 2023b. 2.5.2
Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei
Liu. Videobooth: Diffusion-based video generation with image prompts. In CVPR, pp. 6689–6700, 2024b.
4.1.1, 4.1.1, 4.4
Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong
Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint
arXiv:2410.05954, 2024. 2.1.4
Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu,
and Ying Shan. Miradata: A large-scale video dataset with long durations and structured captions. In
NeurIPS, 2024. 1, 3.1
Philip Torr Junlin Han, Filippos Kokkinos. Vfusion3d: Learning scalable 3d generative models from video
diffusion models. arXiv preprint arXiv:2403.12034, 2024. 4.6.1, 5.4.1
Nal Kalchbrenner, Aäron Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray
Kavukcuoglu. Video pixel networks. In ICML, pp. 1771–1779, 2017. 2.1.2
Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler,
Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski. Spad: Spatially aware multi-view diffusers. In
CVPR, pp. 10026–10038, 2024. 4.6.2
Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M Rehg, and Pinar Yanardag. Rave: Randomized
noise shuffling for fast and consistent video editing with diffusion models. In CVPR, pp. 6507–6516, 2024.
4.1.5
Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose:
Fashion image-to-video synthesis via stable diffusion. In ICCV, pp. 22623–22633, 2023. 4.1.5
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
improving the image quality of stylegan. In CVPR, pp. 8110–8119, 2020. 2.1.1
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based
generative models. In NeurIPS, pp. 26565–26577, 2022. 2.2.3, 2.2.3, 2.4.3
Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing.
TOG, 40(6):1–12, 2021. 4.1.5
Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler.
Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 4.1.1
Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for
real-time radiance field rendering. TOG, 42(4), 2023. 5.4
Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video
generators. In ICCV, pp. 15954–15964, 2023. 2.2.7, 3.1, 4.1.5
64
Taekyung Ki and Dongchan Min. Stylelipsync: Style-based personalized lip-sync video generation. In ICCV,
pp. 22784–22793, 2023. 2.1.1
Gyeongman Kim, Hajin Shim, Hyunsu Kim, Yunjey Choi, Junho Kim, and Eunho Yang. Diffusion video
autoencoders: Toward temporally consistent face video editing via disentangled video encoding. In CVPR,
pp. 6091–6100, 2023a. 4.1.5
Pum Jun Kim, Seojun Kim, and Jaejun Yoo. Stream: Spatio-temporal evaluation and analysis metric for
video generative models. In ICLR, 2023b. 3.3
Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. Drivegan: Towards a controllable
high-quality neural simulation. In CVPR, pp. 5820–5829, 2021. 2.4.4
Diederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2.5.3
Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam,
Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model for zero-shot
video generation. arXiv preprint arXiv:2312.14125, 2023. 2.5.3
Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jian-
wei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin
Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao
Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng,
Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan
Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu,
Jie Jiang, and Caesar Zhong. Hunyuanvideo: A systematic framework for large video generative models,
2025. 2.4.4, 2.5.3, 2.5.4, 3.4
Yu Kong, Zhiqiang Tao, and Yun Fu. Deep sequential context networks for action prediction. In CVPR, pp.
3662–3670, 2017. 5.1
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. In NeurIPS, 2012. 5.1
Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: A tuning-free framework for
any video-to-video editing tasks. TMLR, 2024. 2.2.7, 4.1.5
Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein.
Collaborative video diffusion: Consistent multi-video generation with camera control.
arXiv preprint
arXiv:2405.17414, 2024. 4.1.3, 4.6.1, 4.6.3
Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi. Vivid-1-to-3:
Novel view synthesis with video diffusion models. In CVPR, pp. 6775–6785, 2024. 1, 4.6.4
PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 2.4.4, 2.5.3, 2.5.4
Black Forest Labs. Flux, 2024. 2.5.4
LAION. Laion-coco. https://laion.ai/blog/laion-coco/, 2023. 1
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vi-
mal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech
generation at scale. arXiv preprint arXiv:2306.15687, 2023. 2.2.4
Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs: Context-aware controllable video synthesis.
Advances in Neural Information Processing Systems, 34:14042–14055, 2021. 4.1.4
Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song.
Acav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. In
ICCV, pp. 10254–10264, 2021. 3.1
65
Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, and Jia-Bin Huang. Shape-aware text-
driven layered video editing. In CVPR, pp. 14317–14326, 2023. 4.1.5
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more:
Clipbert for video-and-language learning via sparse sampling. In CVPR, pp. 7331–7341, 2021. 5.1, 5.2
Wentao Lei, Jinting Wang, Fengji Ma, Guanjie Huang, and Li Liu. A comprehensive survey on human video
generation: Challenges, methods, and insights, 2024a. (document), 1
Wentao Lei, Jinting Wang, Fengji Ma, Guanjie Huang, and Li Liu. A comprehensive survey on human video
generation: Challenges, methods, and insights, 2024b. (document), 1
Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem. Vivid-
zoo: Multi-view video generation with diffusion model. In NeurIPS, 2024a. 4.6.2, 5.4.2
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three
insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245,
2024b. 2.5.3
Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven C. H. Hoi.
Align and prompt:
Video-and-language pre-training with entity prompts. In CVPR, pp. 4943–4953, 2022. 5.1
Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun MA, and Chunyuan Li. LLaVA-
neXT-interleave: Tackling multi-image, video, and 3d in large multimodal models. In The Thirteenth
International Conference on Learning Representations, 2025. 3.2
Hengjia Li, Haonan Qiu, Shiwei Zhang, Xiang Wang, Yujie Wei, Zekun Li, Yingya Zhang, Boxi Wu, and
Deng Cai. Personalvideo: High id-fidelity video customization without dynamic and semantic degradation.
arXiv preprint arXiv:2411.17048, 2024c. 4.3
Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-
turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv
preprint arXiv:2405.18750, 2024d. 2.2.5
Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang
Wang. T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and condi-
tional guidance design. arXiv preprint arXiv:2410.05677, 2024e. 2.2.5
Maomao Li, Yu Li, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, and Dong Xu. A video is worth 256
bases: Spatial-temporal expectation-maximization inversion for zero-shot video editing. In CVPR, pp.
7528–7537, 2024f. 4.1.5
Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang. Decap: Decoding clip latents for zero-shot captioning via
text-only training. In ICLR, 2023. 5.1
Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging for zero-shot
video editing. In CVPR, pp. 7486–7495, 2024g. 4.1.5, 4.4
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,
Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.
Oscar: Object-semantics aligned pre-training for
vision-language tasks. In ECCV, pp. 121–137, 2020. 5.2
Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, and Ming-Ming Cheng. Sora generates
videos with stunning geometrical consistency. arXiv preprint arXiv: 2402.17403, 2024h. 4.6
Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander Holynski. Generative image dynamics. In CVPR,
2024i. 4.1.1
Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao
Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: A powerful multi-resolution diffusion transformer
with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024j. 2.5.3, 2.5.4, 3.2
66
Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. Llm-grounded video diffusion models. In
ICLR, 2024. 2.3.1
Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang,
Peizhao Zhang, Peter Vajda, et al. Flowvid: Taming imperfect optical flows for consistent video-to-video
synthesis. In CVPR, pp. 8207–8216, 2024a. 4.1.5
Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos
Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from a single image. arXiv
preprint arXiv:2412.12091, 2024b. 4.6.2, 4.6.3, 20, 5.4.1
HANWEN LIANG, Yuyang Yin, Dejia Xu, hanxue liang, Zhangyang Wang, Konstantinos N Plataniotis, Yao
Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion
models. In NeurIPS, 2024. 4.6.1, 5.4.2
Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, and Rakesh Ranjan.
Movideo:
Motion-aware video generation with diffusion model. In ECCV, pp. 56–74, 2025. 2.5.1
Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity and
temporally coherent video editing. arXiv preprint arXiv:2308.14749, 2023. 4.1.5
Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the
scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025.
4.1.4
Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal.
Videodirectorgpt: Consistent multi-scene video
generation via llm-guided planning, 2024a. 2.4.4
Zongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai,
Liangchen Song, Bowen Zhang, et al. Stiv: Scalable text and image conditioned video generation. arXiv
preprint arXiv:2412.07730, 2024b. 2.4.4
Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians:
Text-to-4d with dynamic 3d gaussians and composed diffusion models, 2024a. 5.4.2
Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu,
Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong,
Gang Hua, Tianyi Zhang, Bedrich Benes, and Aniket Bera. Dl3dv-10k: A large-scale scene dataset for
deep learning-based 3d vision. In CVPR, pp. 22160–22169, 2024b. 4.6.1
Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang,
and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint
arXiv:2406.05338, 2024c. 4.1.5
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
Flow matching for
generative modeling. In iclr, 2023. 2.2.4
Alexander Liu, SouYoung Jin, Cheng-I Lai, Andrew Rouditchenko, Aude Oliva, and James Glass. Cross-
modal discrete representation learning. In ACL, pp. 3013–3035, 2022a. 5.2
Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi
Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model, 2024a. 5.4.1
Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, and Yueqi Duan. Make-your-3d: Fast and consis-
tent subject-driven 3d content generation. In ECCV, pp. 389–406, 2024b. 4.6.2
Hanyuan Liu, Minshan Xie, Jinbo Xing, Chengze Li, and Tien-Tsin Wong. Video colorization with pre-
trained text-to-image diffusion models. arXiv preprint arXiv:2306.01732, 2023a. 4.1.5
67
Haogeng Liu, Tao Wang, Jie Cao, Ran He, and Jianhua Tao. Boosting fast and high-quality speech synthesis
with linear diffusion. arXiv preprint arXiv:2306.05708, 2023b. 2.2.4
Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang. Spatio-temporal lstm with trust gates for 3d human
action recognition. In ECCV, pp. 816–833, 2016. 5.1
Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, and Alex C. Kot. Skeleton-based human action
recognition with global context-aware attention lstm networks. IEEE Trans. Image Process., 27(4):1586–
1599, 2018. 5.1
Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded
image-to-video generation. In European Conference on Computer Vision, pp. 360–378. Springer, 2024c.
4.1.6
Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, and Kai Yu.
Anitalker: Ani-
mate vivid and diverse talking faces through identity-decoupled facial motion encoding. arXiv preprint
arXiv:2405.03121, 2024d. 4.1.4, 4.1.5, 4.3
Tianyi Liu, Kejun Wu, Yi Wang, Wenyang Liu, Kim-Hui Yap, and Lap-Pui Chau. Bitstream-corrupted
video recovery: A novel benchmark dataset and method. In NeurIPS, 2023c. 3.1
Xian Liu, Qianyi Wu, Hang Zhou, Yuanqi Du, Wayne Wu, Dahua Lin, and Ziwei Liu. Audio-driven co-speech
gesture video generation. Advances in Neural Information Processing Systems, 35:21386–21399, 2022b.
4.1.4, 4.3
Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with
rectified flow. In ICLR, 2022c. 2.2.4
Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality
diffusion-based text-to-image generation. In ICLR, 2023d. 2.2.4
Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi
Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of
large vision models. arXiv preprint arXiv:2402.17177, 2024e. 2.5.3
Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: A
benchmark for fine-grained evaluation of open-domain text-to-video generation. In NeurIPS, 2023e. 3.1,
3.3
Yuqi Liu, Pengfei Xiong, Luhui Xu, Shengming Cao, and Qin Jin.
Ts2-net: Token shift and selection
transformer for text-video retrieval. In ECCV, pp. 319–335, 2022d. 5.2
Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang,
Ding Liang, and Wanli Ouyang. Unidream: Unifying diffusion priors for relightable text-to-3d generation.
In ECCV, pp. 74–91, 2024f. 4.6.2
Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-
purpose video diffusion transformers via mask modeling. In ICLR, 2024a. 2.5.2, 4.1.1
Shilin Lu et al. Vine: Video editing with generative feedback priors. arXiv preprint, 2024b. 2.2.5
Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible
vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024c. 2.5.3, 2.5.4
Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint:
Inpainting using denoising diffusion probabilistic models. In CVPR, pp. 11461–11471, 2022. 4.2.2
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical
study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293–304, 2022. 5.2
68
Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, and
Shifeng Chen. Gpt4motion: Scripting physical motions in text-to-video generation via blender-oriented
gpt planning. In CVPR, pp. 1430–1440, 2024. 2.2.7
Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen
Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei
Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang,
Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu,
Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng
Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong,
Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo,
Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li,
Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun,
Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang,
Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai,
Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu
Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao,
Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum,
and Daxin Jiang. Step-video-t2v technical report: The practice, challenges, and future of video foundation
model, 2025. 2.4.4, 2.5.3, 2.5.4
Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie.
Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv
preprint arXiv:2401.08740, 2024a. 2.4.4, 2.5.2, 2.5.3, 2.5.4
Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Cinemo:
Consistent and controllable image animation with motion diffusion models, 2024b. 4.1.1, 4.4
Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao.
Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024c. 2.4.1,
2.5.3, 2.5.4
Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng. Dreamtalk: When
expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767,
2023. 4.1.4, 4.3
Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-
grained contrastive learning for video-text retrieval. In ACM MM, pp. 638–647, 2022. 5.2
Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your
pose: Pose-guided text-to-video generation using pose-free videos. In AAAI, pp. 4117–4125, 2024d. 4.1.5
Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer,
and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368,
2024e. 2.2.7
Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and
Filippos Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation,
2024. 4.6.1, 5.4.1
Andrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren, and Helge Ritter. Video diffusion
models: A survey, 2024. (document), 1
Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag,
Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers
for text-to-video synthesis. In CVPR, pp. 7038–7048, 2024. 2.4.4
Qiaowei Miao, JinSheng Quan, Kehan Li, and Yawei Luo.
Pla4d: Pixel-level alignments for text-to-4d
gaussian splatting, 2024. 5.4.2
69
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV,
pp. 2630–2640, 2019. 3.1
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-
to-end learning of visual representations from uncurated instructional videos. In ECCV, pp. 9876–9886,
2020. 5.2
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren
Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM., 65(1):99–106,
2021. 5.4
Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake
a video with motion and content control. arXiv preprint arXiv:2405.13865, 2024. 4.1.5
Urwa Muaz, Wondong Jang, Rohun Tripathi, Santhosh Mani, Wenbin Ouyang, Ravi Teja Gadde, Baris
Gecer, Sergio Elizondo, Reza Madad, and Naveen Nair. Sidgan: High-resolution dubbed video generation
via shift-invariant learning. In ICCV, pp. 7833–7842, 2023. 2.1.1, 5.2
Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia
Schmid.
Learning audio-video modalities from image captions.
In Shai Avidan, Gabriel Brostow,
Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision – ECCV 2022,
volume 13674, pp. 407–426, 2022. ISBN 978-3-031-19780-2 978-3-031-19781-9. 3.1
Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David B Lindell. Sg-i2v:
Self-guided trajectory control in image-to-video generation. arXiv preprint arXiv:2411.04989, 2024. 4.1.2
Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and
Ying Tai. Openvid-1m: A large-scale high-quality dataset for text-to-video generation. arXiv preprint
arXiv:2407.02371, 2024. 4.6.1
Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional image-to-video
generation with latent flow diffusion models. In CVPR, pp. 18444–18455, 2023. 4.1.1, 4.4
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided
diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2.5.4
Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML,
pp. 8162–8171, 2021. 1, 2.2.1, 2.4.3
Yeongtak Oh, Jooyoung Choi, Yongsung Kim, Minjun Park, Chaehun Shin, and Sungroh Yoon. Control-
dreamer: Blending geometry and style in text-to-3d, 2023. 4.6.2
Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng
Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. In
CVPR, pp. 8089–8099, 2024a. 4.1.5
Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2vedit: First-frame-guided video editing
via image-to-video diffusion models. arXiv preprint arXiv:2405.16537, 2024b. 4.1.5
Yichen Ouyang, jianhao Yuan, Hao Zhao, Gaoang Wang, and Bo zhao. Flexifilm: Long video generation
with flexible conditions, 2024c. 4.4
Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-
to-video transfer learning for action recognition. arXiv preprint arXiv:2206.13559, 2022. 3.2
Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang, Yi Wang, Yu Qiao, and Hongsheng Li.
Retrieving-to-answer: Zero-shot video question answering with frozen large language models. In ICCV,
pp. 272–283, 2023. 5.1
70
Youxin Pang, Yong Zhang, Weize Quan, Yanbo Fan, Xiaodong Cun, Ying Shan, and Dong-ming Yan. Dpe:
Disentanglement of pose and expression for general video portrait editing. In CVPR, pp. 427–436, 2023.
4.1.5
Jae Sung Park, Sheng Shen, Ali Farhadi, Trevor Darrell, Yejin Choi, and Anna Rohrbach. Exposing the
limits of video-text models through contrast sets. In NAACL, pp. 3574–3586, 2022. 5.2
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:
Feature learning by inpainting. In CVPR, pp. 2536–2544, 2016. 4.2.2
William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2.5.2, 2.5.3,
2.5.4
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR,
2023. 2.5.3
Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen.
Fatezero: Fusing attentions for zero-shot text-based video editing. In ICCV, 2023. 2.2.7
Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, and Yueting Zhuang. Instructvid2vid: Controllable
video editing with natural language instructions. In ICME, pp. 1–6, 2024. 4.1.5
Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong
Sang. Hierarchical spatio-temporal decoupling for text-to-video generation. In CVPR, pp. 6635–6645,
2024. 2.4.4
Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free
trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024a. 4.1.2
Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise:
Tuning-free longer video diffusion via noise rescheduling. In ICLR, 2024b. 4.5
Y. Qiu et al. Freescale: High-resolution video generation with cascaded diffusion models. arXiv preprint,
2024c. 2.2.5, 4.2.4
Weize Quan, Jiaxi Chen, Yanli Liu, Dong-Ming Yan, and Peter Wonka. Deep learning-based image and
video inpainting: A survey. Int. J. Comput. Vis., pp. 1–34, 2024. 4.2.2
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are
unsupervised multitask learners, 2019. 5.2
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In ICML, pp. 8748–8763, 2021. 2.5.4, 3.3, 5.1
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
JMLR, 21(140):1–67, 2020. 2.5.4
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward
training trillion parameter models. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, pp. 1–16, 2020. 3.2
Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video
transformer. arXiv preprint arXiv:2006.10704, 2020. 2.1.2
Rustam Rakhimov, Artur Sanakoyeu, et al.
Lvdm:
Latent video diffusion models.
arXiv preprint
arXiv:2301.00203, 2023. 2.4.4
71
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. In ICML, pp. 8821–8831, 2021. 2.5.4
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2.5.3, 2.5.4
Anurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network. In CVPR,
pp. 4161–4170, 2017. 2.4.2
Elham Ravanbakhsh, Yongqing Liang, J. Ramanujam, and Xin Li. Deep video representation learning: a
survey, 2024. 5.1
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr,
Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala,
Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment
anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/
2408.00714. 4.6.1
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr,
Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nico-
las Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment
anything in images and videos. In ICLR, 2025. 3.2
Scott Reed, Aäron Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Yutian Chen, Dan
Belov, and Nando Freitas. Parallel multiscale autoregressive density estimation. In ICML, pp. 2912–2921,
2017. 2.1.2
Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David
Novotny.
Common objects in 3d: Large-scale learning and evaluation of real-life 3d category recon-
struction. In ICCV, 2021. 4.6.1
Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d:
Generative 4d gaussian splatting, 2023. 5.4.2
Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v:
Enhancing visual consistency for image-to-video generation, 2024. 1, 4.1.1, 4.4
Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In
CVPR, pp. 3202–3212, 2015. 3.1
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In CVPR, pp. 10684–10695, 2022. 2.1.3, 2.5.1, 2.5.3, 2.5.4
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In MICCAI, pp. 234–241, 2015. 2.5.1
Kevin Dela Rosa.
Video enriched retrieval augmented generation using aligned video captions.
CoRR,
abs/2405.17706, 2024. 5.2
Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner.
Faceforensics: A large-scale video dataset for forgery detection in human faces. In CVPR, 2018. 1
Claudio Rota, Marco Buzzelli, Simone Bianco, and Raimondo Schettini. Video restoration based on deep
learning: a comprehensive survey. Artif. Intell. Rev., 56(6):5317–5364, 2023. 4.2.1
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-
to-image diffusion models with deep language understanding. In NeurIPS, pp. 36479–36494, 2022. 2.5.1,
2.5.3, 3.1
72
Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value
clipping. In ICCV, pp. 2830–2839, 2017. 2.1.1
Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate densely:
Memory-efficient unsupervised training of high-resolution temporal gan. Int. J. Comput. Vis., 128(10):
2586–2606, 2020. 2.1.1
Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Loïc Barrault, Lucia Specia, and Florian
Metze. How2: A large-scale dataset for multimodal language understanding, 2018. 3.1
Seonguk Seo, Joon-Young Lee, and Bohyung Han.
Urvos: Unified referring video object segmentation
network with a large-scale benchmark. In ECCV, pp. 208–223, 2020. 5.1
Younggyo Seo, Kimin Lee, Fangchen Liu, Stephen James, and Pieter Abbeel. Harp: Autoregressive latent
video prediction with high-fidelity image generator. In ICIP, pp. 3943–3947, 2022. 2.1.2
Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a
hybrid representation for high-resolution 3d shape synthesis. In NeurIPS, 2021. 5.4.1
Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal motion
styles. In CVPR, pp. 5652–5661, 2023. 4.4
Changhao Shi, Haomiao Ni, Kai Li, Shaobo Han, Mingfu Liang, and Martin Renqiang Min.
Exploring
compositional visual generation with latent classifier guidance. In CVPR, pp. 853–862, 2023a. 2.3.1
Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See,
Hongwei Qin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting temporal cues for multi-frame optical
flow estimation. arXiv preprint arXiv:2303.08340, 2023b. 2.4.2
Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin,
Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencoding for pretraining optical
flow estimation. arXiv preprint arXiv:2303.01237, 2023c. 2.4.2
Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun
Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation
with explicit motion modeling. In SIGGRAPH, pp. 1–11, 2024. 2.4.2, 4.1.1, 4.1.2, 4.4
Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion
for 3d generation. In ICLR, 2023d. 4.6.2
Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Edit-a-video: Single video
editing with object-aware consistency. In ACML, pp. 1215–1230, 2024. 4.1.5
Chenyang Si, Weichen Fan, Zhengyao Lv, Ziqi Huang, Yu Qiao, and Ziwei Liu.
Repvideo: Rethinking
cross-layer representation for video generation. arXiv preprint arXiv:2501.08994, 2025. 2.4.4
Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion
model for image animation. In NeurIPS, 2019. 1
Uri Singer, Adam Polyak, et al. Make-a-video: Text-to-video generation without text-video data. arXiv
preprint arXiv:2209.14792, 2022. 2.1.3, 2.4.4, 2.5.3
Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal,
Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint
arXiv:2301.11280, 2023. 2.4.1, 5.4.2
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus
Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In CVPR,
pp. 15617–15629, 2022. 5.2
73
Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator
with the price, image quality and perks of stylegan2. In CVPR, pp. 3616–3626, 2022. 2.1.1, 4.4, 4.5
SkyReels-AI.
Skyreels v1:
Human-centric video foundation model.
https://github.com/SkyworkAI/
SkyReels-V1, 2025. 2.4.4, 2.5.3, 2.5.4
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In ICML, pp. 2256–2265, 2015. 1, 2.2.1
Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models.
arXiv preprint
arXiv:2010.02502, 2020a. 2.2.3
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In ICLR, 2020b. 2.4.3
Yeji Song, Wonsik Shin, Junsoo Lee, Jeesoo Kim, and Nojun Kwak. Save: Protagonist diversification with
structure agnostic video editing. arXiv preprint arXiv:2312.02503, 2023. 4.1.5
Achint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, and
Sherry Yang. Videoagent: Self-improving video generation. arXiv preprint arXiv:2410.10076, 2024a. 4.1.6
Achint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, and
Sherry Yang. Videoagent: Self-improving video generation, 2024b. 2.4.4
Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes
from videos in the wild, 2012. 1, 3.1
Alexandros Stergiou and Ronald Poppe. Adapool: Exponential adaptive pooling for information-retaining
downsampling. IEEE Trans. Image Process., 32:251–266, 2023. 1, 3.1
Jonathan C. Stroud, Zhichao Lu, Chen Sun, Jia Deng, Rahul Sukthankar, Cordelia Schmid, and David A.
Ross. Learning video representations from textual web supervision, 2021. 3.1
Deqing Sun, Stefan Roth, and Michael J Black. A quantitative analysis of current practices in optical flow
estimation and the principles behind them. Int. J. Comput. Vis., 106(2):115–137, 2014. 2.4.2
Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid,
warping, and cost volume. In CVPR, pp. 8934–8943, 2018. 2.4.2
Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature
matching with transformers. In CVPR, pp. 8922–8931, 2021. 2.4.2
Mingzhen Sun, Weining Wang, Zihan Qin, Jiahui Sun, Sihan Chen, and Jing Liu. Glober: Coherent non-
autoregressive video generation via global guided video decoder. In NeurIPS, 2023a. 4.4
Mingzhen Sun, Weining Wang, Xinxin Zhu, and Jing Liu. Moso: Decomposing motion, scene and object for
video prediction. In CVPR, pp. 18727–18737, 2023b. 4.4
Qi Sun, Zhiyang Guo, Ziyu Wan, Jing Nathan Yan, Shengming Yin, Wengang Zhou, Jing Liao, and Houqiang
Li. Eg4d: Explicit generation of 4d object without score distillation, 2024. 5.4.2
Yuqing Sun, Yifan Xu, et al. Latent-shift: Exploring latent shifts for video generation. arXiv preprint
arXiv:2302.02016, 2023c. 2.4.4
Ryan Szeto and Jason J. Corso. The devil is in the details: A diagnostic evaluation benchmark for video
inpainting. In CVPR, pp. 21022–21031, 2022. 3.1
Shuai Tan, Bin Ji, and Ye Pan.
Flowvqtalker: High-quality emotional talking face generation through
normalizing flow and quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 26317–26327, 2024a. 10, 4.1.4, 4.3
74
Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang.
Video-infinity: Distributed long video
generation, 2024b. 4.5
Zhiyu Tan, Mengping Yang, Luozheng Qin, Hao Yang, Ye Qian, Qiang Zhou, Cheng Zhang, and Hao Li.
An empirical study and analysis of text-to-image generation using large language model-powered textual
representation. arXiv preprint arXiv:2405.12914, 2024c. 2.5.4
Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling
holistic multi-view image generation with correspondence-aware diffusion. arXiv preprint, 2023. 4.6.2
Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra,
Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: A dense high-resolution multi-view diffusion
model for single or sparse-view 3d object reconstruction. arXiv preprint arXiv:2402.12712, 2024. 4.6.2
Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024a. 2.4.4, 2.5.3, 3.4
Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv
preprint, 2024b. 2.5.3, 2.5.4
The Movie Gen teamMeta. Movie gen: A cast of media foundation models, 2024. 2.5.3
Zachary Teed and Jia Deng.
Raft: Recurrent all-pairs field transforms for optical flow.
In ECCV, pp.
402–419, 2020. 2.4.2, 3.3
Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. Drag-a-video: Non-rigid video editing
with point-based interaction. arXiv preprint arXiv:2312.02936, 2023. 4.1.5
Yannis Tevissen, Khalil Guetari, and Frédéric Petitpont. Towards retrieval augmented generation over large
video libraries. CoRR, abs/2406.14938, 2024. 5.2
Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo.
Emo: Emote portrait alive-generating expressive
portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485,
2024. 4.1.1, 4.1.4, 4.3, 4.4
Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. A
good image generator is what you need for high-resolution video synthesis. In ICLR, 2020. 2.1.1
Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian FATRAS,
Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch
optimal transport. In ICML Workshop, 2023. 2.2.4
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023a. 2.5.4, 3.2
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288, 2023b. 2.5.4, 3.2
Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotem-
poral features with 3d convolutional networks. In ICCV, pp. 4489–4497, 2015. 5.1
Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor:
Editing video motion via content-aware diffusion. In CVPR, pp. 7882–7891, 2024. 4.1.5
Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content
for video generation. In CVPR, pp. 1526–1535, 2018. 2.1.1
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain
Gelly. Fvd: A new metric for video generation. In ICLR Workshop, 2019. 3.3
75
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. 2.5.3
Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave,
Changxi Zheng, and Carl Vondrick. Generative camera dolly: Extreme monocular dynamic novel view
synthesis. In ECCV, pp. 313–331, 2024. 4.6.1, 4.6.3
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 5.1
Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-
mad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video
generation from open domain textual descriptions. In ICLR, 2022. 2.4.4, 2.5.3, 3.4
Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Mcvd - masked conditional video diffusion
for prediction, generation, and interpolation. In NeurIPS, 2022. 4.2.3, 4.2.5
Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian
Laforte, Robin Rombach, and Varun Jampani.
Sv3d: Novel multi-view synthesis and 3d generation
from a single image using latent video diffusion. arXiv preprint arXiv:2403.12008, 2024. 4.6.3, 5.4.1
Zhen Wan, Yue Ma, Chenyang Qi, Zhiheng Liu, and Tao Gui. Unipaint: Unified space-time video inpainting
via mixture-of-experts. arXiv preprint arXiv:2412.06340, 2024. 4.2.2, 4.2.5
Cong Wang, Jiaxi Gu, Panwen Hu, Songcen Xu, Hang Xu, and Xiaodan Liang. Dreamvideo: High-fidelity
image-to-video generation with image retention and text guidance, 2023a. 4.1.1
Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-
text to long video generation via temporal co-denoising, 2023b. 4.5
Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, and
Hongsheng Li. Animatelcm: Computation-efficient personalized style video generation without personal-
ized video data, 2024a. 2.4.4
Haoran Wang, Di Xu, Dongliang He, Fu Li, Zhong Ji, Jungong Han, and Errui Ding. Boosting video-text
retrieval with explicit high-level semantics. In ACM MM, pp. 4887–4898, 2022a. 5.2
Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail A. Dianat, Majid Rabbani, Raghuveer
Rao, and Zhiqiang Tao. Text is mass: Modeling as stochastic embedding for text-video retrieval. CoRR,
abs/2403.17998, 2024b. 5.1
Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion
prior for real-world image super-resolution. Int. J. Comput. Vis., pp. 1–21, 2024c. 13, 4.2.4
Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator:
Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024d.
4.1.2
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope
text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023c. 2.4.1, 2.4.4, 2.5.1
Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan,
Di Zhang, and Kun Gai. Cinemaster: A 3d-aware and controllable framework for cinematic text-to-video
generation, 2025. URL https://arxiv.org/abs/2502.08639. 4.1.2, 4.1.3, 4.6.1, 4.6.2, 4.6.3
Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang,
Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In
CVPR, pp. 9326–9336, 2024e. 4.1.5
Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen.
Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023d.
4.1.5
76
Wenhao Wang and Yi Yang. Vidprom: A million-scale real prompt-gallery dataset for text-to-video diffusion
models. In NeurIPS, 2024. 1, 3.1
Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Swap attention
in spatiotemporal diffusions for text-to-video generation, 2024f. 3.1
Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli
Zhao, and Jingren Zhou.
Videocomposer: Compositional video synthesis with motion controllability.
arXiv preprint arXiv:2306.02018, 2023e. 4.1.1, 4.1.1
Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli
Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. In
NeurIPS, 2023f. 4.4
Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang Xu, and Yi Yang. Align and tell: Boosting text-
video retrieval with local alignment and fine-grained supervision. IEEE Trans. Multimedia., pp. 1–11,
2022b. 5.2
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-
scale, high-quality multilingual dataset for video-and-language research. In ICCV, pp. 4580–4590, 2019.
3.1
Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He,
Jiashuo Yu, Peiqing Yang, et al.
Lavie: High-quality video generation with cascaded latent diffusion
models. arXiv preprint arXiv:2309.15103, 2023g. 2.4.4, 3.4
Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen,
Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao.
Internvid: A large-scale
video-text dataset for multimodal understanding and generation. In ICLR, 2023h. 3.1
Yikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu.
Vidu4d: Single
generated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. In NeurIPS, 2024g. 5.4.2
Yimu Wang and Peng Shi. Video-text retrieval by supervised sparse multi-grained learning. In EMNLP,
2023. 1
Yimu Wang, Shiyin Lu, and Lijun Zhang.
Searching privately by imperceptible lying: A novel private
hashing method with differential privacy. In ACM MM, pp. 2700–2709, 2020a. 5.2
Yimu Wang, Xiu-Shen Wei, Bo Xue, and Lijun Zhang. Piecewise hashing: A deep hashing method for
large-scale fine-grained search. In PRCV, pp. 432–444, 2020b. 5.2
Yimu Wang, Bo Xue, Quan Cheng, Yuhui Chen, and Lijun Zhang. Deep unified cross-modality hashing by
pairwise data alignment. In IJCAI, pp. 1129–1135, 2021. 5.2
Yuhan Wang, Liming Jiang, and Chen Change Loy. Styleinv: A temporal style modulated inversion network
for unconditional video generation. In ICCV, pp. 22851–22861, 2023i. 2.1.1
Zhen Wang, Yunhao Ba, Pradyumna Chari, Oyku Deniz Bozkurt, Gianna Brown, Parth Patwa, Niranjan
Vaddi, Laleh Jalilian, and Achuta Kadambi. Synthetic generation of face videos with plethysmograph
physiology. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
20587–20596, 2022c. 4.1.6
Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen,
Tianfan Xue, Bo Dai, and Dahua Lin. Humanvid: Demystifying training data for camera-controllable
human image animation, 2024h. 4.6.1, 4.6.3
Zhouxia Wang, Yushi Lan, Shangchen Zhou, and Chen Change Loy. Objctrl-2.5 d: Training-free object
control with camera poses. arXiv preprint arXiv:2412.07721, 2024i. 4.1.2
77
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying
Shan. Motionctrl: A unified and flexible motion controller for video generation. In ACM SIGGRAPH
2024 Conference Papers, pp. 1–11, 2024j. 4.1.3
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying
Shan. Motionctrl: A unified and flexible motion controller for video generation. In SIGGRAPH, 2024k.
4.6.3
Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David J. Fleet. Controlling space and
time with diffusion models, 2024. 4.6.3
Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait
animation. arXiv preprint arXiv:2403.17694, 2024a. 4.1.4, 4.3
Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu,
Zhizhong Huang, Jiaxin Ye, Yingya Zhang, and Hongming Shan. Dreamvideo-2: Zero-shot subject-driven
video customization with precise motion control, 2024b. 2.2.7, 14, 4.3
Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu,
Zhizhong Huang, Jiaxin Ye, et al.
Dreamvideo-2: Zero-shot subject-driven video customization with
precise motion control. arXiv preprint arXiv:2410.13830, 2024c. 2.2.8, 4.1.1, 4.1.2, 4.3, 14
Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu,
Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-to-video generation with diffusion models.
In CVPR, pp. 7395–7405, 2024. 2.1.4
Dongming Wu, Xingping Dong, Ling Shao, and Jianbing Shen. Multi-level representation learning with
semantic alignment for referring video object segmentation. In CVPR, pp. 4986–4995, 2022a. 5.1
Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan,
and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical
perspectives. In ICCV, pp. 20087–20097, 2023a. 3.3
Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context inter-
leaved video-language understanding. In NeurIPS, 2024a. 5.3
Hongjia Wu, Özgü Alay, Anna Brunstrom, Simone Ferlin, and Giuseppe Caso. Peekaboo: Learning-based
multipath scheduling for dynamic heterogeneous environments. IEEE Journal on Selected Areas in Com-
munications, 38(10):2295–2310, 2020. 4.1.2
Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying
Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for
text-to-video generation. In ICCV, pp. 7623–7633, 2023b. 2.2.6, 4.1.5
Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video
object segmentation. In CVPR, pp. 4964–4974, 2022b. 5.1
Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai
Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video inpainting via multimodal large
language models. In CVPR, pp. 12501–12511, 2024b. 4.2.2
Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai
Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint arXiv:2406.17758,
2024c. 4.1.2, 4.1.3
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversa-
tions. In COLM, 2024d. 2.4.4
78
Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn a motion
pattern for few-shot-based video generation. arXiv preprint arXiv:2310.10769, 2023c. 2.2.6
Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, and Aleksander
Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv:2411.18613, 2024e.
4.6.3, 21, 5.4.2
Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Liang Pan Jiawei Ren, Wayne Wu, Lei Yang, Jiaqi Wang,
Chen Qian, Dahua Lin, and Ziwei Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic
perception, reconstruction and generation. In CVPR, 2023d. 4.6.1
Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li,
Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In
European Conference on Computer Vision, pp. 331–348. Springer, 2024f. 4.1.2
X. Wu et al. Videoprefer: A comprehensive video preference dataset and reward model. arXiv preprint,
2024g. 2.2.5
Xiuzhe Wu, Pengfei Hu, Yang Wu, Xiaoyang Lyu, Yan-Pei Cao, Ying Shan, Wenming Yang, Zhongqian
Sun, and Xiaojuan Qi. Speech2lip: High-fidelity speech to lip generation by learning from a short video.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22168–22177, 2023e.
4.1.4
Yue Wu, Yu Deng, Jiaolong Yang, Fangyun Wei, Qifeng Chen, and Xin Tong. Anifacegan: Animatable
3d-aware face image generation for video avatars. In NeurIPS, 2022c. 2.1.1
Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor
Gilitschenski, and Sergey Tulyakov. Mind the time: Temporally-controlled multi-event video generation.
arXiv preprint arXiv:2412.05263, 2024h. 4.1.6
Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d object
learning from rgb-d videos. In CVPR, pp. 22378–22389, 2024. 4.6.1
Jinxi Xiang, Ricong Huang, Jun Zhang, Guanbin Li, Xiao Han, and Yang Wei.
Versvideo: Leveraging
enhanced temporal diffusion models for versatile video generation. In ICLR, 2023. 2.5.2
Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive
autoregressive video diffusion models. arXiv preprint arXiv:2410.08151, 2024a. 2.1.4, 4.5
Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: A high-quality dataset and
benchmark for video face super-resolution. In CVPR Workshops, pp. 656–665, 2022. 3.1
Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object
inpainting with diffusion model. In CVPR, pp. 22428–22437, 2023. 4.2.2
Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content
generation with multi-frame and multi-view consistency, 2024b. 4.6.2, 5.4.2
Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin
Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors, 2023.
3.2, 4.1.1, 4.1.1
Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong.
Tooncrafter: Generative cartoon interpolation. arXiv preprint arXiv:2405.17933, 2024a. 4.2.3
Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient
video generation. In CVPR, pp. 7827–7839, 2024b. 2.4.4, 2.5.4, 3.2, 4.1.5
Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. A
survey on video diffusion models, 2024c. (document), 1
79
Yi Xiong, Ayan Chakrabarti, Ronen Basri, Steven J Gortler, David W Jacobs, and Todd Zickler. Learning
from synthetic data for sky replacement in images. In ECCV, 2018. 1
Dejia Xu, Yifan Jiang, Chen Huang, Liangchen Song, Thorsten Gernoth, Liangliang Cao, Zhangyang Wang,
and Hao Tang. Cavia: Camera-controllable multi-view video diffusion with view-integrated attention.
arXiv preprint arXiv:2410.10774, 2024a. 4.1.3, 18, 4.6.1, 4.6.2, 4.6.3
Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-
controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024b. 4.1.1, 4.1.1,
4.1.3, 4.6.1, 4.6.2, 4.6.3
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video
question answering via gradually refined attention over appearance and motion. In ACM MM, 2017. 5.3
Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang.
Easyanimate: A high-performance long video generation method based on transformer architecture. arXiv
preprint arXiv:2405.18991, 2024c. 2.4.4, 2.5.3, 3.4
Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video
and language. In CVPR, pp. 5288–5296, 2016. 1, 3.1
Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See-Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava
extension from images to videos for video dense captioning. arxiv preprint arXiv:2404.16994, 2024d. 3.2
Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin
Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. arXiv preprint
arXiv:2404.10667, 2024e. 4.1.4, 4.3
Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining
Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In
CVPR, pp. 5026–5035, 2022a. 3.1
L Xue. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934,
2020. 1
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and
Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models. TACL, 10:291–306,
2022b. 2.5.4
Liqi Yan, Qifan Wang, Yiming Cui, Fuli Feng, Xiaojun Quan, Xiangyu Zhang, and Dongfang Liu. Gl-rg:
Global-local representation granularity for video captioning. In IJCAI, pp. 2769–2775, 2022. 5.1
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae
and transformers. arXiv preprint arXiv:2104.10157, 2021. 2.1.2, 2.4.4, 2.5.3
Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion-conditioned image
animation for video editing. arXiv preprint arXiv:2311.18827, 2023a. 4.1.5
Wilson Yan, Danijar Hafner, Stephen James, and Pieter Abbeel. Temporally consistent transformers for
video generation. In ICML, pp. 39062–39098, 2023b. 3.1
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian
Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305,
2023a. 2.5.4
Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai
Zhao. Vript: A video is worth thousands of words. In NeurIPS, 2024a. 1, 3.1
Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi
Feng, and Guosheng Lin. Magic-boost: Boost 3d generation with mutli-view conditioned diffusion, 2024b.
4.6.2
80
Gengshan Yang and Deva Ramanan. Volumetric correspondence networks for optical flow. In NeurIPS, pp.
794–805, 2019. 2.4.2
Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, and Tao Mei. Hi3d:
Pursuing high-resolution image-to-3d generation with video diffusion models. In ACM MM, 2024c. 4.6.3,
5.4.1
Hao Yang, Chunfeng Yuan, Bing Li, Yang Du, Junliang Xing, Weiming Hu, and Stephen J. Maybank.
Asymmetric 3d convolutional neural networks for action recognition. Pattern Recognit., 85:1–12, 2019. 5.1
Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency for
multi-view images diffusion. In CVPR, pp. 7079–7088, 2024d. 4.6.2
Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything:
Unleashing the power of large-scale unlabeled data. In CVPR, 2024e. 4.6.1
Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth
anything v2. arXiv:2406.09414, 2024f. 4.6.1
Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale
Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In ICLR, 2024g. 2.4.4
Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing
Liao. Direct-a-video: Customized video generation with user-directed camera movement and object mo-
tion. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference
Papers ’24, pp. 1–12, 2024h. 4.6.3
Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and
Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object
motion. In ACM SIGGRAPH 2024 Conference Papers, pp. 1–12, 2024i. 4.1.3
Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-
to-video translation. In SIGGRAPH Asia, pp. 1–11, 2023b. 4.1.5
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi
Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert
transformer. arXiv preprint arXiv:2408.06072, 2024j. 2.4.4, 2.5.3, 2.5.4, 3.2, 4.1.1
Yining Yao, Xi Guo, Chenjing Ding, and Wei Wu. Mygo: Consistent and controllable multi-view driving
video generation with camera control, 2024. 4.6.3
Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian, Ji Zhang, and Fei Huang. Hitea: Hierarchical
temporal-aware video-language pre-training. In ICCV, pp. 15359–15370, 2023. 5.1
Chen Yeh, You-Ming Chang, Wei-Chen Chiu, and Ning Yu. T2vs meet vlms: A scalable multimodal dataset
for visual harmfulness recognition, 2024. 5.1
Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong
Tian, Enze Xie, and Daquan Zhou. Magic 1-for-1: Generating one minute video clips within one minute,
2025. 2.4.4, 2.5.3, 2.5.4
Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa:
Fine-grained control in video generation by integrating text, image, and trajectory.
arXiv preprint
arXiv:2308.08089, 2023a. 4.1.2
Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang,
Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Ming Gong, Lijuan Wang, Zicheng Liu, Houqiang Li, and
Nan Duan. Nuwa-xl: Diffusion over diffusion for extremely long video generation. In ACL, pp. 1309–1320,
2023b. 4.5
81
Tianwei Yin, Qiang Zhang, Richard Zhang, William T Freeman, Fredo Durand, Eli Shechtman, and Xun
Huang. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772, 2024.
2.1.4, 2.4.4, 2.5.2
Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view
synthesizer, 2024. 4.6.4
Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Attila
Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video
diffusion models. In NeurIPS, 2024a. 5.4.2
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting
with contextual attention. In CVPR, pp. 5505–5514, 2018. 4.2.2
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:
Contrastive captioners are image-text foundation models. TMLR, 2022a. 3.2
Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexan-
der Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-
image generation. arXiv preprint arXiv:2206.10789, 2022b. 3.1
Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann,
Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer.
In CVPR, pp. 10459–10469, 2023a. 2.4.4, 2.5.3
Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong
Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa,
David A. Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In
ICLR, 2024b. 2.4.4, 2.5.3
Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, and Jingjing Liu. Multimodal federated learning via contrastive
representation ensemble. In ICLR, 2023b. 5.2
Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating
videos with dynamics-aware implicit generative adversarial networks. In ICLR, 2021. 2.1.1, 4.4
Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming
Zhu, Zhangyang Xiong, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Mvimgnet:
A large-scale dataset of multi-view images. In CVPR, pp. 9150–9161, 2023c. 4.6.1
Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. End-to-end concept word detection for video
captioning, retrieval, and question answering. In CVPR, pp. 3261–3269, 2017. 5.2
Lu Yuan et al. Instructvideo: Instructing video diffusion models with human feedback. arXiv preprint, 2023.
2.2.5, 2.4.4
Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan.
Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440,
2024a. 4.3
Xin Yuan, Jinoo Baek, Keyang Xu, Omer Tov, and Hongliang Fei. Inflation with diffusion: Efficient temporal
adaptation for text-to-video super-resolution. In WACV, pp. 489–496, 2024b. 4.2.4
Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, and Lichao Sun.
Mora:
Enabling generalist video generation via a multi-agent framework. arXiv preprint arXiv:2403.13248, 2024c.
2.4.4
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.
Merlot: Multimodal neural script knowledge models. In NeurIPS, pp. 23634–23651, 2021. 3.1
82
Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun.
Leveraging video descriptions to learn video question answering. In AAAI, pp. 4334–4340, 2017. 5.3
Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels
dance: High-dynamic video generation. In CVPR, pp. 8850–8860, 2024a. 8, 4.1.1
Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao
Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. In ECCV, pp. 163–179, 2024b. 5.4.2
Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi
Yan, and Miloš Hašan. Rgb-x: Image decomposition and synthesis using material- and lighting-aware
diffusion models. In Special Interest Group on Computer Graphics and Interactive Techniques Conference
Conference Papers ’24, pp. 1–11, 2024c. 4.1.1
Bowen Zhang, Xiaofei Xie, Haotian Lu, Na Ma, Tianlin Li, and Qing Guo. Mavin: Multi-action video
generation with diffusion models via transition video infilling, 2024a. 4.5
David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and
Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv
preprint arXiv:2309.15818, 2023a. 2.4.1, 2.5.3
Guiwei Zhang, Tianyu Zhang, Guanglin Niu, Zichang Tan, Yalong Bai, and Qing Yang. Camel: Causal
motion enhancement tailored for lifting text-driven video editing. In CVPR, pp. 9079–9088, 2024b. 4.4
Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view
video diffusion model for 4d generation, 2024c. 5.4.2
Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun,
and Ming-Hsuan Yang. Monst3r: A simple approach for estimating geometry in the presence of motion.
arXiv preprint arxiv:2410.03825, 2024d. 4.6.1
Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm:
Large reconstruction model for 3d gaussian splatting, 2024e. 5.4.1
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion
models. In ICCV, pp. 3813–3824, 2023b. 4.1.1
Qihang Zhang, Ceyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Towards smooth video composition.
In ICLR, 2023c. 4.4, 4.5
Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao,
and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models, 2023d.
4.1.1, 4.1.1
Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo:
Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023e. 2.2.7, 4.1.5
Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, and Wang-
meng Zuo. Videoelevator: Elevating video generation quality with versatile text-to-image diffusion models.
arXiv preprint arXiv:2403.05438, 2024f. 2.2.7
Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image
animator via plug-and-play modules in text-to-image models, 2023f. 4.1.1
Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and
Weizhi Wang.
Tora:
Trajectory-oriented diffusion transformer for video generation.
arXiv preprint
arXiv:2407.21705, 2024g. 4.1.2
Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris
Metaxas, and Licheng Yu. Avid: Any-length video inpainting with diffusion model. In CVPR, pp. 7162–
7172, 2024h. 11, 4.2.2
83
Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, and Tao Mei.
Trip:
Temporal residual learning with image noise prior for image-to-video diffusion models. In CVPR, pp.
8671–8681, 2024i. 4.1.1, 4.4
Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. Towards consistent
video editing with text-to-image diffusion models. In NeurIPS, 2024j. 4.1.5
Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen.
Moviedreamer: Hierarchical generation for coherent long visual sequences. In ICLR, 2025. 4.5
Haoyu Zhao, Tianyi Lu, Jiaxi Gu, Xing Zhang, Qingping Zheng, Zuxuan Wu, Hang Xu, and Yu-Gang Jiang.
MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing, pp. 205–221. Springer
Nature Switzerland, September 2024a. ISBN 9783031726491. doi: 10.1007/978-3-031-72649-1_12. URL
http://dx.doi.org/10.1007/978-3-031-72649-1_12. 4.1.1, 4.1.1
Kai Zhao, Xiaoli Lin, et al. Dysen-vdm: Dynamic style enhanced video diffusion models. arXiv preprint
arXiv:2305.01012, 2023a. 2.4.4
Nanxuan Zhao, Jianbo Jiao, Weidi Xie, and Dahua Lin. Cali-nce: Boosting cross-modal video representation
learning with calibrated alignment. In CVPR, pp. 6317–6327, 2023b. 5.1
Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and
Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. arXiv preprint
arXiv:2310.08465, 2023c. 4.1.5
Ruoyu Zhao et al.
Videofactory:
Efficient video generation with fine-grained control.
arXiv preprint
arXiv:2305.01893, 2023d. 1
Shuai Zhao, Linchao Zhu, Xiaohan Wang, and Yi Yang. Centerclip: Token clustering for efficient text-video
retrieval. In SIGIR, pp. 970–981, 2022. 5.1, 5.2
Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan.
Cv-vae: A compatible video vae for latent generative video models. arXiv preprint arXiv:2405.20279,
2024b. 2.5.3
Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: Animating
one image to 4d dynamic scene, 2023e. 5.4.2
Zhou Zhao, Jinghao Lin, Xinghua Jiang, Deng Cai, Xiaofei He, and Yueting Zhuang.
Video question
answering via hierarchical dual-level attention network learning. In ACM MM, pp. 1050–1058, 2017. 5.3
Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-
to-video diffusion model, 2024a. 4.6.2, 4.6.3
Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. A unified approach
for text-and image-guided 4d scene generation. In CVPR, pp. 7300–7309, 2024b. 5.4.2
Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi
Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024c. 1, 2.4.4, 2.5.2,
2.5.3, 2.5.4, 3.1, 3.2, 5
Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Weihong Deng, and Tat-Seng Chua. Video question answer-
ing: Datasets, algorithms and challenges. In EMNLP, pp. 6439–6455, 2022. 5.3
Jia Zhou, Yang Dong, et al. Magicvideo: Efficient video generation with latent diffusion models. arXiv
preprint arXiv:2211.11018, 2022. 2.1.3, 2.4.1, 2.4.4, 1, 2.5.1, 2.5.3
Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instruc-
tional videos. In AAAI, 2018a. 3.1
84
Qiang Zhou, Shaofeng Zhang, Nianzu Yang, Ye Qian, and Hao Li. Motion control for enhanced complex
action video generation. arXiv preprint arXiv:2411.08328, 2024a. 4.1.2
Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy.
Upscale-a-video:
Temporal-consistent diffusion model for real-world video super-resolution. In CVPR, pp. 2535–2545, 2024b.
4.2.4
Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning
view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018b. 4.6.1
Xingcheng Zhou, Mingyu Liu, Ekim Yurtsever, Bare Luka Zagar, Walter Zimmer, Hu Cao, and Alois C.
Knoll. Vision language models in autonomous driving: A survey and outlook. IEEE Trans. Intell. Veh.,
pp. 1–20, 2024c. 1
Yiwen Zhou, Xiaoyu Lin, et al.
Videofusion: Diffusion-based text-to-video generation.
arXiv preprint
arXiv:2302.05798, 2023. 2.4.4
Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent
self-attention for long-range image and video generation. In NeurIPS, 2024d. 4.4
Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, and Gengchen
Mai. Img2loc: Revisiting image geolocalization using multi-modality foundation models and image-based
retrieval-augmented generation. In SIGIR, pp. 2749–2754, 2024e. 5.2
Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In CVPR, pp. 8743–
8752, 2020. 5.2
Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger:
Make your dream a vlog. arXiv preprint arXiv:2401.09414, 2024. 2.4.4, 3.4
Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong,
and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and
compatibility. arXiv preprint arXiv:2403.12035, 2024. 4.2.2
Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong
Dong, Liefeng Bo, and Qixing Huang. Videomv: Consistent multi-view generation based on large video
generative model, 2024. 5.4.2
85

==================================================

==================================================
Paper 2
Title: Automated Vulnerability Injection in Solidity Smart Contracts: A Mutation-Based Approach for Benchmark Development
Abstract: The security of smart contracts is critical in blockchain systems, where even
minor vulnerabilities can lead to substantial financial losses. Researchers
proposed several vulnerability detection tools evaluated using existing
benchmarks. However, most benchmarks are outdated and focus on a narrow set of
vulnerabilities. This work evaluates whether mutation seeding can effectively
inject vulnerabilities into Solidity-based smart contracts and whether
state-of-the-art static analysis tools can detect the injected flaws. We aim to
automatically inject vulnerabilities into smart contracts to generate large and
wide benchmarks. We propose MuSe, a tool to generate vulnerable smart contracts
by leveraging pattern-based mutation operators to inject six vulnerability
types into real-world smart contracts. We analyzed these vulnerable smart
contracts using Slither, a static analysis tool, to determine its capacity to
identify them and assess their validity. The results show that each
vulnerability has a different injection rate. Not all smart contracts can
exhibit some vulnerabilities because they lack the prerequisites for injection.
Furthermore, static analysis tools fail to detect all vulnerabilities injected
using pattern-based mutations, underscoring the need for enhancements in static
analyzers and demonstrating that benchmarks generated by mutation seeding tools
can improve the evaluation of detection tools.
PDF URL: http://arxiv.org/pdf/2504.15948v1
Submission Date: 2025-04-22 14:46:18+00:00

Full Content:
Automated Vulnerability Injection in Solidity Smart Contracts:
A Mutation-Based Approach for Benchmark Development
Gerardo Iuliano, Luigi Allocca, Matteo Cicalese, Dario Di Nucci
University of Salerno
Fisciano (SA), Italy
geiuliano@unisa.it,l.allocca8@studenti.unisa.it,mcicalese@unisa.it,ddinucci@unisa.it
Abstract
The security of smart contracts is critical in blockchain systems,
where even minor vulnerabilities can lead to substantial financial
losses. Researchers proposed several vulnerability detection tools
evaluated using existing benchmarks. However, most benchmarks
are outdated and focus on a narrow set of vulnerabilities. This
work evaluates whether mutation seeding can effectively inject
vulnerabilities into Solidity-based smart contracts and whether
state-of-the-art static analysis tools can detect the injected flaws.
We aim to automatically inject vulnerabilities into smart contracts
to generate large and wide benchmarks. We propose MuSe, a tool
to generate vulnerable smart contracts by leveraging pattern-based
mutation operators to inject six vulnerability types into real-world
smart contracts. We analyzed these vulnerable smart contracts us-
ing Slither, a static analysis tool, to determine its capacity to identify
them and assess their validity. The results show that each vulner-
ability has a different injection rate. Not all smart contracts can
exhibit some vulnerabilities because they lack the prerequisites for
injection. Furthermore, static analysis tools fail to detect all vulner-
abilities injected using pattern-based mutations, underscoring the
need for enhancements in static analyzers and demonstrating that
benchmarks generated by mutation seeding tools can improve the
evaluation of detection tools.
CCS Concepts
• Software and its engineering →Software verification and
validation; • Security and privacy →Software security engi-
neering.
Keywords
Vulnerability, Smart Contract, Mutation Testing, Benchmark
ACM Reference Format:
Gerardo Iuliano, Luigi Allocca, Matteo Cicalese, Dario Di Nucci. 2018. Auto-
mated Vulnerability Injection in Solidity Smart Contracts: A Mutation-Based
Approach for Benchmark Development. In Proceedings of The 29th Inter-
national Conference on Evaluation and Assessment in Software Engineering
(EASE 2025). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.
XXXXXXX
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
EASE 2025, Istanbul, Türkiye
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/2018/06
https://doi.org/XXXXXXX.XXXXXXX
1
Introduction
Smart contracts are self-executing programs operating on blockchain
platforms like Ethereum [7]. They enable decentralized applications
to execute pre-defined terms and conditions autonomously, pro-
moting trust between untrusted parties. With the advancement of
blockchain technology, smart contracts have become indispensable
in various domains, including digital payments and decentralized
finance [35]. However, their immutable nature, while foundational
to the blockchain trust model, presents significant challenges when
vulnerabilities are discovered [26]. Unlike traditional software, de-
ployed smart contracts cannot be modified or patched, making even
minor flaws potentially catastrophic [1].
The security of smart contracts is critical. Vulnerabilities such as
Reentrancy, Timestamp dependence, Transaction Order Dependence
(TOD), Authorization through tx.origin, and Unchecked external calls
have already led to significant financial losses, exemplified by the
infamous DAO hack [27]. Researchers and developers have cre-
ated a myriad of vulnerability detection tools. Often leveraging
static or dynamic analysis, these tools aim to identify and mitigate
potential defects before deployment and, in some cases, on-chain.
Despite their advancements, these tools are not infallible. False
positives [30], false negatives, and the inability to detect complex
vulnerability patterns limit their effectiveness, underscoring the
need for robust evaluation.
One critical barrier to improving detection tools is the lack of
comprehensive and diverse benchmarks [13]. Existing datasets are
often outdated, limited in size, or narrowly focused on specific
vulnerabilities, leaving many tools untested against realistic sce-
narios. The most commonly used benchmarks [15] comprise smart
contracts written using the old Solidity version and affected by a
subset of the known vulnerability. Other datasets often contain
simplistic toy contracts [39], like the SWC Registry. In addition,
about 96% of smart contracts present in datasets are involved in no
more than five transactions [9]. Ren et al. [32] highlighted that tools
should be evaluated using a comprehensive benchmark suite that
integrates multiple vulnerability types. The absence of high-quality
and updated benchmarks hinders progress in the field by affecting
the ability to validate and improve detection tools.
To address this gap, we propose MuSe, a mutation-based tool
based on SuMo [2] to generate benchmarks by injecting vulner-
abilities into smart contracts. By leveraging mutation operators
designed around known vulnerability patterns, our approach sys-
tematically introduces faults into realistic scenarios, enabling the
generation of contracts with vulnerabilities placed in both typi-
cal and unconventional yet valid locations, challenging detection
tools to expand their scope and improve accuracy. Such versatil-
ity allows for evaluating detection tools against various scenarios,
arXiv:2504.15948v1  [cs.SE]  22 Apr 2025
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
Iuliano et al.
including edge cases. The strength of our approach lies in its foun-
dation on pattern-based mutation operators. These operators are
designed to inject vulnerabilities wherever their corresponding pat-
terns are identified, ensuring consistency and extensibility; as new
vulnerabilities are identified, corresponding operators can be added
with minimal effort. We manually validated mutation operators
and evaluated the generated benchmarks using Slither [16], a
state-of-the-art static analysis tool for smart contracts.
The results reveal that each vulnerability has a different injec-
tion rate, and successful injection depends on whether the smart
contract satisfies the necessary preconditions to adhere to vulnera-
bility patterns. Detection outcomes reveal significant limitations of
Slither in identifying vulnerabilities, especially when they are in-
jected into unconventional or unexpected locations. These findings
underscore the weaknesses of current static analysis tools and high-
light the pressing need for more advanced detection techniques.
Moreover, MuSe allows to increase the benchmark size of above
840%. In conclusion, our study identifies the gaps in the static ana-
lyzer, offering a tangible solution for researchers and practitioners
to enhance or generate new benchmarks to evaluate their detection
tools. Our work provides the following contributions:
• MuSe, a mutation seeding tool to generate benchmarks in-
jecting vulnerabilities in Solidity smart contracts using pattern-
based mutation operators;
• an enhanced version of the dataset smartbugs-wild contain-
ing 350,493 vulnerable smart contracts;
• a list of weaknesses that affect the capabilities of Slither in
detecting vulnerabilities.
Paper Structure. This paper is organized as follows. Section 2
establishes the background and reviews related work. Section 3
details the research method, including research questions and the
mutation-based approach. Section 4 presents the experimental re-
sults, while Section 5 discusses key findings, implications, and their
relevance. Section 6 addresses threats to validity, and Section 7
concludes the paper and provides future research directions.
2
Background and Related Work
This section introduces smart contracts, their vulnerabilities, and
mutation testing. Furthermore, we provide some related work that
is relevant to our study.
2.1
Smart Contracts
Smart contracts are self-executing programs designed to automati-
cally enforce terms and conditions between untrusted parties [23].
Initially envisioned as a way to automate legal contracts, the rise
of blockchain technology has transformed smart contracts into
scripts that execute synchronously across nodes in a distributed
ledger [40]. On the Ethereum blockchain, smart contracts run
within the Ethereum Virtual Machine (EVM), a Turing-complete,
stack-based virtual machine that ensures isolated contract code
execution. A smart contract is identified by a unique address, pri-
vate storage, and a balance in Ether, and it contains executable
code. When a transaction is sent to a contract address, it triggers
the contract functionality, providing invocation data and paying
transaction fees using Gas [23].
2.2
Smart Contract Vulnerabilities
Ethereum smart contracts are prone to various vulnerabilities unique
to blockchain technology. Reentrancy is one of the most criti-
cal [24, 25], as evidenced by the infamous DAO hack, where an
attacker repeatedly called back into a contract to drain its funds.
Another major issue is the misuse of transaction origin for autho-
rization, which attackers can easily spoof to gain unauthorized
access. Timestamp manipulation by miners is another concern, al-
lowing them to alter timestamps and compromise the security of
critical contract functions [22, 28]. Transaction-ordering depen-
dence (TOD), where the order of transactions is unpredictable, can
be exploited to unfairly manipulate outcomes, such as reducing
rewards before submitting a valid solution [14, 34, 37]. External
calls also pose significant risks. Attackers can exploit these calls to
execute malicious code. Furthermore, failing to handle a function
return value properly may enable attackers to drain contract bal-
ances. Denial of Service (DoS) attacks can arise in several ways [33],
such as through costly external calls or inefficient looping behavior.
Smart contract development differs fundamentally from traditional
software programming. Vulnerabilities in smart contracts deployed
on public blockchains are particularly challenging to fix due to the
immutable nature of blockchain systems. While some traditional se-
curity techniques are applicable, smart contracts introduce unique
challenges, and many vulnerabilities arise from the distinctive char-
acteristics of blockchain technology.
2.3
Mutation Testing
Mutation testing is a software testing technique to evaluate the ef-
fectiveness of test cases by intentionally introducing small changes,
called mutants, into the source code to simulate potential faults or
errors. The primary goal is to assess whether the existing test cases
can detect these changes, thereby measuring the fault-detection
capability of a test suite [21, 29]. This process ensures software
is rigorously validated, improving its reliability and reducing the
likelihood of undetected faults in production.
Mutation testing in Solidity applies the same principles as tradi-
tional mutation testing. Still, it focuses on the unique characteristics
of smart contracts, injecting Solidity-specific faults to evaluate the
effectiveness of vulnerability detection and test suite robustness.
In the literature, some tools have been proposed for this purpose.
Chapman et al. proposed Deviant [8], a mutation testing tool
designed for Solidity smart contracts. It automatically generates
mutated versions of a given Solidity project and runs them against
existing test suites to assess their effectiveness. Deviant includes
mutation operators that cover Solidity-specific features based on a
Solidity fault taxonomy and traditional programming constructs
to simulate faults. The authors used Deviant to evaluate the test
effectiveness of three Solidity projects. Their findings show that
achieving high statement and branch coverage in Solidity does not
guarantee strong code quality. This study provides valuable insights
for Solidity developers, emphasizing the need for more rigorous
testing to minimize financial risks. Ivanova and Khritankov pre-
sented RegularMutator [20], a tool for improving the reliability
of smart contracts written using Solidity language. RegularMu-
tator implements language-specific operators that correspond to
common errors made during the development of smart contracts.
Automated Vulnerability Injection in Solidity Smart Contracts:
A Mutation-Based Approach for Benchmark Development
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
Injection of mutations in program code is implemented using regu-
lar expressions. The tool demonstrated its effectiveness in testing
large-scale smart contract projects. The study concluded that muta-
tion analysis provides a more reliable measure of test suite quality
than traditional test line coverage. Barboni et al. proposed SuMo [3],
a mutation testing tool designed for Solidity smart contracts, in-
corporating 25 Solidity-specific mutation operators alongside 19
traditional ones. It enables mutation testing on Solidity projects to
assess test effectiveness. SuMo was later extended by ReSuMo [4],
which introduces a regression mutation testing approach. ReSuMo
employs a static, file-level technique to selectively mutate a sub-
set of smart contracts and rerun only relevant test cases during
regression testing. After each mutation testing run, ReSuMo in-
crementally updates its results by leveraging test outcomes from
previous program revisions, improving efficiency and reducing
redundant computations.
2.4
Related Work
Bug injection is a testing method widely studied in traditional
software programs; however, only a few studies have addressed its
application to smart contracts using mutation security testing.
Ghaleb and Pattabiraman proposed SolidiFI [17], an automated
and systematic approach to evaluate static analysis tools. The tool
injects bugs into a smart contract to introduce the targeted vulner-
abilities and then checks the generated buggy contracts using the
static analysis tools. The tool injects bugs by adding vulnerable
code snippets, code transformations, and weakening security mech-
anisms. We extended the code transformation approach, focusing
on more vulnerabilities and a larger dataset to experiment. We also
validated the mutation seeding tool to provide a tool to generate
new vulnerable smart contracts starting from an initial set.
Chu et al. [10] introduced SGDL (Smart Contract Vulnerability
Generation with Deep Learning), an approach to create authentic
and diverse vulnerability datasets for smart contracts. SGDL com-
bines generative adversarial networks (GANs) with static analysis
to extract syntactic and semantic information from contracts. Us-
ing this information, it generates realistic vulnerability fragments
and injects them into smart contracts via an abstract syntax tree,
ensuring syntactic correctness. The approach depends on a labeled
dataset and researchers’ expertise in vulnerabilities to train the
GANs. However, comprehensive datasets for various vulnerabilities
remain scarce or are not openly accessible for academic research.
Consequently, SGDL focus is limited to specific vulnerabilities. In
contrast, our mutation-based approaches rely on predefined pat-
terns and manually designed mutation operators. New operators
do not depend on datasets but on vulnerability patterns, making
the approach easy to extend to other vulnerabilities.
Hajdu et al. [18] conducted a study using software-implemented
fault injection (SWIFI) to evaluate the dependability of permis-
sioned blockchain systems in the presence of faulty smart contracts.
They introduced general software and blockchain-specific faults
into smart contract code to assess their impact on system reliability
and integrity. They also investigated the effectiveness of formal
verification and runtime protection mechanisms in detecting and
mitigating these faults. The authors used Hyperledger Fabric and
evaluated 15 smart contracts, each tested in three versions: a base
version, a version with extensive protections, and a version with-
out protections. Faults were injected into the unprotected versions,
resulting in 651 faulty variants. The findings revealed that formal
verification and runtime protections complement built-in platform
checks but cannot detect all faults. Our study focused on a signifi-
cantly larger dataset of faulty smart contracts and targeted critical
smart contract vulnerabilities. Additionally, the goal was distinct,
focusing on generating benchmarks that can be used to validate
and improve the effectiveness of vulnerability detection tools.
Regarding benchmark generation, other techniques have been
explored in the literature, such as analyzing audit reports and lever-
aging large language models (LLMs). On the one hand, Zheng et
al. [39] created a large-scale dataset of SWC weaknesses from real-
world DApp projects. They recruited 22 participants to analyze
1,199 open-source audit reports from 29 security teams, identifying
9,154 weaknesses. Their work resulted in two distinct datasets. The
DAppSCAN-Source dataset contains 39,904 Solidity files with 1,618
SWC weaknesses. However, these files may not be directly com-
pilable for automated analysis. To address this, the authors devel-
oped a tool that automatically identifies dependency relationships
within DApp projects and resolves missing public libraries. The
DAppSCAN-Bytecode dataset includes 6,665 compiled smart con-
tracts containing 888 SWC weaknesses. Evaluation results showed
that existing detection tools perform poorly on these datasets, high-
lighting the need for future research to focus on real-world smart
contract datasets rather than simplistic toy contracts. On the other
hand, Daspe et al. [11] utilized large language models (LLMs) to
generate a dataset of Solidity smart contracts. To guide the LLM
during the generation process, they adopted an approach inspired
by Test-Driven Development (TDD) [5]. Each prompt was submit-
ted to an LLM, producing Solidity code that was then parsed. The
Solidity compiler verified the syntax, and if the contract compiled
successfully, Slither was applied for static analysis to detect vul-
nerabilities. Next, they created a project containing the generated
contract and functional tests, which were then executed. They col-
lected the compilation status, vulnerability report, and functional
test results for each prompt. After multiple generations, they con-
ducted an evaluation based on prompt complexity and the model
used. The study found that LLMs struggled with increased complex-
ity, demonstrating low accuracy as contract intricacy grew. Most
compilation errors arose from incorrect type usage, like strings and
arrays, and issues related to the payable/call pattern.
The work described above highlights the challenges faced and
the different approaches used in literature to generate benchmarks.
Our work bridges some of the gaps by offering a possible solution.
3
Research Method
The following section presents the details of the study, highlighting
the main goal and its related research questions.
◎Goal of the study.
Our goal is to automatically inject vulnerability in smart con-
tracts to generate large and wide benchmarks that researchers
and developers can use to improve detection tool evaluation.
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
Iuliano et al.
Figure 1: Summary of the Research Method.
To achieve this goal, we developed MuSe, a tool to generate
vulnerable smart contracts that are challenging to detect automati-
cally. Inspired by mutation testing [21, 29], we extended the SuMo
mutation testing tool [2] to mutate Solidity smart contracts into
vulnerable mutants. To this end, we implemented mutation oper-
ators designed to inject known vulnerabilities by modifying the
smart contracts appropriately. Afterward, we analyzed to what
extent the injected vulnerabilities are challenging to detect by ob-
serving how well static analyzers can detect them. We leveraged
Slither [16], a popular static analyzer to detect vulnerabilities in
Solidity, and compared the results achieved by running it before
and after the mutation phase, assessing the presence of the injected
vulnerabilities and the performance of Slither in detecting them.
Our motivation is the critical role that smart contract security
plays in the blockchain and the limitations of existing vulnerability
detection tools in handling complex vulnerabilities. By combining
mutation security testing with static analysis, this work seeks to
provide empirical evidence of the strengths and weaknesses of
tools like Slither, offering insights that can drive the development
of more robust security solutions and methodologies for smart
contracts. Figure 1 depicts our research method.
As shown in Table 1, we selected six vulnerabilities [19, 31, 36]
to inject based on their relevance in the literature and the ability of
Slither to detect them with at least medium confidence: Unchecked
call return value, Unchecked send, Authentication through tx.origin,
Delegatecall to untrusted callee, and Unused return. These vulner-
abilities are among the top 15 most discussed in the literature, as
highlighted in the work of Zaazaa Oualid and El Bakkali Hanan [38].
Based on our goal, we formulate these research questions (RQs):
ü RQ1. To what extent are the mutation operators implemented
in MuSe generalizable?
𝑅𝑄1 allows us to assess the feasibility of introducing vulnerabili-
ties into real-world smart contracts and understand the generaliz-
ability of the mutation operators to inject a vulnerability.
ü RQ2. How can the mutants injected by MuSe be detected
through static analysis?
𝑅𝑄2 analyzes the performance of static analyzer when detecting
vulnerabilities injected into contracts. The goal of applying various
patterns to introduce vulnerabilities is twofold. On the one hand, we
aim to identify whether the injected vulnerabilities are challenging
to detect. On the other hand, we want to evaluate whether a static
analysis tool can identify these specific patterns, highlighting its
strengths and weaknesses.
3.1
Data Collection
To answer our research questions, we used the real-world dataset
smartbugs-wild1, which contains 47,398 smart contracts extracted
from the Ethereum network that have at least one transaction. We
ran Slither using SmartBugs [12], which allows us to parallelize its
execution using several Docker images and easily parse the results.
3.2
Mutation Operators
We extended SuMo [2], a mutation testing tool, by creating new
mutation operators focusing on security. The tool uses the solidity-
parser-antlr2, a parser built from a robust ANTLR4 grammar, which
generates an Abstract Syntax Tree of the code based on the Solidity
grammar. We implement a mutation operator for each vulnerability
to inject and leverage the parser to identify injection patterns where
the vulnerabilities can be injected. Table 1 maps the implemented
mutation operators, the vulnerability they inject, and the detector
Slither uses to identify them.
UC Operator. To inject the Unchecked low-level call return value,
we identify the possible instructions or statements to mutate. We
identified all the low-level call functions. We mutated the contract
by removing the controls whenever the return value of the functions
was checked using the require function or an if statement.
1https://github.com/smartbugs/smartbugs-wild
2https://github.com/solidity-parser/parser
Automated Vulnerability Injection in Solidity Smart Contracts:
A Mutation-Based Approach for Benchmark Development
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
Table 1: Vulnerabilities injected into smart contracts by MuSe.
Vulnerability
Description
Slither’s Detector
Operator
Unchecked low-level call return value
Low-level calls return false on failure instead of throwing exceptions, risking
critical vulnerabilities if unchecked.
unchecked-lowlevel
UC
Unchecked send
The send function returns false on failure without throwing an exception, risking
vulnerabilities if unchecked.
unchecked-send
US
Authentication via tx.origin
Using tx.origin for authorization risks vulnerabilities if an authorized account
interacts with a malicious contract.
tx-origin
TX
Unused return
The return value of an external call is not stored in a local or state variable.
unused-return
UR
Multiple calls in a loop
Calls inside a loop might lead to a denial-of-service attack.
calls-loop
CL
Delegatecall to untrusted callee
Delegatecall executes the code at the target address in the context of the calling
contract. It allows a SC to load code dynamically from a different address.
controlled-delegatecall
DTU
1 // Before UC mutation
2 function withdraw(uint amount) public {
3
require(msg.sender.call.value(amount)());
4 }
5 // After UC mutation
6 function withdraw(uint amount) public {
7
msg.sender.call.value(amount)();
8 }
US Operator. To inject Unchecked send, we identified all the send
functions and removed any control on the return value.
1 // Before US mutation
2 function sendEth(address payable giftee) public {
3
if (! giftee.send(1 ether)) {
4
revert("Send failed");
5
}
6 }
7 // After US mutation
8 function sendEth(address payable giftee) public {
9
giftee.send(1 ether)
10 }
TX Operator. Authentication through tx.origin was injected sub-
stituting the msg.sender variable with tx.origin. We mutate the
contracts when the msg.sender variable is used in a binary opera-
tion like “==” to check the ownership of the contract or a specific
address having some privileges or access to the asset.
1 // Before TX mutation
2 modifier onlyOwner () {
3
require(msg.sender == owner , "No owner"); _;
4 }
5 // After TX mutation
6 modifier onlyOwner () {
7
require(tx.origin == owner , "No owner"); _;
8 }
UR Operator. Unused return was injected in two cases. First, when
a binary operation like “=” assigns the return value of a function to
a variable. Second, when a variable is declared and then initialized
with the return value of a function. In both cases, we removed the
left side of the assignment and left the call function.
1 // Before UR mutation
2 function addNumbers(uint256 a, uint256 b) public {
3
c = SafeMath.add(a, b);
4 }
5 // After UR mutation
6 function addNumbers(uint256 a, uint256 b) public {
7
SafeMath.add(a, b);
8 }
CL Operator. Multiple calls in a loop were injected by wrapping
call, send, and transfer functions in a statement of 1,000 loops
implemented using the “for” construct.
1 // Before CL mutation
2 function payMember(address payable member) public {
3
require(member.send (0.1 ether);
4 }
5 // After CL mutation
6 function payMember(address payable member) public {
7
for (uint256 i = 1; i <= 5; i++) {
8
require(member.send (0.1 ether);
9
}
10 }
DTU Operator. Delegatecall to untrusted callee was injected by
introducing a new address variable and a new function that allows
users to replace the address variable with another. Each use of
delegatecall was mutated, replacing the address used to delegate
with the new personalizable address, which can be set to malicious.
1 // Before DTU mutation
2 function setFalseValue(address _address) public {
3
require(_address.delegatecall(
4
abi.encodeWithSignature("setFalse(uint256)")));
5 }
6 // After DTU mutation
7 address public delegate;
8 function setDelegate(address _delegate) public {
9
delegate = _delegate;
10 }
11 function setFalseValue(address _address) public {
12
require(delegate.delegatecall(
13
abi.encodeWithSignature("setFalse(uint256)")));
14 }
3.3
MuSe Validation
To ensure the validity of our results, we manually validated MuSe
on a statistically significant subset of mutated smart contracts with
a 95% confidence level and a 5% margin error. The tool generated
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
Iuliano et al.
350,716 mutated contracts, from which we randomly selected a
subset of 384 smart contracts, representing a statistically significant
sample size. Our validation process involved manually analyzing
each contract to verify whether SuMo injected the intended vulner-
abilities correctly. The procedure consisted of these steps:
(1) Compilation for Syntactical Correctness. Each selected smart
contract was compiled to ensure its syntactical correctness.
(2) Comparison with SuMo Logs. We compared the logs provided
by SuMo, which detail the applied mutation type and the
lines of code affected, against the corresponding mutated
smart contracts.
(3) Pattern Adherence Verification. We verified that the points
where vulnerabilities were injected adhered to the patterns
defined by the mutational operator. This step ensured that
SuMo identified the correct lines of code and statements for
injecting vulnerabilities.
(4) Modification Assessment. We examined SuMo’s modifications
or additions to the original code to determine whether the
injected vulnerabilities were accurately implemented.
A mutation was marked as correctly injected if (i) the mutated
contract contained the new lines of code introducing the target
vulnerability or (ii) existing lines of code were altered to render the
smart contract vulnerable to the intended issue.
The validation results show that our tool failed to inject vulnera-
bilities in 20 out of 384 smart contracts, or 5.21% of cases. The main
causes of these failures are exceptional cases that the mutational
operator does not adequately handle. One common issue arises
when the mutated statement contains a semicolon (“;”). Strings
including a semicolon within the mutated statement can interfere
with the operator logic, leading to unintended code truncation. As a
result, the generated mutant may have incorrect syntax, rendering
the contract uncompilable. Another cause of injection failure is
conflicts between the scopes of contract variables and the variables
introduced by the mutation. For example, the CL operator injects a
for loop that uses the variable uint i for iteration. If the mutation
is applied to a statement declaring a uint i variable, the compiler
cannot differentiate between the two variables, leading to a com-
pilation error. In all other cases, the mutation process successfully
injects the vulnerability without issues, producing valid mutants.
3.4
Replication Package
We have made MuSe publicly available on GitHub3, allowing re-
searchers and practitioners to replicate our study or utilize the tool
for their purposes. Additionally, we have uploaded the sample used
to manually validate the mutation operators.4
4
Empirical Results
We ran MuSe on the SmartBugs-wild dataset, applying the six
previously described mutation operators to each contract. Starting
from 47,398 smart contracts, we generated 350,493 vulnerable ones.
MuSe mutates a contract each time it matches the pattern of a
mutation operator. A contract could exhibit more than a pattern.
Table 2 shows the number of mutants generated by each operator
and highlights the number of contracts suitable to be mutated by
3https://anonymous.4open.science/r/MuSe/
4https://figshare.com/s/5473d31f2aead13d2fa8
each operator. MuSe mutated 41,337 out of 47,398 smart contracts,
about 87% of cases. The remaining 6,061 smart contracts were not
mutated for two reasons: (i) the absence of any patterns used by
mutation operators in 5,990 smart contracts and (ii) the invalid
content of the files for 71 of them, e.g., a JSON representation of
the smart contract instead of well-formatted Solidity code.
4.1
RQ1. To what extent are the mutation
operators implemented in MuSe
generalizable?
To answer RQ1, we observed the number of smart contracts that
could be correctly mutated. We aimed to understand how many
pattern occurrences applied by each mutation operator could be
injected in real-world scenarios.
Table 2: Results for mutation operators ordered by injection
rate, total number of generated mutants, and average injec-
tion rate of MuSe.
Operator
# Mutated SCs
# Mutants
Injection Rate
UR
33,910
213,912
71.50%
TX
32,250
65,825
68.00%
CL
26,604
61,687
56.00%
UC
4,094
4,992
8.60%
US
2,248
3,928
4.70%
DTU
113
149
0.23%
-
-
350,493
34.83%
As shown in Table 2, the most injectable vulnerability is the
unused return, with an injection rate of 71.5%. The patterns used
to inject vulnerabilities are not only common but also frequently
encountered in smart contracts, highlighting their prevalence in
typical contract design. On average, each smart contract contains
six occurrences of these patterns, reflecting their foundational role
in contract development. Patterns such as assignments, declarations,
and initializations are essential building blocks in smart contract
programming. However, their prevalence also increases the likeli-
hood of vulnerabilities arising from improper or unintended usage.
The second vulnerability, authorization via tx.origin, is injectable
in a real-world scenario in 68% of cases. The high injection rate
respects the frequency of the pattern of the TX operator in the
smart contracts. As described in Section 3, the TX operator mu-
tates a contract when the variable msg.sender is used to check the
ownership of a contract or the privilege of an address on the asset.
The injection rate for multiple calls in a loop is 56%, showing
only just over half of the contracts involve the use of a call, send,
or transfer function. The moderate injection rate highlights that
these functions are commonly employed in contracts but not exces-
sively frequent. In addition, 8% of the unmutated smart contracts
already contained the vulnerability.
Unchecked low-level call return value and unchecked send follow
with injection rates of 8.6% and 4.7%, respectively. The frequency of
a call function is almost double that of a send function. It is gener-
ally better to use call function than send but with some important
security and implementation considerations. The call function is
more flexible and allows specifying the amount of gas sent and calls
Automated Vulnerability Injection in Solidity Smart Contracts:
A Mutation-Based Approach for Benchmark Development
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
with data. Nevertheless, it is more vulnerable to reentrancy attacks
because it enables the receiving contract to execute arbitrary code.
Finally, delegatecall to untrusted callee has the lowest injection
rate. The use of the delegatecall function is relatively rare but
not negligible. It is mainly limited to specific use cases that require
advanced behavior, like implementing the Proxy pattern.
By analyzing the injection rate, it is possible to see that some
contracts do not have the necessary conditions to inject a given
vulnerability. Contracts that lack specific patterns, constructs, or
known Solidity functions are intrinsically safe from vulnerabilities
that try to exploit these elements. Overall, mutation operators can
increase the size of a dataset by 840% by creating new vulnerable
versions of smart contracts.
¤ RQ1 Summary. The results highlight that the patterns
needed to inject “unused return” (71.5%), “authorization via
tx.origin” (68%), and “multiple calls in a loop” (56%) are com-
mon, whereas those related to “unchecked low-level call return
values” (8.6%) and “unchecked send” (4.7%) are less prevalent.
The pattern for “delegatecall to untrusted callee” is rare (0.23%)
because the delegatecall function is used infrequently and only
in specific locations.
4.2
RQ2. How can the mutants injected by MuSe
be detected through static analysis?
To answer RQ2, first, we performed an initial detection with Slither
on the smartbugs-wild dataset and collected the findings for each
contract to have a baseline. Then, we ran Slither on the mutants
generated by MuSe. Under the assumption that the mutation oper-
ator correctly injects the vulnerability, we compared the detection
results before and after the mutation. Slither correctly detects a
mutant if it is labeled as vulnerable to the type of injected vulnera-
bility. In the case where the contract is already vulnerable and has
been mutated, we checked whether, in addition to the pre-existing
vulnerability, the injected vulnerability had also been detected by
analyzing the lines of code related to the injected vulnerability.
Slither successfully analyzed 335,234 mutants out of the 350,493
generated. Similarly, the execution on the smartbugs-wild dataset
failed on 2,700 smart contracts out of 47,398. After excluding the
failed executions, we mapped the results achieved by Slither on the
original contracts with the mutated ones, if present, and compared
the results for further analysis.
Table 3: Detection rate of injected vulnerability and overall
performance of Slither on the six considered vulnerabilities.
Vulnerability
TP
FN
Recall
FNR
UC
4,876
0
1.000
0.000
US
3,570
0
1.000
0.000
CL
45,261
10,563
0.810
0.189
UR
124,858
81,184
0.605
0.394
TX
21,765
42,937
0.336
0.663
DTU
15
134
0.100
0.899
-
200,345
134,818
0.597
0.402
As shown in Table 3, the results achieved by Slither against
the unchecked low-level call return value (UC) and unchecked send
(US) vulnerabilities are surprisingly high. The tool detected all mu-
tants with the injected vulnerability, showing a recall value equal to
1.00 in both cases. The two vulnerabilities are conceptually similar,
which shows that Slither implements strong detectors to check
whether the return values of the call and send functions are handled
correctly. Slither also performs very well in detecting multiple
calls in a loop (CL), with a recall of 0.81. While the detection is
strong, a noticeable portion of vulnerabilities remains undetected,
indicating room for improvement in the detection mechanism of
this vulnerability. Performance slows down on unused return (UR)
with a recall value of 0.63, suggesting that the detection mechanism
of these vulnerabilities might be less robust or prone to specific
limitations. Performance deteriorates on authorization via tx.origin
(TX), 0.33 of recall, pointing out significant gaps in detection capa-
bilities for this category. The worst result is detecting delegatecall
to untrusted callee (DTU) with a recall value of 0.10.
Overall, Slither achieved a recall value of 0.597. The recall
value indicates that while Slither effectively identifies certain
vulnerabilities, it fails to detect a significant portion (40.2%) of the
injected vulnerabilities. The result underscores the need to improve
static analysis tools or complement them with additional detection
techniques to enhance their accuracy and reduce false negatives.
¤ RQ2 Summary. Slither performs very well in some cases
but inconsistently across vulnerability types. While it excels in
detecting simple vulnerabilities like unchecked low-level call
return value and unchecked send, it struggles with more complex
vulnerabilities like authorization via tx.origin and delegatecall
to untrusted callee. Finally, Slither detected the 59.7% of the
injected vulnerabilities using MuSe.
5
Discussions and Limitations
In this section, we analyze the false negatives resulting from run-
ning Slither on the mutated smart contracts. We also discuss the
limitations of the study, offering insights about our mutation-based
approach to injecting vulnerabilities.
5.1
False Negative Analysis
We analyzed false negatives to extract information about the errors
achieved by Slither in detecting vulnerabilities. The way we con-
ducted the experiment allows us to extract only true positives (TP)
and false negatives (FN). The mutational operators, validated as
described in Section 3, inject the target vulnerability. Having Slither
results before and after the mutation, we can analyze whether the
tool detects the injected vulnerability (TP) or fails detection (FN).
Authorization via tx.origin (TX). For this vulnerability, the FNR
is 0.663. We found some patterns that Slither does not check when
detecting this vulnerability type. The most relevant and alarming
is related to the Solidity modifier. A Solidity modifier is a reusable
function that encapsulates and enforces reusable logic, such as ac-
cess control or precondition checks, simplifying code and improving
maintainability. Figure 2 shows the incorrect implementation of a
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
Iuliano et al.
modifier used to check the contract owner using tx.origin and
that Slither cannot label as vulnerable.
Figure 2: Incorrect modifier to restrict owner’s access.
1 modifier onlyOwner () {
2
require(tx.origin == owner); _;
3 }
Another scenario that Slither fails to detect is when the clause
to check the contract ownership is composed of several conditions
combined using AND/OR operators. For example, Figure 3 shows
the addPartner function, which allows the caller to add a new
partner. The clause in the require function ensures that only
authorized users (_dev or _owner) can call this function. The TX
operator mutated the second occurrence of msg.sender, replacing
it with tx.origin. Using tx.origin for authorization introduces
a security risk because it refers to the address that initiated the
transaction, even if there were intermediate contract calls.
Figure 3: Clause with two conditions combined using OR.
1 function addPartner(address _partner) public {
2
require ((msg.sender == _dev) || (tx.origin ==
_owner));
3
exchangePartners[_partner] = true;
4 }
Unused return (UR). Slither achieved a false negative rate of
0.394 on this vulnerability. Analysis of false negatives produced
an interesting finding. The unused return is correctly detected
by Slither when the function being called in the contract is a
library function; see Figure 4a. However, the detection fails when
the function called is inherited from a contract; see Figure 4b.
Multiple calls in a loop (CL). Slither achieved a false negative
rate of 0.189 on this vulnerability. The mutated smart contracts are
characterized by several aspects, like function visibility and the
presence of modifiers. In addition, the mutation can be injected
into the function bodies or in-depth, e.g., nested into an if state-
ment. Finally, the statement that undergoes the mutation may be
contained in another statement. Nevertheless, we could not find a
recurrent pattern in the inconsistent behavior of the detection tool.
Delegatecall to untrusted callee (DTU). The analysis of false nega-
tives revealed two interesting aspects. On the one hand, we noticed
that Slither fails to detect simple cases like the one shown in
Figure 5. On the other hand, we noticed that most delegatecall
functions are used in the constructor, which is invoked only at
deployment time. Therefore, it is impossible to inject an instance of
the vulnerability into the constructor that is exploitable. In addition,
the delegatecall is often rewritten using the assembly to create
a custom function to delegate. Although the mutational operator
showed a poor injection rate (0.23%), the impact of this vulnerability
Figure 4: Two examples of unused return.
(a) Unused return detected at line 10.
1 library SafeMath {
2
function add(uint256 a, uint256 b) internal pure
returns (uint256) {
3
uint256 c = a + b;
4
require(c >= a, "addition overflow");
5
return c;
6
}
7 }
8 contract SafeMathExample {
9
function addNumbers(uint256 a, uint256 b) public {
10
SafeMath.add(a, b);
11
}
12 }
(b) Unused return not detected at line 10.
1 contract SafeMath {
2
function add(uint256 a, uint256 b) internal pure
returns (uint256) {
3
uint256 c = a + b;
4
require(c >= a, "addition overflow");
5
return c;
6
}
7 }
8 contract SafeMathExample is SafeMath{
9
function addNumbers(uint256 a, uint256 b) public {
10
SafeMath.add(a, b);
11
}
12 }
is catastrophic if misused in the Proxy pattern. The Proxy pattern
uses delegatecall to separate state and logic, allowing updates
without losing contract data.
Figure 5: Delegatecall to untrusted callee.
1 address public delegate;
2 function setDelegate(address _delegate) public {
3
delegate = _delegate;
4 }
5 function upgradeAndCall(address implementation ,
bytes calldata data) external payable ifAdmin {
6
_upgradeTo(implementation);
7
(bool success ,) = delegate.delegatecall(data);
8
require(success);
9 }
5.2
Mutations and Side Effects
This section analyzes the side effects that mutation testing could
have on smart contracts. We observed that injecting vulnerabilities
through mutational operators often introduces side effects, like
code smells. We relied on the Slither official documentation5 to
5https://github.com/crytic/slither/wiki/Detector-Documentation
Automated Vulnerability Injection in Solidity Smart Contracts:
A Mutation-Based Approach for Benchmark Development
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
understand the functionality of its detectors and the labels they use
for vulnerabilities.
Unchecked low-level call return value (UC) and Unchecked send
(US). Both vulnerabilities are injected using the same approach.
The differences are in the conditions used to inject them, but the
type of mutation is quite similar. Indeed, the side effects that affect
the injection of these vulnerabilities are the same and occur in
75% of the cases. A deprecated-statement occurs when outdated
constructs are used in a contract, e.g., throw instead of revert. The
two mutation operators, UC and US, remove checks on the return
value of a function. When these checks are implemented using an
if statement, the true branch typically reverts the execution of the
function using a throw construct. By eliminating the check on the
return value, the condition to trigger the throw is also removed,
resulting in the removal of the throw from the mutated code. Since
throw is deprecated, its removal eliminates using a deprecated
statement in 3,772 cases.
Analyzing cases (3,697) where the side effect is the missing-zero-
check reveals an interesting finding. This issue arises when no
validation is performed to ensure that an address, either used as an
argument or on which a function is invoked, is not the zero address
(address(0)). The zero address is often used as a burn address for
tokens. When the mutation operator modifies the condition by mov-
ing it outside an if statement, Slither identifies the presence of a
missing zero check. Although the statement remains unchanged,
its move outside the if condition allows Slither to detect an issue
that should be detected regardless of its position in the code.
Authorization via tx.origin (TX). The mutation operator intro-
duces some unexpected behavior in 70% of the cases. When a func-
tion performs critical mathematical operations on the contract bal-
ance, these operations must be signaled by throwing an event. Func-
tions that perform mathematical operations on the contract balance
and do not emit an event are labeled by Slither as events-math.
When the TX operator mutates them, Slither stops labeling them
vulnerable in 2,581 cases. The mutation does not involve the mathe-
matical operations in the contract; therefore, they should continue
to be labeled as events-math. One possible explanation could be the
absence of the msg.sender variable, which seems necessary for a
math operation to be signaled by an event.
A similar case involves events-access, emitting an event when-
ever the msg.sender variable is used to change the owner of the
contract. In 2,398 cases, it is understandable to eliminate this vul-
nerability when msg.sender is replaced with tx.origin.
The last side effect, called by Slither suicidal, occurs 1,058
times and concerns using the selfdestruct function, which should
be restricted to the contract owner. When the mutation replaces
msg.sender with tx.origin in the modifier used to give access to
selfdestruct, Slither correctly detects the misuse of selfdestruct.
The detector applies the appropriate control, discussed in the False
Negative Analysis section—specifically, verifying the contract owner
using a modifier. However, the detection behavior appears incon-
sistent depending on the functionality implemented within the
function. For instance, if the function includes the invocation of
a well-known operation, such as selfdestruct, Slither verifies
access to the function by analyzing the associated modifier. Con-
versely, Slither fails to check the modifier if the function does
not include recognized features. This inconsistency highlights a
significant issue: regardless of the functionality implemented, an in-
correct modifier implementation should always be detected, which
is especially critical when the modifier is responsible for ensuring
that the function invoker is the contract owner.
Unused return (UR). We noted that the mutation operator intro-
duces other vulnerabilities or code smells alongside the injected
vulnerabilities in 67% of the cases. For example, in some cases,
the operator splits a single statement containing declaration and
initialization into two separate statements, one for declaring the
variable and one for initializing it. The initialization is then made
vulnerable if the return value of a function is used to initialize the
variable. The return value is not assigned to the variable, thus cre-
ating an unused return and leaving the variable uninitialized. The
side effect of this mutation is the creation or removal of some code
smells. The mutation has introduced uninitialized-state (27,890),
initialized-local (97,932), or constable-states (28,352). While not ex-
pected, these side effects are understandable, given the type of
mutation introduced. Variables uninitialized due to the mutation
introduce uninitialized-state and uninitialized-local, depending on
the scope of the uninitialized variable. In cases where the variable
affected by the mutation undergoes no other changes in the code,
we have a constable-states, a variable not declared constant.
Interesting side effects are those that remove some vulnerabilities
or code smells present before the mutation and that were removed
by the mutation. For example, divide-before-multiply (-3,913) is
resolved when the return value of multiplication is not assigned to
the variable in which the result of the previous division was saved.
Another is incorrect-equality (-3,851), which occurs when a variable
to which the contract balance is assigned is used in a strict equality.
Removing the contract balance assignment from the variable also
removes the vulnerability. Then we have reentrancy-benign (-5,340),
reentrancy-no-eth (-3,191), reentrancy-eth (-830), and reentrancy-
unlimited-gas (-818), all of which are removed from the mutation
because by not assigning the return value to a variable involved
in reentrancy, it does not change state and does not create the
preconditions for reentrancy. Additionally, controlled-array-length
was added and removed depending on the case. It was introduced
in 371 mutants but removed in 790. It was eliminated when the
array index was not initialized using the return value of a function
and introduced when the index remained uninitialized.
Interestingly, some vulnerabilities or code smells emerged, al-
though not directly connected to the mutation. For example, external-
function or timestamp were introduced by the UR operator but ap-
peared in different lines of code than those affected by the mutation.
The mutated statement and the statement affected by the newly
emerged vulnerability shared no common functions or variables.
We hypothesize that these instances represent false positives, po-
tentially caused by conflicts within the Slither execution flow or
interactions between its detectors.
Multiple calls in a loop (CL). The CL operator introduces side
effects that are strictly related to the vulnerability injected in 30% of
the cases. It introduces msg-value-loop (6617), costly-loop (451), and
cyclomatic-complexity (126). The first case occurs when msg.value
is used in mathematical operations inside a loop. The second occurs
when we use costly operations inside a loop that might waste gas, so
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
Iuliano et al.
optimizations are justified. The last case occurs when the cyclomatic
complexity is higher than 10. In this case, all side effects are closely
related to the mutation and, in some cases, are unavoidable.
Delegatecall to untrusted callee. The DTU operator introduces
missing-zero-check and naming-convention. The mutation intro-
duces a new address called delegate and a new function to set the ad-
dress called setDelegate. The setter function does not check that the
address is not the zero address (address(0)). The naming-convention
is a warning that refers to the address argument of setDelegate; it is
not in mixedCase violating the Style Guide of Solidity6.
¤ Discussion Summary. Slither exhibits inconsistent behav-
ior and misses vulnerabilities in specific scenarios, suggesting
potential gaps in its detection algorithms. Furthermore, its abil-
ity to detect some warnings depends on the presence of specific
variables, even when these variables are not directly related to
the core logic of the vulnerability.
6
Threats To Validity
This section describes the potential threats to validity, including
construct, internal, external, and conclusion validity.
Internal Validity. The vulnerability injection approach threatens
the validity of our study. The initial version of our mutation opera-
tors exhibits side effects, which can impact the results. As discussed
in Section 5, some side effects are understandable and legitimate,
others are unavoidable, and some can be mitigated by refining the
mutation operators.
Another threat concerns the relationship between the injected
vulnerability and its side effects. Sometimes, the code smells that
emerged as side effects were not attributable to the injected vulner-
ability. Mutations could introduce code smells independently of the
injected vulnerability. We have discussed all observed side effects
and explained their underlying causes, aiming to increase aware-
ness about their proper usage and highlight areas for improvement
in the mutation operators.
External Validity. We relied on Slither [16] as the static analysis
tool to detect injected vulnerabilities, which may limit the general-
izability of our findings regarding static analysis tools. However,
Slither is one of the most widely used tools in smart contract analy-
sis, recognized for its fast execution time and popularity in academic
and industrial settings. It is open-source, actively maintained, and
readily accessible, making it ideal for this study. Importantly, the
experiment could be easily extended to evaluate additional tools.
Another external threat to validity is the focus on only six vulner-
abilities. However, these vulnerabilities were selected because they
are among the most frequently discussed in the literature [38] and
represent common security issues in smart contracts. In addition,
the tool can be easily extended by implementing new mutation
operators to inject new vulnerabilities.
Construct Validity. A threat to construct validity concerns the
analysis of false negatives. An exhaustive analysis of all false neg-
atives would take too much time and might reveal cases we did
6https://docs.soliditylang.org/en/latest/
not consider. We restricted the false negative analysis to observ-
ing a statistically significant sample from which we extracted the
findings reported in the paper.
A second threat concerns using SmartBugs [12], which accel-
erates the experiments and simplifies the analysis of the results.
We followed its official documentation, which lists some issues and
discusses how to mitigate them. Nevertheless, we acknowledge that
our study could have been threatened by relying on this framework.
Another threat to construct validity relates to MuSe implemen-
tation. Our tool is based upon SuMo [2], a widely recognized and
tested mutation testing tool that has already been extended for re-
gression mutation testing [4]. However, by relying on SuMo, MuSe
may inherit its defects, potentially impacting the performance of
MuSe. To mitigate the potential issue, we manually validated our
extensions using a statistically significant set of smart contracts
composed of a random subset of the smartbugs-wild dataset.
Lastly, choosing the dataset for our experiments and validation
introduces a potential threat to external and construct validity. We
selected the smartbugs-wild dataset [15], one of the most widely
used datasets in the literature. The dataset is recognized for its
considerable size and frequent use in empirical investigations, rein-
forcing its relevance and reliability for our study.
7
Conclusion
This paper explores a mutation-based approach to inject known
vulnerabilities into smart contracts to generate new benchmarks to
evaluate vulnerability detection tools. We proposed MuSe, a muta-
tion seeding tool that implements mutational operators that identify
the appropriate pattern in which to inject the mutation. MuSe is ca-
pable of injecting six vulnerabilities. An injection rate characterizes
each vulnerability since not all smart contracts have the conditions
to be mutated and thus be vulnerable to a problem. MuSe has been
validated and is easily extended by adding new mutational opera-
tors. Slither, a static analyzer, analyzed smart contracts generated
by mutation. The results showed gaps in Slither detectors, show-
ing the current limitations of static analyzers and leaving room for
improvement. Our study highlights that using new benchmarks
generated through a mutation-based approach can improve the
validation of static analysis tools.
As part of our future work, we aim to enhance MuSe by intro-
ducing additional mutation operators and refining existing ones
to reduce unintended behavior. Our ultimate goal is to develop a
fully automated tool capable of generating vulnerable smart con-
tracts starting from an initial set of smart contracts. We plan to
address the gaps identified by Bobadilla et al. [6] by creating new
datasets. The current literature offers limited labeled benchmarks,
most of which consist of smart contracts written in older versions
of Solidity. Using our mutation seeding tool, we intend to inject
vulnerabilities into audited and updated smart contracts to produce
new, labeled benchmarks and fill these critical gaps.
Acknowledgments
Finanziato dall’Unione Europea - Next Generation EU, Missione 4
Componente 1 CUP D53D23008400006.
Automated Vulnerability Injection in Solidity Smart Contracts:
A Mutation-Based Approach for Benchmark Development
EASE 2025, 17–20 June, 2025, Istanbul, Türkiye
References
[1] Nicola Atzei, Massimo Bartoletti, and Tiziana Cimoli. 2017. A survey of attacks
on ethereum smart contracts (sok). In Principles of Security and Trust: 6th Interna-
tional Conference, POST 2017, Held as Part of the European Joint Conferences on
Theory and Practice of Software, ETAPS 2017, Uppsala, Sweden, April 22-29, 2017,
Proceedings 6. Springer, 164–186.
[2] Morena Barboni, Andrea Morichetta, and Andrea Polini. 2021. SuMo: A Mutation
Testing Strategy for Solidity Smart Contracts. In 2021 IEEE/ACM International
Conference on Automation of Software Test (AST). 50–59. doi:10.1109/AST52587.
2021.00014
[3] Morena Barboni, Andrea Morichetta, and Andrea Polini. 2022. SuMo: A mutation
testing approach and tool for the Ethereum blockchain. Journal of Systems and
Software 193 (2022), 111445. doi:10.1016/j.jss.2022.111445
[4] Morena Barboni, Andrea Morichetta, Andrea Polini, and Francesco Casoni. 2024.
ReSuMo: a regression strategy and tool for mutation testing of solidity smart
contracts. Software Quality Journal 32, 1 (2024), 225–253.
[5] Kent Beck. 2022. Test driven development: By example. Addison-Wesley Profes-
sional.
[6] Sofia Bobadilla, Monica Jin, and Martin Monperrus. 2025. Do Automated Fixes
Truly Mitigate Smart Contract Exploits? arXiv preprint arXiv:2501.04600 (2025).
[7] Vitalik Buterin et al. 2014. Ethereum white paper: a next generation smart
contract & decentralized application platform. First version 53 (2014).
[8] Patrick Chapman, Dianxiang Xu, Lin Deng, and Yin Xiong. 2019. Deviant: A
Mutation Testing Tool for Solidity Smart Contracts. In 2019 IEEE International
Conference on Blockchain (Blockchain). 319–324. doi:10.1109/Blockchain.2019.
00050
[9] Ting Chen, Yuxiao Zhu, Zihao Li, Jiachi Chen, Xiaoqi Li, Xiapu Luo, Xiaodong
Lin, and Xiaosong Zhange. 2018. Understanding Ethereum via Graph Analysis. In
IEEE INFOCOM 2018 - IEEE Conference on Computer Communications. 1484–1492.
doi:10.1109/INFOCOM.2018.8486401
[10] Hanting Chu, Pengcheng Zhang, Hai Dong, Yan Xiao, and Shunhui Ji. 2024. SGDL:
Smart contract vulnerability generation via deep learning. Journal of Software:
Evolution and Process 36, 12 (2024), e2712.
[11] Etienne Daspe, Mathis Durand, Julien Hatin, and Salma Bradai. 2024. Bench-
marking Large Language Models for Ethereum Smart Contract Development.
1–4. doi:10.1109/BRAINS63024.2024.10732686
[12] Monika di Angelo, Thomas Durieux, João F. Ferreira, and Gernot Salzer. 2023.
SmartBugs 2.0: An Execution Framework for Weakness Detection in Ethereum
Smart Contracts. In 38th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE Computer Society, 2102–2105. doi:10.1109/
ASE56229.2023.00060
[13] Monika Di Angelo and Gernot Salzer. 2019. A survey of tools for analyzing
ethereum smart contracts. In 2019 IEEE international conference on decentralized
applications and infrastructures (DAPPCON). IEEE, 69–78.
[14] Ardit Dika and Mariusz Nowostawski. 2018. Security Vulnerabilities in Ethereum
Smart Contracts. In 2018 IEEE International Conference on Internet of Things
(iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE
Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData).
955–962. doi:10.1109/Cybermatics_2018.2018.00182
[15] Thomas Durieux, João F Ferreira, Rui Abreu, and Pedro Cruz. 2020. Empirical
review of automated analysis tools on 47,587 ethereum smart contracts. In Pro-
ceedings of the ACM/IEEE 42nd International conference on software engineering.
530–541.
[16] Josselin Feist, Gustavo Grieco, and Alex Groce. 2019. Slither: a static analysis
framework for smart contracts. In 2019 IEEE/ACM 2nd International Workshop on
Emerging Trends in Software Engineering for Blockchain (WETSEB). IEEE, 8–15.
[17] Asem Ghaleb and Karthik Pattabiraman. 2020. ‘. In Proceedings of the 29th ACM
SIGSOFT International Symposium on Software Testing and Analysis (Virtual Event,
USA) (ISSTA 2020). Association for Computing Machinery, New York, NY, USA,
415–427. doi:10.1145/3395363.3397385
[18] Ákos Hajdu, Naghmeh Ivaki, Imre Kocsis, Attila Klenik, László Gönczy, Nuno
Laranjeiro, Henrique Madeira, and András Pataricza. 2020. Using Fault Injection
to Assess Blockchain Systems in Presence of Faulty Smart Contracts. IEEE Access
8 (2020), 190760–190783. doi:10.1109/ACCESS.2020.3032239
[19] Gerardo Iuliano and Dario Di Nucci. 2024.
Smart Contract Vulnerabil-
ities, Tools, and Benchmarks: An Updated Systematic Literature Review.
arXiv:2412.01719 [cs.SE] https://arxiv.org/abs/2412.01719
[20] Y. Ivanova and A. Khritankov. 2020. RegularMutator: A Mutation Testing Tool
for Solidity Smart Contracts.
Procedia Computer Science 178 (2020), 75–83.
doi:10.1016/j.procs.2020.11.009 9th International Young Scientists Conference in
Computational Science, YSC2020, 05-12 September 2020.
[21] Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development
of Mutation Testing. IEEE Trans. Softw. Eng. 37, 5 (Sept. 2011), 649–678. doi:10.
1109/TSE.2010.62
[22] Bo Jiang, Ye Liu, and Wing Kwong Chan. 2018. Contractfuzzer: Fuzzing smart
contracts for vulnerability detection. In Proceedings of the 33rd ACM/IEEE inter-
national conference on automated software engineering. 259–269.
[23] Satpal Singh Kushwaha, Sandeep Joshi, Dilbag Singh, Manjit Kaur, and Heung-No
Lee. 2022. Systematic Review of Security Vulnerabilities in Ethereum Blockchain
Smart Contract. IEEE Access 10 (2022), 6605–6621. doi:10.1109/ACCESS.2021.
3140091
[24] Satpal Singh Kushwaha, Sandeep Joshi, Dilbag Singh, Manjit Kaur, and Heung-No
Lee. 2022. Systematic review of security vulnerabilities in ethereum blockchain
smart contract. IEEE Access 10 (2022), 6605–6621.
[25] Chao Liu, Han Liu, Zhao Cao, Zhong Chen, Bangdao Chen, and Bill Roscoe.
2018. Reguard: finding reentrancy bugs in smart contracts. In Proceedings of the
40th International Conference on Software Engineering: Companion Proceeedings.
65–68.
[26] Loi Luu, Duc-Hiep Chu, Hrishi Olickel, Prateek Saxena, and Aquinas Hobor.
2016. Making smart contracts smarter. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security. 254–269.
[27] Muhammad Izhar Mehar, Charles Louis Shier, Alana Giambattista, Elgar Gong,
Gabrielle Fletcher, Ryan Sanayhie, Henry M Kim, and Marek Laskowski. 2019.
Understanding a revolutionary and flawed grand experiment in blockchain: the
DAO attack. Journal of Cases on Information Technology (JCIT) 21, 1 (2019),
19–32.
[28] Alexander Mense and Markus Flatscher. 2018.
Security vulnerabilities in
ethereum smart contracts. In Proceedings of the 20th international conference
on information integration and web-based applications & services. 375–380.
[29] Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark
Harman. 2019. Chapter Six - Mutation Testing Advances: An Analysis and Survey.
Advances in Computers, Vol. 112. Elsevier, 275–378. doi:10.1016/bs.adcom.2018.
03.015
[30] Reza M Parizi, Ali Dehghantanha, Kim-Kwang Raymond Choo, and Amritraj
Singh. 2018. Empirical vulnerability analysis of automated smart contracts
security testing on blockchains. arXiv preprint arXiv:1809.02702 (2018).
[31] Heidelinde Rameder, Monika Di Angelo, and Gernot Salzer. 2022. Review of
automated vulnerability analysis of smart contracts on Ethereum. Frontiers in
Blockchain 5 (2022), 814977.
[32] Meng Ren, Zijing Yin, Fuchen Ma, Zhenyang Xu, Yu Jiang, Chengnian Sun,
Huizhong Li, and Yan Cai. 2021. Empirical evaluation of smart contract testing:
What is the best choice?. In Proceedings of the 30th ACM SIGSOFT international
symposium on software testing and analysis. 566–579.
[33] Noama Fatima Samreen and Manar H Alalfi. 2021. Smartscan: an approach
to detect denial of service vulnerability in ethereum smart contracts. In 2021
IEEE/ACM 4th International Workshop on Emerging Trends in Software Engineering
for Blockchain (WETSEB). IEEE, 17–26.
[34] Sarwar Sayeed, Hector Marco-Gisbert, and Tom Caira. 2020. Smart contract:
Attacks and protections. Ieee Access 8 (2020), 24416–24427.
[35] Fabian Schär. 2021. Decentralized finance: On blockchain-and smart contract-
based financial markets. FRB of St. Louis Review (2021).
[36] Fernando Richter Vidal, Naghmeh Ivaki, and Nuno Laranjeiro. 2024. OpenSCV:
an open hierarchical taxonomy for smart contract vulnerabilities. Empirical
Software Engineering 29, 4 (2024), 101.
[37] Xiaoyin Wang, Jiaze Sun, Chunyang Hu, Panpan Yu, Bin Zhang, and Donghai
Hou. 2022. EtherFuzz: mutation fuzzing smart contracts for TOD vulnerabil-
ity detection. Wireless Communications and Mobile Computing 2022, 1 (2022),
1565007.
[38] Oualid Zaazaa and Hanan El Bakkali. 2023. A systematic literature review of
undiscovered vulnerabilities and tools in smart contract technology. Journal of
Intelligent Systems 32 (09 2023). doi:10.1515/jisys-2023-0038
[39] Zibin Zheng, Jianzhong Su, Jiachi Chen, David Lo, Zhijie Zhong, and Mingxi Ye.
2024. DAppSCAN: Building Large-Scale Datasets for Smart Contract Weaknesses
in DApp Projects. IEEE Trans. Softw. Eng. 50, 6 (June 2024), 1360–1373. doi:10.
1109/TSE.2024.3383422
[40] Weiqin Zou, David Lo, Pavneet Singh Kochhar, Xuan-Bach Dinh Le, Xin Xia,
Yang Feng, Zhenyu Chen, and Baowen Xu. 2021. Smart Contract Development:
Challenges and Opportunities. IEEE Transactions on Software Engineering 47, 10
(2021), 2084–2106. doi:10.1109/TSE.2019.2942301

==================================================

==================================================
Paper 3
Title: VR-based Intervention for Perspective Change: A Case to Investigate Virtual Materiality
Abstract: This paper addresses the concept of materiality in virtual environments,
which we define as being composed of objects that can influence user experience
actively. Such virtual materiality is closely related to its physical
counterpart, which is discussed in theoretical frameworks such as
sociomateriality and actor-network theory. They define phenomena in terms of
the entanglement of human and non-human elements. We report on an early
investigation of virtual materiality within the context of reflection and
perspective change in nature-based virtual environments. We considered the case
of university students reflecting on the planning and management of their
theses and major projects. Inspired by nature's known positive cognitive and
affective effects and repeated questioning processes, we established a virtual
reflection intervention to demonstrate the environmental mechanisms and
material characteristics relevant to virtual materiality. Our work is a
preliminary step toward understanding virtual materiality and its implications
for research and the design of virtual environments.
PDF URL: http://arxiv.org/pdf/2504.16031v1
Submission Date: 2025-04-22 16:53:21+00:00

Full Content:
VR-based Intervention for Perspective Change: A Case to 
Investigate Virtual Materiality 
Ali Arya* 
Anthony Scavarelli* 
Dan Hawes* 
Luciara Nardon**
 
Figure 1: The menu screen from the Nature Treks VR app used by the participants 
 
ABSTRACT 
This paper addresses the concept of materiality in virtual 
environments, which we define as being composed of objects that 
can influence user experience actively. Such virtual materiality is 
closely related to its physical counterpart, which is discussed in 
theoretical frameworks such as sociomateriality and actor-network 
theory. They define phenomena in terms of the entanglement of 
human and non-human elements. We report on an early 
investigation of virtual materiality within the context of reflection 
and perspective change in nature-based virtual environments. We 
considered the case of university students reflecting on the 
planning and managing of their theses and major projects. 
Inspired by nature’s known positive cognitive and affective 
effects and repeated questioning processes, we established a 
virtual reflection intervention to demonstrate the environmental 
mechanisms and material characteristics relevant to virtual 
materiality. Our work is a preliminary step toward understanding 
virtual materiality and its implications for research and the design 
of virtual environments.  
CCS CONCEPTS 
•Human-centered 
computing~Human 
computer 
interaction 
(HCI)~Empirical studies in HCI  
•Human-centered 
computing~Human 
computer 
interaction 
(HCI)~Interaction paradigms~Virtual reality 
KEYWORDS 
virtual reality, virtual environment, reflection, perspective 
1 Introduction 
There is a large body of research that highlights the effect of the 
environment on cognition and emotion [5,33] and a growing 
number of studies that emphasize the active role of material 
objects and their effect on human experience [32,41], proposing 
the notion of sociomateriality [32]. Sociomateriality sees the 
material world’s effects as significant as the social context. 
According to this research, the material world is not just a 
collection of objects that humans manipulate but a set of 
(somewhat) active agents that contribute to user experiences 
through entanglement and intra-action (a term that aims to 
highlight a bi-directional relationship that can go beyond human-
centred manipulations and interactions) [32]. Materiality refers to 
the collective characteristics of environmental objects that give 
them a level of agency to initiate and participate in the intra-
actions with human agents. For example, Hultin [20], when 
observing the interaction of newcomers with immigration officers, 
noticed how setting up a glass shield halfway through the research 
study significantly affected the newcomers’ experience without 
having been considered in initial research assumptions.  
Popularized by many researchers from psychology [13] to 
design [31] and human-computer interaction (HCI) [12], an 
affordance is a property of an object or environment that is 
“compatible and relevant to people’s interaction” [12]. Virtual 
reality (VR) has a unique combination of affordances such as 
 
*   School of Information Technology, Carleton University, Ottawa, Canada 
** Sprott School of Business, Carleton University, Ottawa, Canada 
arXiv Pre-print 
Corresponding Author: Ali Arya, arya@carleton.ca 
GI 2025, May 2025, Kelowna, BC, Canada 
Author et al. 
 
 
 
immersion, visualization, and interaction [37] [39], which have 
been shown to make VR an effective tool in applications such as 
learning [15], well-being [40], and increasing awareness and 
empathy [17]. In this paper, we address virtual materiality as a 
less-investigated affordance of virtual environments (VEs), 
defined as being composed of objects able to influence user 
experience in an active way, closely related to its physical 
counterpart. Our notion of virtual materiality differs from those 
based on the common usage of the word material in 3D graphics, 
referring to the visual surface properties of a 3D model [4]. Also, 
we use the terms VR, VE, and 3D VE interchangeably, even 
though they may have different meanings in other contexts. 
There is considerable research on the cognitive and emotional 
effects of VEs on users [8,10,11,15,16,17,18,19]. However, most 
of them investigated environmental factors and elements that were 
specifically designed to influence users and their performance of 
specific tasks, such as learning a topic, playing a game, or 
exploring space. While entanglement with material (and other 
non-human) elements is suggested as the basis for the fourth wave 
of HCI research and design [9], investigating the effect of virtual 
environments on users’ cognition, emotion, and experience has 
been mostly limited to objects and settings that are controlled and 
planned for a specific effect. This limitation does not properly 
account for and allow the study of materiality in virtual 
environments and how it can initiate different, unplanned, and 
unexpected experiences, as the environmental parameters are 
highly controlled, and observations are for pre-defined metrics.  
We propose the notion of virtual materiality defined as an 
affordance of VR that offers a virtual counterpart to the intra-
active material objects in physical environments. Such an 
affordance suggests that a VE can be considered as a complex set 
of agents affecting the user experience beyond the initial intent 
and in unexpected manners. For example, a virtual room may 
have been created for a virtual classroom application, but the 
design may have initiated certain cognitive or affective processes 
in users that were not expected or planned [16]. While materiality 
in physical environments has been investigated and examples of it 
have been mentioned in virtual environments, there is not 
sufficient knowledge on whether or not virtual materiality acts in 
a way similar to its physical counterpart. The unique 
characteristics of VR/VE suggest that the mechanisms for virtual 
materiality may be different from the physical one or have unique 
and different features, which warrants specific and dedicated 
research. The study of virtual materiality requires extensive 
research on the effects of existing and newly designed VEs on 
users to understand how VE objects could initiate unplanned 
influences on users performing different tasks. While such a 
characteristic can complicate the design of VEs, it can offer new 
possibilities, as do the physical counterparts in the physical world 
vs. controlled lab environments.  
We briefly discuss potential research and design implications 
of virtual materiality in this paper, after presenting our research 
study, as a preliminary step towards investigating materiality in 
virtual environments. Our study was focused on a specific set of 
VEs and user tasks to be performed in them. We chose reflection 
and perspective change, through an established repeated 
questioning intervention [44], for university students working on 
their theses or major projects, as an initial context of our 
investigation. This choice was motivated by various cognitive and 
affective processes that could be affected by the environment and 
the existing literature providing a theoretical grounding for such 
effects [1,3,5,6]. We also chose different nature VEs as examples 
to use due to research suggesting generic natural settings can act 
as catalysts for reflection and overall well-being [6,11,29]. By 
generic, we mean general purpose, not planned for the specific 
task. Examples can be using a common environment (e.g., a park 
or other general-purpose setting) for a particular activity (e.g., 
learning) or being exposed to dynamic, unpredictable, and 
unintended situations (e.g., seeing a flock of birds). Our primary 
research question is how (through what mechanisms and material 
characteristics) VEs affect participants’ perspectives and 
approaches on a topic/problem they are thinking about (or are 
struggling with) and how they approach and solve it. As a 
secondary question, we also wanted to see if VE-based 
intervention has the reflective effect of traditional methods and 
allows users to go through the expected stages. More explicitly, 
exploring the reflective effect will enable us to verify that the 
environmental effects were, in fact, positive and not a distraction 
in the process. Due to the generic nature of the environments, we 
hope that our findings, as initial as they may be, can help 
understand how materiality works in VEs and how it can be 
considered an additional affordance. 
2 Related Work 
Many VR researchers have directly investigated the effect of 
virtual environments on thinking and learning. For example, 
Rizzo et al. [34] discuss the effect of VR in clinical situations and 
how it can be used for cognitive rehabilitation, and other 
researchers have investigated using VR as an experiential learning 
tool [7,36]. Most of these examples focus on environments 
intentionally designed to assist with primary tasks such as 
learning [7]. Research on the effect that random and generic parts 
of a VE can have on users is mostly focused on natural settings, 
owing to the body of research related to the positive effect of 
nature on well-being [1,6] and thinking/cognition [3]. Research 
suggests that virtual environments can have spiritual and 
emotional effects similar to those of their physical counterparts 
[30]. VR researchers have investigated the effects of virtual nature 
exposure and showed the potential positive role such exposure can 
play in emotional well-being [8,10,21,25,29,38]. Despite these 
examples, the effect of generic environments (such as nature) on 
thinking and perspective change is not adequately investigated in 
VR. Investigation of virtual materiality should go beyond overall 
well-being and emotions and consider more specific aspects of the 
experience, such as cognition and behaviour, and the mechanisms 
through which the VE affects them.  
While theories such as situated cognition do discuss the 
dependency of cognition on context, alternative perspectives, such 
as sociomateriality [2,32], go beyond dependence and suggest 
VR-based Interventions for Perspective Change 
GI 2025, May 2025, Kelowna, BC, Canada 
 
 
entanglement., i.e., a close inter-dependence between human(s) 
(i.e., social) and objects including nature, technology, etc. (i.e., 
material). Such interdependence requires the elements to be 
studied together as a phenomenon rather than in isolation. 
Entanglement and related theories, such as Actor-Network Theory 
(ATN) [23], are considered as the basis of a fourth wave for HCI 
[9]. Sociomateriality offers a new ontological perspective that 
gives material objects a key agential ability, and as such is a 
suitable framework for studying material characteristics and 
effects of VEs. Karen Barad [2] introduced agential realism, the 
foundation of sociomateriality, as a relational or “becoming” 
ontology, which posits that “the world is an ongoing open process 
of mattering” [2]. Being relational means that the primary unit of 
ontology is not the object but the relations. Objects’ identity and 
properties are dynamic and are defined through their relationships. 
Becoming or mattering happens through intra-actions, a term that 
implies that actors are not independent but defined within 
entangled relationships, affecting each other. The agency here 
does not mean that, for example, a book writes itself, but that all 
actors in a writing process influence each other; the writer is 
influenced by the technology they use and the surrounding objects 
as much as they change them [2].  
3 Study Design 
Our study investigates the effect of the virtual environment 
(Figure 2) on reflective thinking in the context of university 
students dealing with challenges in planning and managing their 
theses or major projects. We used a repeated questioning process 
called Clean Networks [43,44]. Repetitive processes are shown to 
help with reflection and perspective change [22,28,35]. Clean 
Networks is an intervention based on Clean Language, a coaching 
approach designed to guide a coach to communicate with a client 
[42,44]. The key concept in this practice is to avoid any input 
from the coach that can influence the client’s thinking. So, it relies 
on “clean” questions and comments, i.e., those causing the least 
bias (a.k.a. contamination). The Clean Networks method is 
particularly suitable for the study of materiality as it uses 
environmental effects by repeatedly asking the participants what 
they know about the topic of reflection while moving to a new 
location/perspective after each answer. 
3.1 Participants 
Our study was approved by the institutional research ethics board. 
We invited students at the research team’s university to participate 
and ran the study with 21 participants with an average age of 27.8 
(ranging from 18 to 44). Eight identified as man and thirteen as 
woman. The numbers of undergraduate, Master’s, and PhD 
students were 5, 7, and 9, respectively, coming from different 
fields of study. Two participants considered themselves 
experienced with VR, while 9 had limited familiarity and 10 had 
no experience with it. Similarly, 4 were experienced in formal 
reflection, 9 had some and 8 had no experience with it. Our 
findings showed no significant relation between age, gender, and 
VR/reflection experience and the results discussed in Section 4. 
However, our number of participants is not large enough for such 
statistical analysis. 
 
 
Figure 2. Examples of Virtual Environments 
3.2 Apparatus 
The study was performed in person using the Meta Quest 3 VR 
headset. We used the Nature Treks VR app, available on the Meta 
Quest Store (Figure 1). Participants could choose to visit a variety 
of virtual environments as illustrated in Figure 2. Since the 
primary data collection was through observation and interview, no 
survey, questionnaire, or other instrument was used. The 
GI 2025, May 2025, Kelowna, BC, Canada 
Author et al. 
 
 
 
conversations with the participants were audio recorded using a 
mobile phone or laptop, and no other instrument was used. The 
participants used Quest controls for interaction. On the right hand, 
they only used the trigger button to select a virtual environment 
and navigate using teleportation. On the left hand, they used the 
menu button to return to the main menu. No other buttons and 
interaction methods were used. The environments were very 
interactive in the sense that they provided plenty of opportunities 
to navigate, view, get closer, and occasionally pick up objects in a 
wide range of situations. As initial research, we wanted to focus 
on a limited and basic set of interactions, not including 
manipulation of the environment or influencing the elements. 
Such interactions can be included in future research. 
3.3 Procedure 
We designed and followed a VR version of the Clean Networks 
process. Participants attended the session in a lab on the university 
campus, with the following steps taking about 30-45 minutes: 
1. 
Briefly explaining (to the researcher) the academic goal 
they would like to reflect on. 
2. 
Using the VR HMD, briefly exploring the virtual 
environments to get comfortable with the procedure. 
3. 
Going to the main menu, thinking about their reflection 
topic, and choosing an environment option. 
4. 
Navigating within the environment, choosing and 
stopping at a location. Then, answering two clean 
questions with proper pauses between them for looking 
around, getting comfortable, and thinking: 
i. 
What do you know about [topic] from that 
… space … there? 
ii. 
Is there anything else?  
5. 
Moving to a new location by following the instructions: 
i. 
Find another place in this or in a different 
environment that feels right 
6. 
Repeating steps 4 and 5 above until six rounds of 
questions were completed. 
7. 
After 6 rounds, going back to the main menu and 
answering the final concluding clean questions: 
i. 
And what do you know NOW about 
[topic]?” 
ii. 
And is there anything else you know… 
now? 
iii. 
And what difference does it make? 
8. 
Removing the HMD and answering a few short 
questions about how they feel and think about this 
experience.   
 
3.4 Data Analysis Method 
We used a mixed method (mostly qualitative with some 
quantitative data collection and analysis) approach where we 
collected data through interviews (clean questions and 
background information) and the observation of the virtual 
environments the participants visited and their emotional 
reactions. As the work reported here was an initial step, we 
analyzed the data through a top-down deductive coding for 
concepts already identified in existing literature [27]. These were 
related to stages of the reflective process and environmental 
influences on thinking, with the goal of establishing associations 
between questions, environment, and the participants’ cognitive 
process (i.e., how their ideas on the subject evolved). As discussed 
later, after our analysis, we realized that further coding and 
analysis is necessary with more focus on material characteristics 
of the environment, which will be the topic of future work. 
Based on the commonly mentioned mechanisms through 
which the environment affects thinking [5,33], we considered the 
following environmental effects: Association, Triggered emotion, 
Recalled memory, Metaphorical thinking, Ethical/moral 
consideration, Social relations, and Embodied cues. These formed 
the set of codes for the first dimension of our study, World 
Analysis, where world refers to the virtual environment for each 
stop. These codes were to help answer our primary research 
question on how the VE influenced the reflective process. In 
coding for world analysis, we paid attention to actual 
environmental characteristics causing these effects, such as the 
shape, colour, and nature of objects. 
We considered different models of reflective thinking used in 
Clean Language coaching [14,24] and drew on these models, and 
chose five key aspects for the reflection process: Problem (what 
the participant doesn’t like), Outcome (what the participant wants 
to happen), Resource (anything that can help the participant reach 
the desired outcome), Change (any explicit change in thoughts 
and feelings while going through questioning), and Action 
(anything the participant decides to do). Based on our pilot tests, 
we also considered Comment (general descriptions of the VE or 
participant’s background, not directly related to the above stages) 
and Wobble (inability to say or think of anything) [14]. These 
formed the set of codes for the second dimension of our study, 
Round Analysis, where round refers to each stop at VE by a 
participant. This analysis was to answer our second research 
question to see if the VE helped follow the expected reflective 
process, as in traditional Clean Network approach. 
4 Results and Analysis 
The interventions were done by three members of the research 
team, who each did the first iteration of coding for their 
corresponding data. We analyzed two samples together first to 
make sure we all had the same understanding of the codes and 
made some minor adjustments. The fourth member of the team 
then did a second iteration of coding, and we compared the 
results, discussed to make sure no evidence of code occurrence 
was overlooked or mistaken and reached a conclusion on all 
items.  
As common and accepted in HCI qualitative research [26], we 
did not calculate and emphasize quantitative inter-coder reliability 
measures, as we wanted to encourage different insights and 
discuss them. The primary sources of initial difference were 
inherent vagueness in the nature of data and the possibility of 
more than one code being present in any round where we were to 
choose only one as the dominant. For example, the code Resource 
VR-based Interventions for Perspective Change 
GI 2025, May 2025, Kelowna, BC, Canada 
 
 
was supposed to be used when the participant identifies anything 
that can help them achieve the desired outcome, including people, 
ideas, tools, and feelings. The following text was considered by 
one of the coders are a Resource: 
“I can observe other kinds of life and other ways to live, 
another time to leave, another frequency, and it's a good 
feeling.” 
But others pointed out that while it shows a positive and 
potentially helpful emotion, it is not directly pointed at solving the 
problem. It is more about how the participant feels about the 
environment. So, the code was assigned as Comment as it doesn’t 
directly map to any of the major codes (Problem, Outcome, 
Resource, Change, and Action). 
4.1 Summary of Coding 
From the data, we observed that the reflective process in VR is 
working as expected, i.e., earlier rounds include a higher 
occurrence of Problem and Comment, and moving forward, we 
could see more Outcome, Resource, and then Change and Action. 
Most participants (18 out of 21) concluded with Change or Action 
in round 7. The data suggests that the process was somewhat 
effective in reducing the relative number of Comments in favour 
of identifying Problems, Outcomes, and Resources. Overall, 
round analysis showed a process similar to what was expected in 
non-VE cases. This answered our secondary question, which is 
not the focus of this paper and acted mostly as check point. The 
specific and potential role of VR in reflective process will be the 
topic of another research path.  
We considered Association as a comprehensive term that 
shows establishing any connection between the elements of the 
virtual environment and any concept related to the topic of 
reflection. As such, it included other codes such as memory, 
emotion, and metaphor. This resulted in the number of association 
codes being much higher than the others, as expected. While we 
saw world codes happening with all of the round codes, some 
specific observations can be made from Table 1.  
 
Table 1: Round vs. World Analysis Codes. The number of world analysis codes occurring with each of the round stages 
Round 
Association 
Memory 
Emotion 
Metaphor 
Moral 
Social 
Embodied 
 Total 
Problem 
8 
4 
3 
6 
1 
1 
2 
25 
Comment 
16 
11 
11 
13 
0 
3 
8 
62 
Outcome 
7 
4 
5 
7 
2 
4 
1 
30 
Resource 
13 
4 
9 
11 
1 
8 
2 
48 
Change 
18 
11 
11 
14 
0 
8 
6 
68 
Action 
11 
4 
11 
10 
1 
2 
1 
40 
Total 
73 
38 
50 
61 
5 
26 
20 
 273 
 
Metaphors were clearly the highest occurring influence 
mechanism, followed by emotional responses and memories. 
Problem and Outcome co-occurred with fewer world codes, which 
makes sense as the participants did not need to dig deeper or be 
influenced by the environment to state their challenges and 
desired situations. Resource, Change, and Action, on the other 
hand, happened together with many more VE-triggered 
phenomena, which can suggest the VE was influential in reaching 
changes or identifying actions and resources as intended. Moral, 
social, and embodied triggers happened less frequently, in 
general, than other world codes. This did not surprise us as the 
VEs were single user, with no human element, and had limited 
interactions (mostly walking and looking around). Social concepts 
were triggered through metaphors initiated by animals or 
environmental objects. Embodied effects were experienced 
through actions such as walking, going to higher elevation, being 
surrounded by water, and sitting down. Overall, we observed that 
VEs with more active elements, such as Blue Deep and Jade 
Jurassic, had more occurrences of world codes. The coding 
process illustrated how (1) the participants’ thought process was 
progressing through the stages defined in the round analysis and 
(2) certain characteristics of VEs were influencing them through 
mechanisms in world analysis code, as illustrated in the next 
section. 
4.2 Environmental Effects 
The objects in VE influenced the user experience and reflection 
process through their material characteristics and the mechanisms 
identified in our world analysis. Following, we illustrate these 
influences grouped by those mechanisms. In the examples, the 
material characteristics that initiated the effects are boldfaced. 
Association 
While some participants expressed their challenges without 
any association with the VE, some benefitted from such 
associations to clarify, for themselves and us, what is troubling 
them. In the following two examples, associating with the 
arrangement or size of objects helped participants understand 
and express their current situation and problem: 
Q: And what do you know, from that space there? 
A: In this space, I know that I’m kind of stuck between two 
rocks. That one, maybe, represents my kind of vision of 
GI 2025, May 2025, Kelowna, BC, Canada 
Author et al. 
 
 
 
what I want my research to be. And the other rock sort of 
represents what’s realistic, given the time and scope. So, 
it’s kind of like I’m stuck here having to figure out how to 
get unstuck while also maintaining my, I guess, core values. 
Q: And what do you know about your thesis from there? 
A: Looking at the ocean, I think, maybe, tells me about how big 
of a task it is. 
Memory 
Many cases of association specifically involved recall of past 
memories that were effective in various stages of reflection. For 
example, in the following case, natural scenery reminded the 
participants of their roots and motivations, and so helped with a 
change of perspective and identifying new emotional resources. 
So, I think this place reminds me a little bit of Canada, 
obviously with the snow, and the mountains and the 
animals. But the thing that came to my head when I entered 
this area was Why, why am I doing the PhD in the first 
place? I think one of the main reasons I’ve started it was to 
kind of raise the expectations for my sons. Show them that 
they are capable of doing something difficult, hard, 
something intellectually challenging. 
Emotion 
Similar to the previous example, scenery and especially 
lighting caused emotional responses, which in turn helped 
participants identify desired outcomes, e.g., “joy in research” in 
the following case: 
Oh, well, that first one, this world looked really pretty. But 
when I popped in, I was like, Oh, this is kind of scary. But 
now that I’m here, I can see like, the lake is really pretty. 
And these kind of like Northern Lights, or something, are 
really pretty and it’s kind of the music is nice kind of, 
actually nice and serene. Which I think is making me 
remember when I first started [my research], how like 
nervous I was and everything. And I think it’s also 
important to remember the joy in research. [laughter] And 
kind of allow, maybe allow myself to be excited, instead of 
so focused on deadlines, and employability and all these 
things around the thesis. 
The following participant experienced a relaxing and calm 
virtual environment that helped them discover the importance of 
setting up a happy and relaxing workplace as an action for 
achieving their goals: 
I feel like this is the optimal place. You could do things in 
different ways, like my thesis, the quality will be different. 
This one is the optimum one where all the conditions are 
met then you could end up not only finishing the thesis but 
finishing with a good quality. You need to be happy, I think, 
if I’m to make progress. That’s the idea that’s getting to my 
head now. So, you really have to maybe look at what makes 
you happy and develop environment that makes you happy 
and then you start making progress. 
Metaphor 
A common and particular example of making associations 
was through metaphors. Actions, arrangement, and status of VE 
objects provided opportunities for metaphors which helped 
identify resources in the following cases: 
These two dinosaurs are kind of far from that one. [laughter] 
And they’re together and they’re kind of not caring. And 
they’re wandering together and it kind of makes me think 
like I guess, to focus on my thesis and graduate on time. 
You know, I just have to keep going. Even if you know, the 
threat of not graduating on time, or things like, gives me 
anxiety, I can, you know, I still have to keep living and 
surviving and moving. And there’s other people like [X] 
and [Y] that are there to support me through it as well and 
to help me and that are like on this journey with me. 
All these tall vertical trees, remind me of researchers in this 
area and each area to some extent that are kind of like 
pillars of that particular research stream. So, if I’m 
struggling to find relevant articles, a useful thing to do is 
always go back to their papers and look at the people that 
they’ve referenced. And also recognizing to some extent of 
the people on my committee fit that mold. So, I can always 
reach out to them if I have questions or concerns. 
Moral/Value 
Particular settings and arrangements caused participants to 
consider moral issues and personal values and so changed their 
perspectives on the challenge they had, as in the following 
example:   
I feel like a concert because I’m on the stage or podium. It’s 
weird because at the same time, nobody’s here. Just my 
journey. And it’s interesting because I can remember that 
my journey is about me, not about another person or people 
and I need to put my focus on myself. 
Social 
Interacting elements or even remains of human 
manipulation initiated social considerations resulting in different 
forms of change in perspective or identifying resources. 
This [fire pit] gave me the evidence that another person was 
here. It’s making me think what I’m forgetting is that not 
only there are people around me while I go through this, 
but there’s other people who have been in my position who 
have gone through this and who now have great careers. 
And it’s a nice reminder. 
Embodied 
Many aspects of the VEs facilitated embodied experiences 
through a sense of motion, elevation, immersion, etc. Through 
these experiences, the participants felt more connected to the 
environment and were able to comment on it and then on their 
particular challenges. 
Let’s find out. Can I get in this boat? No. I gotta look up. Can 
I climb? Oh, my gosh, I can climb. Oh my god. [Excitement] 
[Scream] I’m on the floor. I’m not gonna fall. Oh. 
[expressing excitement] This is too crazy. Do you feel 
nervous? I am sweating! I realized that maybe I do have a 
fear of heights. Crazy! 
I really like being by the water. I like to swim. I think it’s a good 
place to read also it looks pretty. I like how the water is 
VR-based Interventions for Perspective Change 
GI 2025, May 2025, Kelowna, BC, Canada 
 
 
always moving so I don’t know makes me think about life 
how nothing is static everything is moving. 
5 Discussion  
In response to our primary research question on the role of 
materiality in virtual environments, we demonstrated that different 
elements in VE could trigger and initiate thoughts and emotions in 
the user, even in cases where these elements were not specifically 
planned for that purpose. This initiation happened through 
mechanisms such as memory, metaphor, and embodiment, and 
was related to material characteristics such as size, shape, 
arrangement, lighting, and scenery. The affected thought 
processes (in our case, reflection) could lead the user through a 
discovery of novel insights (i.e., gaining new perspectives) on the 
topics they found challenging. Our findings are twofold. First, 
they show the ability of generic VEs to act as a catalyst for 
reflection and perspective change. Second, they emphasize the 
role of such material objects and settings in the overall virtual 
experience.  
Based on our findings, we suggest further investigating virtual 
materiality as a counterpart to, or extension of, sociomateriality in 
VEs. Such virtual materiality may posit that despite their 
limitations in terms of interaction and modality, VEs are made of 
objects (material entities) that are not merely acted upon. They 
can actively shape users’ experience based on different 
characteristics and through different mechanisms without being 
intended for that effect. A virtual phenomenon, in this sense, can 
be an intra-action (in a sociomateriality sense) between entangled 
entities, just like its counterpart in the physical world.  
Knowing that random virtual environments, not designed for 
any specific purpose, can trigger new thoughts means that there is 
a new way of helping individuals with their reflective goals. This 
is particularly important once we consider the ability of automated 
systems to implement “clean” repeated questions. This idea can 
then be extended to other cognitive tasks, especially learning, 
where a learner or entire groups can practice their activities within 
different environments. A balance between positive influences as 
observed in our study and negative ones such as unwanted 
distraction should be the subject of further research, so should the 
suitability of different environments for different tasks. 
On the other hand, the materiality of virtual environments and 
the effect of VEs on user’s mood and cognition can raise 
significant concerns about the way VR design and research is 
conducted. If a small object can trigger significant cognitive and 
affective consequences, the validity of many research findings and 
design decisions based on controlled settings will be questionable 
or limited. Again, further research is needed to investigate the way 
materiality presents itself in virtual environments and how it 
should be considered in future work. 
Our analysis was preliminary, as we only coded for the 
environmental influence mechanisms mentioned in the literature, 
in a purely deductive way. Further deductive and inductive (i.e., 
abductive) research can look for other mechanisms and also code 
for specific elements. Also, due to logistic reasons and the 
preliminary nature of this study, we only investigated natural 
settings. Urban, abstract, and other forms of environments can 
have their own materiality and influences, which were not 
considered in our study. Similarly, other tasks and experiences 
beyond academic reflection, and other forms of interaction with 
the environment, should be included in investigating materiality. 
It will also be helpful to have a non-virtual baseline for comparing 
how effective VR is for reflection, but our focus on this 
preliminary research was only to demonstrate that material 
characteristics of VEs could meaningfully change perspective and 
influence user experience. 
We further recognize that the limited number of participants 
does not allow generalizations and that the occurrence of the 
world effects at the same time as reflective stages does not mean 
direct causation. However, we believe our qualitative data 
suggests a link and is a motivation and useful starting point for 
future research. 
6 Conclusion 
Our study was an exploratory investigation of how VR can 
include materiality as a significant characteristic and how such 
virtual materiality can influence user experience in unexpected 
ways. We illustrated this through the ability of generic 
environments to guide participants through a repeated questioning 
process and allow them to reach new insights through exposure to 
random virtual stimuli. Based on the findings, we introduced the 
notion of virtual materiality and reviewed some possible 
implications it may have. Future research is needed to overcome 
the limitations of our study and offer more insights into the role of 
materiality in virtual environments. 
REFERENCES 
[1] M. B. Andreucci, A. Loder, M. Brown and B. J., “Exploring Challenges and 
Opportunities of Biophilic Urban Design: Evidence from Research and 
Experimentation,” Sustainability, vol. 13, p. 4323, 2021.  
[2] K. Barad, Meeting the universe halfway: Quantum physics and the entanglement 
of matter and meaning, 2007: Duke Up.  
[3] G. N. Bratman, G. C. Daily, B. J. Levy and J. J. Gross, “The benefits of nature 
experience: Improved affect and cognition,” Landscape and urban planning, no. 
138, pp. 41-50, 2015.  
[4] M. Brennan and L. Christiansen, “Virtual materiality: A virtual reality framework 
for the analysis and visualization of cultural heritage 3D models,” Digital 
Heritage, 2018.  
[5] J. S. Brown, A. Collins and P. Duguid, “Situated cognition and the culture of 
learning,” Subject Learning in the Primary Curriculum, vol. 18, no. 1, 1989.  
[6] C. Capaldi, H. A. Passmore and N. E., “Flourishing in nature: A review of the 
benefits of connecting with nature and its application as a well-being 
intervention,” Intnl Journal of Well-being, vol. 5, pp. 1-16, 2015.  
[7] P. Chan, T. Van Gerven, J.-L. Dubois and K. Bernaerts, “Virtual chemical 
laboratories: A systematic literature review of research, technologies and 
instructional design,” Computers and Education Open, vol. 2, p. 100053, 2021.  
[8] S. H. M. Chan, L. Qiu and G. Esposito, “Nature in virtual reality improves mood 
and reduces stress: evidence from young adults and senior citizens,” Virtual 
Reality, vol. 27, p. 3285–3300, 2023.  
[9] C. Frauenberger, "Entanglement HCI the next wave?," ACM Transactions on 
Computer-Human Interaction (TOCHI), vol. 27, no. 1, 2019.  
[10] I. Frigione, G. Massetti, M. Girondini, R. Etzi, G. Scurati, F. Ferrise, A. Chirico, 
A. Gaggioli and A. Gallace, "An Exploratory Study on the Effect of Virtual 
Environments on Cognitive Performances and Psychophysiological Responses," 
Cyberpsychology, Behavior, and Social Networking, vol. 25, no. 10, pp. 666-
671, 2022.  
GI 2025, May 2025, Kelowna, BC, Canada 
Author et al. 
 
 
 
[11] S. Frost, L. Kannis-Dymand and V. Schaffer, “Virtual immersion in nature and 
psychological well-being: A systematic literature review,” Journal of 
Environmental Psychology, vol. 80, 2022.  
[12] W. Gaver, “Technology affordances,” in SIGCHI conference on Human factors 
in computing systems, 1991.  
[13] J. J. Gibson, The ecological approach to visual perception: classic edition, 
Psychology press, 2014.  
[14] P. Harland, The power of six: a six part guide to self-knowledge, Wayfinder, 
2012.  
[15] D. Hawes and A. Arya, “VR-based student priming to reduce anxiety and 
increase cognitive bandwidth,” in 2021 IEEE Virtual Reality and 3D User 
Interfaces (VR), 2021.  
[16] D. Hawes and A. Arya, “VR-based context priming to increase student 
engagement and academic performance,” in International conference of the 
immersive Learning research Network (iLRN), 2022.  
[17] F. Herrera, J. Bailenson, E. Weisz, E. Ogle and J. Zaki, “Building long-term 
empathy: A large-scale comparison of traditional and virtual reality perspective-
taking,” PloS one, vol. 13, no. 10, 2018.  
[18] F. Herrera and J. N. Bailenson, “Virtual reality perspective-taking at scale: Effect 
of avatar representation, choice, and head movement on prosocial behaviors,” 
New Media & Society, vol. 23, no. 8, 2021.  
[19] S. Higgins, S. Alcock, B. De Aveiro, W. Daniels, H. Farmer and S. Besharati, 
“Perspective matters: a systematic review of immersive virtual reality to reduce 
racial prejudice,” Virtual Reality, vol. 28, no. 3, 2024.  
[20] L. Hultin, “On becoming a sociomaterial researcher: Exploring epistemological 
practices grounded in a relational, performative ontology,” Information and 
Organization, vol. 29, no. 2, pp. 91-104, 2019.  
[21] C. Jiang, Y. Chang and W. C. Sullivan, “A dose of nature: Tree cover, stress 
reduction, and gender differences,” Landscape and Urban Planning, vol. 132, no. 
December, pp. 26-36, 2014.  
[22] A. King, “Guiding knowledge construction in the classroom: Effects of teaching 
children how to question and how to explain,” American educational research 
journal, vol. 31, no. 2, pp. 338-368, 1994.  
[23] B. Latour, Reassembling the Social: An Introduction to Actor-network-theory, 
Oxford University Press, 2005.  
[24] J. Lawley and P. Tompkins, “REPROCess: Modelling Attention,” Neuro 
Linguistic Programming, vol. 5, 2012.  
[25] H. Li, X. Zhang and H. Wang, “Access to Nature via Virtual Reality: A Mini-
Review,” Frontiers in Psychololgy, vol. 12, 2021.  
[26] N. McDonald, S. Schoenebeck and A. Forte, “Reliability and inter-rater 
reliability in qualitative research: Norms and guidelines for CSCW and HCI 
practice,” Proceedings of the ACM on human-computer interaction, vol. 3, pp. 
1-23, 2019.  
[27] M. B. Miles and A. M. Huberman, Qualitative Data Analysis: An Expanded 
Sourcebook. Sage, 1994. 
[28] J. Moon, Reflection in learning and professional development: Theory and 
practice, Routledge, 2013.  
[29] F. Mostajeran, F. Steinicke and S. Reinhart, “Adding virtual plants leads to higher 
cognitive performance and psychological well-being in virtual reality,” Science 
Report, vol. 13, p. 8053, 2023.  
[30] M. Murdoch and J. Davies, “Spiritual and affective responses to a physical 
church and corresponding virtual model,” Cyberpsychology, Behavior, and 
Social Networking, vol. 20, no. 11, p. 702—708, 2017.  
[31] D. Norman, The design of everyday things: Revised and expanded edition, Basic 
Books, 2012.  
[32] W. J. Orlikowski and S. V. Scott, “Sociomateriality: challenging the separation 
of technology, work and organization,” Academy of Management annals, vol. 2, 
no. 1, 2008.  
[33] A. M. Paul, The extended mind: The power of thinking outside the brain, Eamon 
Dolan Books, 2021.  
[34] A. Rizzo, S. T. Koenig and T. B. Talbot, “Clinical results using virtual reality,” 
Journal of Technology in Human Services, vol. 37, no. 1, 2019.  
[35] K. Saville, “Strategies for using repetition as a powerful teaching tool,” Music 
Educators Journal, vol. 98, no. 1, pp. 69-75, 2011.  
[36] A. Scavarelli, A. Arya and R. J. Teather, “Virtual reality and augmented reality 
in social learning spaces: a literature review,” Virtual Reality, vol. 25, p. 257–
277, 2021.  
[37] D.-H. Shin, “The role of affordance in the experience of virtual reality learning: 
Technological and affective affordances in virtual reality,” Telematics and 
Informatics, vol. 34, p. 1826–1836, 2017.  
[38] G. Spano, A. Theodorou and G. Reese, “Virtual nature, psychological and 
psychophysiological outcomes: A systematic review,” Journal of Environmental 
Psychology, vol. 89, 2023.  
[39] J. H. Steffen, J. E. Gaskin, T. O. Meservy, J. L. Jenkins and I. Wolman, 
“Framework of affordances for virtual reality and augmented reality,” Journal of 
Management Information Systems, vol. 36, p. 683–729, 2019.  
[40] E. R. Stepanova, D. Quesnel and B. E. Riecke, “Space—A Virtual Frontier: How 
to Design and Evaluate a Virtual Reality Experience of the Overview Effect,” 
Frontiers in Digital Humanities, vol. 6, 2019.  
[41] L. A. Suchman, Plans and situated actions: The problem of human-machine 
communication, 1987.  
[42] W. Sullivan, Clean language: Revealing metaphors and opening minds, Crown 
House Publishing, 2008.  
[43] C. Wilson, “Coaching: Tools of the Trade,” Training Journal, no. May, 2009.  
[44] C. Wilson, The work and life of David Grove: Clean language and emergent 
knowledge, Troubador Publishing, 2017.  

==================================================
